# Appendix {.unnumbered}

## Set-up Your Elastic Cluster

If you're going to learn from the various examples in this study guide
it would help to have a running Elastic cluster. You can get one in one
of three ways:

1.  Elastic Cloud

    1.  Access to a paid cluster

    2.  14-day free trial

2.  Local instance

If you have access to a paid Elastic Cloud instance then you can skip
the rest of these instructions as you have already been using Elastic to
some extent and you can just jump into the exercises.

If you don't already have an account with Elastic, then the most
convenient method is to have a local cluster running on your PC/Mac
using Docker or, if you insist, creating a 14-day free trail of Elastic
that will run out long before you are ready to give it up. In other
words, create a local instance and spare yourself some pain.

Let's briefly go over the set-up of either:

-   the 14-day free trail

-   local instance

### The Elastic Cloud 14-day Free Trial

Once you have signed up for your 14-day trial you will be directed to
create your first deployment. Don't rush it. In the process of creating
the new deployment you will be shown:

-   the username (**elastic)**

-   the password (**the usual collection of
    numbers/letters/punctuation)**

![](media/image1.png){width="6.925in" height="6.648611111111111in"}

> Figure N: The Username/Password for use from the Command Line

Once the deployment is created and you are taken to one of the many
landing pages you will need the URL to the Elastic cluster endpoint that
will allow you to work with Elasticsearch. The **Elastic cluster
endpoint** can be found by:

> 1\. Pressing the hamburger menu (the three parallel lines) on the top
> left hand side of the Elastic web page to open the main menu:
>
> ![](media/image2.jpg){width="3.0722222222222224in"
> height="2.0416666666666665in"}
>
> Figure N:
>
> 1\. Press **Manage This Deployment**. That will take you the
> **Deployments** page which should list your deployment. There will be
> a section titled Applications. Click on the **Elasticsearch** link
> labeled **Copy Endpoint**.
>
> ![](media/image3.jpg){width="4.749305555555556in"
> height="2.988888888888889in"}
>
> Figure N:
>
> 1\. Paste that link somewhere for future use. You're going to be using
> it quite a bit. If you lose it just go back to the **Deployments**
> page and click on the link again.

### Local Instance

Docker. We are going to use Docker. It is the easiest to clean up when
you are done vs. copying github repos, etc.

The [[instructions on the Elastic
site]{.underline}](https://www.elastic.co/guide/en/elasticsearch/reference/8.13/getting-started.html)
explain how to install the Elastic Stack locally using Docker. This book
assumes you are using Ubuntu Linux and understand the Linux command line
and shell. If you are using another version of Linux then it may be
worth looking at the Elastic documentation for the variations (there
aren't many).

***On The Off-chance You Have already Done This and Need to Reset
Everything***

We know. It happens. You follow all the steps exactly and still need to
redo all the steps.

To reset you Elasticsearch install do the following (assuming you have
docker installed):

sudo docker stop es01 es02 kibana sudo docker rm es01 es02 kibana sudo
docker network rm elastic

You can safelyl start again.

***Install Docker (if you don't already have it installed)*** Open a
terminal window and execute: sudo snap install docker

***Create a Docker Network Named Elastic*** sudo docker network create
elastic ***Install the Elastic image***

The version available at the time this was written was 8.14.1. It will
probably have changed by the time you get this study guide. The exam
only cares about [[Elastic version
8.1]{.underline}](https://www.elastic.co/training/certification/faq).
sudo docker pull docker.elastic.co/elasticsearch/elasticsearch:8.14.1
***Run the Elastic image***

Create a single-node cluster named es01. It will use the Docker network
we created above as well as the default HTTP port, transport port, and
the Elastic image we just downloaded.

At the very end of the installation there will be output that you need
to retain.

sudo docker run \--name **es01** \--net elastic -p 9200:9200 -p
9300:9300 -e

\"discovery.type=single-node\" -e \"cluster.name=cluster-1\" -m 1GB -t
docker.elastic.co/elasticsearch/elasticsearch:8.14.1

There will be a lot of output, but at the end of the installation there
will be some output that includes:

-   the password for user elastic

-   the HTTP CA certificate SHA-256 fingerprint

-   an enrollment token (you will use it when you set up Kibana)

-   another enrollment token used to enroll other nodes (you won't be
    using that in this study guide) Just copy the entire block to a text
    editor and remember where you saved it.

Note that the name of the container instance is **es01**. You'll need to
remember that when you start the container again later.

***Pull/Run the Kibana image***

Open another terminal tab or window as es01 should still be running and
waiting to hook up with the Kibana instance.

sudo docker pull docker.elastic.co/kibana/kibana:8.14.1 sudo docker run
\--name kibana \--net elastic -p 5601:5601

docker.elastic.co/kibana/kibana:8.14.1

Notice that the name of the container instance is **kibana**. You'll
need that when you start the container again later.

When you run the Kibana instance (the last command above) the output to
the command line will print out a URL to the Kibana instance. The URL
will be something like:

[[http://0.0.0.0:5601/?code=971626]{.underline}](http://0.0.0.0:5601/?code=971626)

The code will probably be different.

Copy and paste the URL into your browser which should open on the
following landing page:

![](media/image4.png){width="6.925in" height="6.69375in"}

> Figure A-1: Configure Kibana page

You need to enter the enrollment token that appeared during the
execution of es01 (the **Elasticsearch** container instance). Enter the
token in the Kibana screen and press **Configure Elastic**. If all goes
well the Welcome to Elastic page should appear.

![](media/image5.png){width="6.925in" height="6.69375in"}

Figure A-2: The Kibana Login Page to the Elastic Instance Log in with
the user name **elastic** and the password from the **es01** output.

Press **Log In**.

A Welcome Home landing page should appear. The first time it will
display a dialog asking if you want it to do things for you or if you
want to **Explore On Your Own**.

Press **Explore on My Own**.

Press **\[hamburger menu\] → Management → DevTools**

That should open on the **Console** where you will be working through
the various examples listed in this book. Enter the following and press
the triangle to the right on the line where you entered this:

GET /\_cluster/settings

You should get something that looks like this:

{

\"persistent\": {},

\"transient\": {}

}

Go celebrate! You have a working Elasticsearch cluster.

To shut it down the first time just press Ctrl-C in each tab/terminal
where you have the instances running.

To restart your cluster open another tab or terminal and enter:

sudo docker start es01 kibana

When you decide to stop your cluster enter: sudo docker stop es01 kibana

### Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)

***Setting up es01***

13. docker exec ssh es01 sudo docker exec -it es01 /bin/bash

14. create the p12 certificate

\#
https://www.elastic.co/guide/en/elasticsearch/reference/8.14/security-basic-setup.html

elasticsearch-certutil ca

elasticsearch-certutil cert \--ca elastic-stack-ca.p12

15. move the p12 file into the config/certs directory and modify the
    file permissions

cd config/certs mv ../../elastic-certificates.p12 . chmod ug+rw
elastic-certificates.p12

16. reset the passwords (even if there is none)

elasticsearch-keystore add
xpack.security.transport.ssl.keystore.secure_password
elasticsearch-keystore add
xpack.security.transport.ssl.truststore.secure_password

17. exit the container

exit

18. docker copy the elasticsearch.yml file from **es01**

sudo docker cp es01:/usr/share/elasticsearch/config/elasticsearch.yml

elasticsearch-es01.yml sudo chmod ugo+rw elasticsearch.yml

19. Edit elasticsearch-es01.yml:

cluster.name: \"cluster-1\" network.host: 0.0.0.0 node.name: c1-node-1

\# Enable security features xpack.security.enabled: true
xpack.security.enrollment.enabled: false
xpack.security.transport.ssl.enabled: true

xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.client_authentication: required

xpack.security.transport.ssl.keystore.path:
certs/elastic-certificates.p12
xpack.security.transport.ssl.truststore.path:
certs/elastic-certificates.p12 xpack.security.authc.token.enabled: false

\# Enable encryption for HTTP API client connections, such as Kibana,
Logstash, and

Agents xpack.security.http.ssl: enabled: false

keystore.path: certs/http.p12

20. Copy elasticsearch-es01.yml back to es01

sudo docker cp elasticsearch-es01.yml

es01:/usr/share/elasticsearch/config/elasticsearch.yml

21. restart es01 and confirm commication with kibana

sudo docker stop es01 kibana sudo docker start es01 kibana

sudo docker logs -f es01 \# may need separate terminal sudo docker logs
-f kibana \# may need separate terminal

***es02***

22. open another tab or terminal and docker run es02

sudo docker run \--name es02 \--net elastic -p 9201:9200 -p 9301:9300 -e

\"discovery.type=single-node\" -e \"cluster.name=cluster-2\" -m 1GB -t
docker.elastic.co/elasticsearch/elasticsearch:8.14.1

23. docker copy elastic-certificates.p12 from es01 (both clusters have
    to have the same transport certificate)

sudo docker cp
es01:/usr/share/elasticsearch/config/certs/elasticcertificates.p12 .

24. docker copy elastic-certificates.p12 to es02

sudo docker cp elastic-certificates.p12
es02:/usr/share/elasticsearch/config/certs

25. Copy elasticsearch-es01.yml to elasticsearch-es02.yml

cp elasticsearch-es01.ymlcelasticsearch-es02.yml

26. Edit elasticsearch-es02.yml. The only thing to change is the cluster
    name and the node name.

**cluster.name: \"cluster-2\"** network.host: 0.0.0.0 **node.name:
c2-node-1**

\# Enable security features xpack.security.enabled: true
xpack.security.enrollment.enabled: false
xpack.security.transport.ssl.enabled: true

xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.client_authentication: required

xpack.security.transport.ssl.keystore.path:
certs/elastic-certificates.p12
xpack.security.transport.ssl.truststore.path:
certs/elastic-certificates.p12 xpack.security.authc.token.enabled: false

\# Enable encryption for HTTP API client connections, such as Kibana,
Logstash, and

Agents xpack.security.http.ssl: enabled: false

keystore.path: certs/http.p12

27. Confirm rw permissions to elasticsearch.ymlls -l
    elasticsearch-es02.yml

28. docker copy the modified elasticsearch-es02.yml file to es02

sudo docker cp elasticsearch-es02.yml

es02:/usr/share/elasticsearch/config/elasticsearch-es02.yml

29. docker exec ssh es02 sudo docker exec -it es02 /bin/bash

30. reset the passwords to access the p12 file (even if there is none)

\#
https://www.elastic.co/guide/en/elasticsearch/reference/8.14/security-basic-

setup.html

elasticsearch-keystore add
xpack.security.transport.ssl.keystore.secure_password
elasticsearch-keystore add
xpack.security.transport.ssl.truststore.secure_password

31. restart es02

sudo docker stop es02 sudo docker start es02

#### 32. THE FIRST TIME USE THIS TO CONFIGURE THE LOCAL CLUSTER (ES01) TO RECOGNIZE THE REMOTE CLUSTER (ES02)

\# Do this from Kibana -\> Management -\> DevTools

PUT /\_cluster/settings

{

\"persistent\": {

\"cluster\": {

\"remote\": {

\"es01\": {

\"seeds\": \[

\"es01:9300\"

\],

\"skip_unavailable\": true

},

\"es02\": {

\"seeds\": \[

\"es02:9300\"

\],

\"skip_unavailable\": false

}

}

}

}

}

Go to the Remote Cluster dashboard page. You should see es01 and es02
listed and communication as active.

![](media/image6.jpg){width="6.925in" height="2.345138888888889in"}

Figure N: es01 and es02 are communicating and ready for cross-cluster
queries.

If you ever feel the need to delete the connection between the two
cluster just execute this:

33\. AFTER EVERY SESSION YOU MUST DELETE THE CLUSTER SETTINGS AND
RESTORE THEM BEFORE STARTING AGAIN:

\# TO REMOVE THE CLUSTER SETTINGS

PUT /\_cluster/settings

{

\"persistent\": {

\"cluster.remote.local.seeds\": null,

\"cluster.remote.local.skip_unavailable\": null,

\"cluster.remote.remote.seeds\": null,

\"cluster.remote.remote.skip_unavailable\": null

}

}

## FAQ

**Q: What is the URL for the running instance of Elasticsearch?**

**A:** On your local machine the URL is usually
[[https://172.18.0.2:9200]{.underline}](https://172.18.0.2:9200/) or
[[https://localhost:9200]{.underline}](https://localhost:9200/).

In the Elastic Cloud, after you have created a deployment, you can click
on:

 \[hamburger menu\] → Manage this deployment → \[takes you to the
landing page of your deployment\] → Applications → Elasticsearch → Copy
endpoint Paste the copied endpoint into a browser and get to work.

**Q: How do I run the two docker images of elastic and kibana when I
need to restart them?**

**A:** In a terminal window run:

sudo docker start es01 sudo docker start kibana

Go to the Kibana URL to run all the exercises:

http://0.0.0.0:5601

**Q: What is the importance of the \@timestamp field?**

**A:** The **\@timestamp** field in Elasticsearch serves a crucial
purpose related to time-based data.

> 1\. **Timestamp for Time-Series Data**:

-   Elasticsearch often deals with time-series data, where each document
    represents an event or record associated with a specific timestamp
    (e.g., log entries, sensor readings, transactions).

-   The \@timestamp field explicitly indicates when the document was
    created or when the event occurred.

> 7\. **Data Streams and Rollover Policies**:

-   When using **data streams**, which handle append-only time series,
    the \@timestamp field is mandatory.

-   Data streams manage collections of documents with a time dimension.
    Having an explicit timestamp field allows Elasticsearch to apply
    time-based rollover policies.

-   Rollover policies create new write indices (current write index) and
    backing indices (for historical data) based on time intervals or
    document count thresholds.

-   The \@timestamp field ensures that data streams can efficiently
    manage these indices.

> 8\. **Write and Backing Indices**:

-   Writes are directed to the current write index, while reads can
    access all backing indices.

-   The \@timestamp field plays a key role in determining which indices
    contain relevant data for a given time range.

-   Without this field, Elasticsearch wouldn't know how to organize data
    into time-based segments.

> 9\. **Mapping and Default Options**:

-   Every document indexed to a data stream must contain an \@timestamp
    field.

-   By default, Elasticsearch maps \@timestamp as a date field with
    default options.

-   You can customize the mapping if needed, but having the field is
    essential for proper data stream functioning.

In summary, the \@timestamp field ensures that Elasticsearch can
effectively manage time-series data, apply rollover policies, and
organize indices based on time intervals. [[It's a fundamental part
of]{.underline} [handling time-based data in
Elasticsearch3]{.underline}](https://stackoverflow.com/questions/68317063/what-is-the-use-of-the-mandatory-timestamp-field-in-elasticsearch-datastream)
[[4]{.underline}](https://stackoverflow.com/questions/21292284/is-the-timestamp-field-needed-when-using-logstash-to-store-in-elasticsearch)
. If you're working with logs, events, or any time-stamped data, the
\@timestamp field becomes critical for accurate indexing and querying.

## DevTools Console Auto-complete

This is all about auto-complete. The more you know of the various nodes
needed by the REST API call you are trying to make the quicker you can
get back to completing your exercise.

The Console where you enter your REST API calls to Elasticsearch has a
number of keyboard shortcuts to help you fill out the JSON that needs to
be sent with most calls. Here is a list of the keyboard shortcuts in no
particular order.

On a new line the first thing the auto-complete will do is fill in the
HTTP command as soon as you **enter one letter** (press **Enter** after
selecting one of the choices):

![](media/image7.jpg){width="6.925in" height="2.604861111111111in"}

Figure N:

As soon as you press another key another dropdown will open giving you
more choices. For example, pressing slash (/):

> ![](media/image8.jpg){width="6.5611122047244095in"
> height="2.7180555555555554in"}

Figure N:

Pressing dot(.):

> ![](media/image9.jpg){width="6.5611122047244095in"
> height="2.7180555555555554in"}

Figure N:

Pressing any alphabetic key (a-z):

> ![](media/image10.jpg){width="6.5611122047244095in"
> height="2.7180555555555554in"}

Figure N:

Notice how pressing practically any letter will who results with the
letter that matches being highlighted:

> ![](media/image11.jpg){width="6.5611122047244095in"
> height="2.7180555555555554in"}

Figure N:

It will also list full word matches as well:

> ![](media/image12.jpg){width="6.5611122047244095in"
> height="1.4895833333333333in"}

Figure N:

When you enter a single curly brace to open up a JSON node you will see
a white X in a red box:

> ![](media/image13.jpg){width="6.5611122047244095in"
> height="1.4895833333333333in"}

Figure N:

Fear not! Press Enter and the Console editor will add the closing curly
brace and indent the position of the cursor so you can type your next
line:

> ![](media/image14.jpg){width="6.5611122047244095in"
> height="1.4895833333333333in"}

Figure N:

To determine what the allowable names are press open quote ("):

![](media/image15.jpg){width="6.925in" height="2.6527777777777777in"}

Figure N:

If you selected index_patterns it would complete the line for you:

![](media/image16.jpg){width="6.925in" height="1.2638888888888888in"}

Figure N:

Unfortunately, not all of the node types will appear. Notice, for this
example, dynamic_template does not appear after mappings even though it
is meant to be used after mappings in the creation of a dynamic
template:

![](media/image17.jpg){width="6.925in" height="2.2888888888888888in"}

Figure N:

Once you start entering node information for an unrecognized node the
auto-complete will stop working. I'm sure there is a way to fix that,
but I have not found it. **Be aware.**

Other valid attributes that don't appear in auto-complete:

> • terms_set

You can select the Settings tab to change your Console editor settings:

![](media/image18.jpg){width="6.925in" height="2.6909722222222223in"}

Figure N:

![](media/image19.jpg){width="6.925in" height="4.531944444444444in"}

Figure N:

Or you can select the **Help** tab to look at the full list of supported
keyboard commands:

![](media/image18.jpg){width="6.925in" height="2.6909722222222223in"}

Figure N:

![](media/image20.jpg){width="6.925in" height="3.897222222222222in"}

Figure N:

***Random cut and paste stuff***

***Test***

1.  Verify the index creation

> GET \[index name\] or GET /\_cat/indices

2.  Verify the field mappings GET /\[index name\]/\_mapping

3.  Index and search sample data:

> ◦ Index
>
> POST /\[index name\]/\_doc
>
> {
>
> \[field name\] : \[value\]
>
> \[field name\] : \[value\]
>
> }
>
> ◦ Search
>
> GET /\[index name\]/\_search?q=\[field name\]:\[value\]\*

4.  Any other appropriate tests

> \[CODE HERE\]

========

**To check out the cluster from either a browser or using curl**

For example If you are running a local cluster then you know your
username/password. You should also know your endpoint as it will look
something like:

localhost:9200/\[paths based on what you are trying to do\]

The port might be different based on any customizations you might have
made when you installed the cluster. Check that you can get to it by
opening a command line window and run:

curl -u \[your username\]:\[your password\] -X GET
\"localhost:9200/nonexistent/\_search? pretty\"

You should get an error message from your cluster (unless you have an
index called **nonexistent**) that looks something like this:

{

\"error\" : {

\"root_cause\" : \[

{

\"type\" : \"index_not_found_exception\",

\"reason\" : \"no such index \[nonexistent\]\",

\"resource.type\" : \"index_or_alias\",

\"resource.id\" : \"nonexistent\",

\"index_uuid\" : \"\_na\_\",

\"index\" : \"nonexistent\"

}

\],

\"type\" : \"index_not_found_exception\",

\"reason\" : \"no such index \[nonexistent\]\",

\"resource.type\" : \"index_or_alias\",

\"resource.id\" : \"nonexistent\",

\"index_uuid\" : \"\_na\_\",

\"index\" : \"nonexistent\"

},

\"status\" : 404

}

With all that said, at some point you need to stop the containers and
restart them at a later time (like when you want to get some sleep):

-   Ctrl+C to stop the containers (doesn't matter which you stop first)

-   In one terminal window start Elasticsearch

> docker start es01 #or whatever name you gave the container

-   In another terminal window start Kibana

> docker start kibana \# or whatever name you gave the containers

Give it a minute or so and you should be able to see the login page
again.
