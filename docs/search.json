[
  {
    "objectID": "1-data-management.html",
    "href": "1-data-management.html",
    "title": "Data Management",
    "section": "",
    "text": "The platform hosts articles, each with text content, a publication date, author details, and tags.\nArticles need to be searchable by content, title, and tags.\nThe application requires fast search responses and efficient storage.\nThe application should handle date-based queries efficiently.\nAuthor details are nested objects that include the author’s name and email.\n\n\n\n\n\nChoose Appropriate Settings:\n\nUse a custom analyzer for the articles’ content to improve text search.\nSet the number of shards to balance between write performance and search speed.\nConfigure replicas for high availability and read performance.\n\nDefine Mappings:\n\nContent and Title: Use the text data type with a custom analyzer.\nPublication Date: Use the date data type.\nTags: Use the keyword data type for exact matching.\nAuthor: Use a nested object to keep author details searchable and well-structured.\n\nOpen the Kibana Console or use a REST client.\nCreate the index\n\nPUT blog_articles\n{\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 1,\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_text_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_text_analyzer\"\n      },\n      \"content\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_text_analyzer\"\n      },\n      \"publication_date\": {\n        \"type\": \"date\"\n      },\n      \"author\": {\n        \"type\": \"nested\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"text\",\n            \"analyzer\": \"custom_text_analyzer\"\n          },\n          \"email\": {\n            \"type\": \"keyword\"\n          }\n        }\n      },\n      \"tags\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\nOr insert the settings and mappings separately.\nPUT blog_articles\n{\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 1,\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_text_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  }\n}\nAnd:\nPUT blog_articles/_mapping\n{\n  \"properties\": {\n    \"title\": {\n      \"type\": \"text\",\n      \"analyzer\": \"custom_text_analyzer\"\n    },\n    \"content\": {\n      \"type\": \"text\",\n      \"analyzer\": \"custom_text_analyzer\"\n    },\n    \"publication_date\": {\n      \"type\": \"date\"\n    },\n    \"author\": {\n      \"type\": \"nested\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"text\",\n          \"analyzer\": \"custom_text_analyzer\"\n        },\n        \"email\": {\n          \"type\": \"keyword\"\n        }\n      }\n    },\n    \"tags\": {\n      \"type\": \"text\"\n    }\n  }\n}\n\n\n\n\nVerify the index\n\nGET /_cat/indices\n\nVerify the mappings\n\nGET /blog_articles/_mapping\n\nIndex and search for a document\n\n# Index\nPOST /blog_articles/_doc\n{\n  \"title\" : \"My First Blog Post\",\n  \"content\" : \"What an interesting way to go...\",\n  \"publication_date\" : \"2024-05-15\",\n  \"tags\" : \"superb\",\n  \"author\" : {\n    \"name\" : \"John Doe\",\n    \"email\" : \"john@doe.com\"\n  }\n}\n# Search like this\nGET /blog_articles/_search\n# Or search like this\nGET /blog_articles/_search?q=tags:superb\n# Or search like this\nGET blog_articles/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"default_field\": \"tags\",\n      \"query\": \"superb\"\n    }\n  }\n}\n# Or search like this\nGET blog_articles/_search\n{\n  \"query\": {\n    \"nested\": {\n      \"path\": \"author\",\n      \"query\": {\n        \"match\": {\n          \"author.name\": \"john\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nAnalyzers: Custom analyzers are used for content and title to optimize for search effectiveness. While we create a custom analyzer it was not part of the requirements.\nShards and Replicas: Adjust these settings based on expected data volume and query load.\nNested Objects: These are crucial for maintaining the structure and searchability of complex data like author details.\n\n\n\n\n\nIn the console execute the following\n\nDELETE blog_articles\n\n\n\n\ncat indices API\nCreate Index API\nCustom Analyzer\nGet mapping API\nSearch API\n\n\n\n\n\n\n\n\nStore log data with a timestamp field\nEnable time-based data management\nUse a custom routing value based on the log source\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index\n\nPUT /log_data\n{\n  \"settings\": {\n    \"number_of_shards\": 3\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"@timestamp\": {\n        \"type\": \"date\"\n      },\n      \"log_source\": {\n        \"type\": \"keyword\"\n      },\n      \"message\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index creation\n\nGET /log_data\nOr\nGET /_cat/indices\n\nVerify the field mapping\n\nGET /log_data/_mapping\n\nIn order for the indexing and searching of example docs to work\nIndex and search for a sample document\n\nIndex\n\nPUT /log_data/_doc/1\n{\n  \"@timestamp\": \"2023-05-16T12:34:56Z\",\n  \"log_source\": \"web_server\",\n  \"message\": \"HTTP request received\"\n}\n\nSearch\n\nGET /log_data/_search\nThe response should show the indexed document.\n\n\n\n\n\nIn settings, number_of_replicas doesn’t appear as its default is set to one 1 which is sufficient. The field number_of_shards should be higher than 1 depending on the requirements for a log index. No, you do not need to have a settings block for the index to be created.\nThe @timestamp field is mapped as a date type for time-based data management.\nThe log_source field is mapped as a keyword type to enable custom routing based on its value.\nThere is no routing-specific configuration listed in this example. Routing configurations are often not explicitly defined in the index creation step because Elasticsearch uses default routing based on the document IDs. For specialized use cases, like logs, you might want to define custom routing to optimize query performance or to balance the load more effectively.\n\n\n\n\n\nIn the console execute the following\n\nDELETE log_data \n\n\n\n\ncat indices API\nCreate Index API\nGet mapping API\nMapping Routing Fields\nSearch API\n\n\n\n\n\n\n\n\nStore product information including name, description, category, price, and stock level.\nAllow filtering and searching based on product name, category, and price range.\nEnable aggregations to calculate average price per category.\n\n\n\n\n\nDefine mappings:\n\nUse the text data type for name and description to allow full-text search.\nUse the keyword data type for category to enable filtering by exact terms.\nUse the integer data type for price to allow for range queries and aggregations.\nUse the integer data type for stock_level for inventory management.\n\nOpen the Kibana Console or use a REST client.\nCreate the index\n\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": { \"type\": \"text\" },\n      \"description\": { \"type\": \"text\" },\n      \"category\": { \"type\": \"keyword\" },\n      \"price\": { \"type\": \"integer\" },\n      \"stock_level\": { \"type\": \"integer\" }\n    }\n  }\n}\n\nConfigure analyzers (optional):\n\nYou can define custom analyzers for name and description to handle special characters or stemming based on your needs. Notice two things:\n\nHow the custom_analyzer refers to the filter and tokenizer (both of which are optional).\nThe fields that will use custom_analyzer, name and description, have an analyzer reference to custom_analyzer.\n\n\n\nPUT /products\n{\n  \"settings\": {\n    \"analysis\": {\n      \"tokenizer\": {\n        \"custom_tokenizer\": {\n          \"type\": \"standard\"\n        }\n      },\n      \"filter\": {\n        \"custom_stemmer\": {\n          \"type\": \"stemmer\",\n          \"name\": \"english\"\n        },\n        \"custom_stop\": {\n          \"type\": \"stop\",\n          \"stopwords\": \"_english_\"\n        }\n      },\n      \"analyzer\": {\n        \"custom_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"custom_tokenizer\",\n          \"filter\": [\n            \"lowercase\",\n            \"custom_stop\",\n            \"custom_stemmer\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      },\n      \"description\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"integer\"\n      },\n      \"stock_level\": {\n        \"type\": \"integer\"\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index creation\n\nGET products\nOr\nGET /_cat/indices\n\nVerify the field mapping\n\nGET /products/_mapping\n\nIndex and search some sample product data\n\nIndex some products\n\n\nPOST /products/_bulk\n{ \"index\": { \"_index\": \"products\", \"_id\": \"1\" } }\n{ \"name\": \"Wireless Bluetooth Headphones\", \"description\": \"High-quality wireless Bluetooth headphones with noise-cancellation and long battery life.\", \"category\": \"electronics\", \"price\": 99, \"stock_level\": 250 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"2\" } }\n{ \"name\": \"Stainless Steel Water Bottle\", \"description\": \"Durable stainless steel water bottle, keeps drinks cold for 24 hours and hot for 12 hours.\", \"category\": \"home\", \"price\": 25, \"stock_level\": 500 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"3\" } }\n{ \"name\": \"Smartphone\", \"description\": \"Latest model smartphone with high-resolution display and fast processor.\", \"category\": \"electronics\", \"price\": 699, \"stock_level\": 150 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"4\" } }\n{ \"name\": \"LED Desk Lamp\", \"description\": \"Energy-efficient LED desk lamp with adjustable brightness and flexible neck.\", \"category\": \"home\", \"price\": 45, \"stock_level\": 300 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"5\" } }\n{ \"name\": \"4K Ultra HD TV\", \"description\": \"55-inch 4K Ultra HD TV with HDR support and smart features.\", \"category\": \"electronics\", \"price\": 499, \"stock_level\": 200 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"6\" } }\n{ \"name\": \"Vacuum Cleaner\", \"description\": \"High-suction vacuum cleaner with multiple attachments for versatile cleaning.\", \"category\": \"home\", \"price\": 120, \"stock_level\": 100 }\n\nSearch\n\nGET /products/_search?q=name:desk\n\nUse aggregations to calculate the average price per category.\n\nPOST /products/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"average_price_per_category\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nUsing the appropriate data types ensures efficient storage and querying capabilities.\nText fields allow full-text search, while keyword fields enable filtering by exact terms.\n\n\n\n\n\nIn the console execute the following\n\nDELETE products\n\n\n\n\nAggregations\ncat indices API\nCreate Index API\nGet mapping API\nSearch API\n\n\n\n\n\n\n\n\n\n\n\nCreate an index template named “user_profile_template”.\nThe template should apply to indices starting with “user_profile-”.\nThe template should have two shards and one replica.\nThe template should have a mapping for the “name” field as a text data type with an analyzer of “standard”.\nThe template should have a mapping for the “age” field as an integer data type.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\n\nPUT /_index_template/user_profile_template\n{\n  \"index_patterns\": [\"user_profile-*\"],\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 2,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"name\": {\n          \"type\": \"text\",\n          \"analyzer\": \"standard\"\n        },\n        \"age\": {\n          \"type\": \"integer\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index template was created\n\nGET _index_template/user_profile_template\n\nCreate an index named “user_profile-2024” using the REST API:\n\nPUT /user_profile_2024\n\nVerify that the index was created with the expected settings and mappings:\n\nGET /user_profile_2024/_settings\nGET /user_profile_2024/_mapping\n\n\n\n\nTwo shards are chosen to allow for parallel processing and improved search performance.\nOne replica is chosen for simplicity and development purposes; in a production environment, this would depend on the expected data volume and search traffic.\nThe “standard” analyzer is chosen for the “name” field to enable standard text analysis.\n\n\n\n\n\nIn the console execute the following\n\nDELETE /_index_template/user_profile_template\n\n\n\n\nAnalyzers\nCreate Index API\nIndex templates\n\n\n\n\n\n\n\n\nIndex name pattern: products-*\nIndex settings:\n\nNumber of shards: 3\nNumber of replicas: 2\n\nMapping:\n\nField name should be of type text\nField description should be of type text\nField price should be of type float\nField category should be of type keyword\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\n\nPUT _template/monthly_products\n{\n  \"index_patterns\": [\"products-*\"],\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 2\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index template was created\n\nGET _index_template/monthly_products\n\nCreate a new index matching the pattern (e.g., products-202305):\n\nPUT products-202305\n\nVerify that the index was created with the expected settings and mappings:\n\nGET /products-202305/_settings\nGET /products-202305/_mapping\n\nIndex a sample document and verify that the mapping is applied correctly:\n\nIndex\n\n\nPOST products-202305/_doc\n{\n  \"name\": \"Product A\",\n  \"description\": \"This is a sample product\",\n  \"price\": 19.99,\n  \"category\": \"Electronics\"\n}\n\nSearch\n\nGET products-202305/_search\nThe response should show the correct mapping for the fields specified in the index template.\n\n\n\n\nThe index_patterns field specifies the pattern for index names to which this template should be applied.\nThe number_of_shards and number_of_replicas settings are chosen based on the expected data volume and high availability requirements.\nThe text type is used for name and description fields to enable full-text search and analysis.\nThe float type is used for the price field to support decimal values.\nThe keyword type is used for the category field to prevent analysis and treat the values as exact matches.\n\n\n\n\n\nIn the console execute the following\n\nDELETE products-202305\nDELETE _template/monthly_products\n\n\n\n\nCreate Index API\nIndex templates\nSearch API\n\n\n\n\n\n\n\n\nThe template should apply to any index starting with logs-.\nThe template must define settings for three primary shards and one replica.\nThe template should include mappings for fields timestamp, log_level, and message.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\n\nPUT /_index_template/logs_template\n{\n  \"index_patterns\": [\"logs-*\"],\n  \"template\": {\n    \"settings\": {\n      \"index\": {\n        \"number_of_shards\": 3,\n        \"number_of_replicas\": 1\n      }\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"timestamp\": {\n          \"type\": \"date\"\n        },\n        \"log_level\": {\n          \"type\": \"keyword\"\n        },\n        \"message\": {\n          \"type\": \"text\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index template was created\n\nGET _index_template/logs_template\n\nCreate a new index matching the pattern (e.g., logs-202405)\n\nPUT logs-202405\n\nVerify that the index was created with the expected settings and mappings\n\nGET /logs-202405/_settings\nGET /logs-202405/_mapping\n\nIndex a sample document and verify that the mapping is applied correctly:\n\nIndex\n\n\nPOST logs-202405/_doc\n{\n  \"@timestamp\": \"2024-05-16T12:34:56Z\",\n  \"log_level\": \"ERROR\",\n  \"message\": \"Help!\"\n}\n\nSearch\n\nGET logs-202405/_search\nThe response should show the correct mapping for the fields specified in the index template.\n\n\n\n\nIndex Patterns: The template applies to any index starting with logs-, ensuring consistency across similar indices.\nNumber of Shards: Three shards provide a balance between performance and resource utilization.\nReplicas: A single replica ensures high availability and fault tolerance.\nMappings: Predefined mappings ensure that the fields are properly indexed and can be efficiently queried.\n\n\n\n\n\nIn the console execute the following\n\nDELETE logs-202405\nDELETE _index_template/logs_template\n\n\n\n\nCreate Index API\nLogstash: Event Dependent Configuration\nIndex templates\nSearch API\n\n\n\n\n\n\nFYI: The difference between index templates and dynamic templates is: * An index template is a way to define settings, mappings, and other configurations that should be applied automatically to new indices when they are created. * A dynamic template is part of the mapping definition within an index template or index mapping that allows Elasticsearch to dynamically infer the mapping of fields based on field names, data patterns, or the data type detected.\nThere is one example per field mapping type. They all use an explicit dynamic template, but Exercise 1 also shows the use of a dynamic template embedded in the index definition.\n\n\n\n\n\nApply a specific text analysis to all fields that end with _log.\nUse a keyword type for all fields that start with status_.\nDefault to text with a standard analyzer for other string fields.\nDefine a custom log_analyzer for _log fields.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nDefine the dynamic template\n\n\nAs part of the index definition\n\nPUT /logs_index\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"log_fields\": {\n          \"match\": \"*_log\",\n          \"mapping\": {\n            \"type\": \"text\",\n            \"analyzer\": \"log_analyzer\"\n          }\n        }\n      },\n      {\n        \"status_fields\": {\n          \"match\": \"status_*\",\n          \"mapping\": {\n            \"type\": \"keyword\"\n          }\n        }\n      },\n      {\n        \"default_string\": {\n          \"match_mapping_type\": \"string\",\n          \"mapping\": {\n            \"type\": \"text\",\n            \"analyzer\": \"standard\"\n          }\n        }\n      }\n    ]\n  },\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"log_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\"lowercase\", \"stop\"]\n        }\n      }\n    }\n  }\n}\n\nor as a standalone definition to be added to indexes as needed using the index_pattern\n\nPUT /_index_template/logs_dyn_template\n{\n  \"index_patterns\": [\"logs_*\"],\n  \"template\": {\n    \"mappings\": {\n      \"dynamic_templates\": [\n        {\n          \"log_fields\": {\n            \"match\": \"*_log\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"log_analyzer\"\n            }\n          }\n        },\n        {\n          \"status_fields\": {\n            \"match\": \"status_*\",\n            \"mapping\": {\n              \"type\": \"keyword\"\n            }\n          }\n        },\n        {\n          \"default_string\": {\n            \"match_mapping_type\": \"string\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"standard\"\n            }\n          }\n        }\n      ]\n    },\n    \"settings\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"log_analyzer\": {\n            \"type\": \"custom\",\n            \"tokenizer\": \"standard\",\n            \"filter\": [\"lowercase\", \"stop\"]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the dynamic template was created\n\nIf you used the embedded version\n\n\nGET /logs_index/_mapping\n\nIf you used the standalone version\n\nGET /_index_template/logs_dyn_template\n\nCreate a new index matching the pattern (e.g., logs-202405)\n\nOptional if you used the embedded version\n\n\nPUT logs_index\n\nVerify that the created index has the expected settings and mappings\n\nEnsure error_log is of type text with log_analyzer\nEnsure status_code is of type keyword\nEnsure message is of type text with standard analyzer\n\n\nGET /logs_index/_mapping\n\nIndex a sample document and verify that the mapping is applied correctly\n\nPOST /logs_index/_doc/1\n{\n  \"error_log\": \"This is an error log message.\",\n  \"status_code\": \"200\",\n  \"message\": \"Regular log message.\"\n}\n\nPerform Searches:\n\nSearch within error_log and verify the custom analyzer is applied\n\n\nGET /logs_index/_search\n{\n  \"query\": {\n    \"match\": {\n      \"error_log\": \"error\"\n    }\n  }\n}\n\nCheck if status_code is searchable as a keyword\n\nGET /logs_index/_search\n{\n  \"query\": {\n    \"term\": {\n      \"status_code\": \"200\"\n    }\n  }\n}\n\n\n\n\nThe custom analyzer log_analyzer is used to provide specific tokenization and filtering for log fields.\nThe keyword type for status_* fields ensures they are treated as exact values, useful for status codes.\nThe default_string template ensures other string fields are analyzed with the standard analyzer, providing a balanced default.\n\n\n\n\n\nDelete the index\n\nDELETE logs_index\n\nDelete the dynamic template\n\nDELETE /_index_template/logs_dyn_template\n\n\n\n\nDynamic Templates\nCustom Analyzers\nPut Mapping API\nIndex API\nSearch API\n\n\n\n\n\n\n\n\nAll string fields should be treated as text with a standard analyzer.\nAll long fields should be treated as integer.\nAll date fields should use a specific date format.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nDefine the Dynamic Template\n\nPUT /_index_template/data_type_template\n{\n  \"index_patterns\": [\"data_type_*\"],\n  \"template\": {\n    \"mappings\": {\n      \"dynamic_templates\": [\n        {\n          \"strings_as_text\": {\n            \"match_mapping_type\": \"string\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"standard\"\n            }\n          }\n        },\n        {\n          \"longs_as_integer\": {\n            \"match_mapping_type\": \"long\",\n            \"mapping\": {\n              \"type\": \"integer\"\n            }\n          }\n        },\n        {\n          \"dates_with_format\": {\n            \"match_mapping_type\": \"date\",\n            \"mapping\": {\n              \"type\": \"date\",\n              \"format\": \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nVerify the dynamic template was created\n\nGET /_index_template/data_type_template\n\nCreate a new index matching the pattern\n\nPUT data_type_202405\n\nCheck the Field Types\n\nVerify that all string fields are mapped as text with the standard analyzer.\nVerify that all long fields are mapped as integer.\nVerify that all date fields are mapped with the correct format.\n\n\nGET /data_type_202405/_mapping\n\nInsert sample documents to ensure that the dynamic template is applied correctly\n\nPOST /data_type_202405/_bulk\n{ \"index\": { \"_index\": \"data_type_202405\", \"_id\": \"1\" } }\n{ \"name\": \"Wireless Bluetooth Headphones\", \"release_date\": \"2024-05-28T14:35:00.000Z\", \"price\": 99 }\n{ \"index\": { \"_index\": \"data_type_202405\", \"_id\": \"2\" } }\n{ \"description\": \"Durable stainless steel water bottle\", \"launch_date\": \"2024-05-28T15:00:00.000Z\", \"quantity\": 500 }\n\nPerform Searches\n\nSearch launch_date\n\n\nGET /data_type_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"launch_date:\\\"2024-05-28T15:00:00.000Z\\\"\"\n    }\n  }\n}\n\nCheck if price is searchable as a value\n\nGET /data_type_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"price: 99\"\n    }\n  }\n}\n\n\n\n\nDynamic Templates: Using dynamic templates based on data types allows for flexible and consistent field mappings without needing to know the exact field names in advance.\nData Types: Matching on data types (string, long, date) ensures that fields are mapped appropriately based on their content.\nDate Format: Specifying the date format ensures that date fields are parsed correctly, avoiding potential issues with date-time representation.\n\n\n\n\n\nDelete the index\n\nDELETE data_type_202405\n\nDelete the dynamic template\n\nDELETE /_index_template/data_type_template\n\n\n\n\nDynamic Templates\nMapping\nAnalyzers\n\n\n\n\n\n\n\n\nAutomatically map fields that end with “_ip” as IP type.\nMap fields that start with “timestamp_” as date type.\nMap any field containing the word “keyword” as a keyword type.\nUse a custom analyzer for fields ending with “_text”.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the dynamic template\n\nPUT /_index_template/logs_template\n{\n  \"index_patterns\": [\"logs*\"],\n  \"template\": {\n    \"settings\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"custom_analyzer\": {\n            \"type\": \"standard\",\n            \"stopwords\": \"_english_\"\n          }\n        }\n      }\n    },\n    \"mappings\": {\n      \"dynamic_templates\": [\n        {\n          \"ip_fields\": {\n            \"match\": \"*_ip\",\n            \"mapping\": {\n              \"type\": \"ip\"\n            }\n          }\n        },\n        {\n          \"date_fields\": {\n            \"match\": \"timestamp_*\",\n            \"mapping\": {\n              \"type\": \"date\"\n            }\n          }\n        },\n        {\n          \"keyword_fields\": {\n            \"match\": \"*keyword*\",\n            \"mapping\": {\n              \"type\": \"keyword\"\n            }\n          }\n        },\n        {\n          \"text_fields\": {\n            \"match\": \"*_text\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"custom_analyzer\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nVerify the dynamic template was created\n\nGET /_index_template/logs_template\n\nCreate a new index matching the pattern\n\nPUT logs_202405\n\nCheck the Field Types\n\nVerify that all _ip fields are mapped as ip\nVerify that all timestamp_* fields are mapped as date\nVerify that all keyword fields are mapped as keyword\n\n\nGET /logs_202405/_mapping\n\nInsert sample documents to ensure that the dynamic template is applied correctly\n\nPOST /logs_202405/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"source_ip\": \"192.168.1.1\", \"timestamp_event\": \"2024-05-28T12:00:00Z\", \"user_keyword\": \"elastic\", \"description_text\": \"This is a log entry.\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"destination_ip\": \"10.0.0.1\", \"timestamp_access\": \"2024-05-28T12:05:00Z\", \"log_keyword\": \"search\", \"details_text\": \"Another log entry.\" }\n\nPerform Searches\n\nSearch source_ip\n\n\nGET /logs_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"source_ip:\\\"192.168.1.1\\\"\"\n    }\n  }\n}\n\nCheck if timestamp_event is searchable as a date\n\nGET /logs_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"timestamp_event:\\\"2024-05-28T12:00:00Z\\\"\"\n    }\n  }\n}\n\n\n\n\nThe use of patterns in the dynamic template ensures that newly added fields matching the criteria are automatically mapped without the need for manual intervention.\nCustom analyzer configuration is critical for ensuring text fields are processed correctly, enhancing search capabilities.\n\n\n\n\n\nDelete the index\n\nDELETE logs_202405\n\nDelete the dynamic template\n\nDELETE /_index_template/logs_template\n\n\n\n\nDynamic Templates\nMapping Types\nAnalyzers\n\n\n\n\n\n\n\n\n\n\n\nIndices are prefixed with “logstash-” and a datestamp (e.g. logstash-2024.05.16).\nIndices should be rolled over daily (create a new index every day).\nOld indices should be deleted after 30 days.\n\n\n\n\n\nOpen the hamburger menu and click on Management → Life Cycle Policies.\nPress + Create New Policy.\nEnter the following:\n\nPolicy name: logstash-example-policy.\nHot phase:\n\nChange Keep Data in the Phase Forever (the infinity icon) to Delete Data After This Phase (the trashcan icon).\nClick Advanced Settings.\nUnselect Use Recommended Defaults.\nSet Maximum Age to 1.\n\nDelete phase:\n\nMove data into phase when: 30 days old.\n\n\nPress Save Policy.\nOpen the Kibana Console or use a REST client.\nCreate an index template that will match on indices that match the pattern “logstash-*“.\n\nPUT /_index_template/ilm_logstash_index_template\n{\n  \"index_patterns\": [\"logstash-*\"]\n}\n\nReturn to the Management → Life Cycle Policies page.\nPress the plus sign (+) to the right of logstash-example-policy.\n\nThe Add Policy “logstash-example-policy” to index template dialog opens.\nClick on the Index Template input field and type the first few letters of the index template created above.\nSelect the template created above (ilm_logstash_index_template).\nPress Add Policy.\n\nOpen the Kibana Console or use a REST client.\nList ilm_logs_index_template. Notice the ILM policy is now part of the index template.\n\nGET /_index_template/ilm_logstash_index_template\nOutput from the GET:\n{\n  \"index_templates\": [\n    {\n      \"name\": \"ilm_logstash_index_template\",\n      \"index_template\": {\n        \"index_patterns\": [\"logstash-*\"],\n        \"template\": {\n          \"settings\": {\n            \"index\": {\n              \"lifecycle\": {\n                \"name\": \"logstash-example-policy\"\n              }\n            }\n          }\n        },\n        \"composed_of\": []\n      }\n    }\n  ]\n}\n\nCreate an index.\n\nPUT logstash-2024.05.16\n\nVerify the policy is there.\n\nGET logstash-2024.05.16\nThe output should look something like this:\n{\n  \"logstash-2024.05.16\": {\n    \"aliases\": {},\n    \"mappings\": {},\n    \"settings\": {\n      \"index\": {\n        \"lifecycle\": {\n          \"name\": \"logstash-example-policy\"\n        },\n        \"routing\": {\n          \"allocation\": {\n            \"include\": {\n              \"_tier_preference\": \"data_content\"\n            }\n          }\n        },\n        \"number_of_shards\": \"1\",\n        \"provided_name\": \"logstash-2024.05.16\",\n        \"creation_date\": \"1717024100387\",\n        \"priority\": \"100\",\n        \"number_of_replicas\": \"1\",\n        \"uuid\": \"mslAKuZGTpSDdFr4hSpAAA\",\n        \"version\": {\n          \"created\": \"8503000\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the ILM policy.\n\nPUT _ilm/policy/logstash-example-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_age\": \"1d\"\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n\nCreate an index template that includes the above policy. The two fields within settings are required.\n\nPUT /_index_template/ilm_logstash_index_template\n{\n  \"index_patterns\": [\"logstash-*\"],\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"logstash-example-policy\",\n      \"index.lifecycle.rollover_alias\": \"logstash\"\n    }\n  }\n}\n\n\n\n\nVerify the ILM policy exists in Kibana under Management → Index Lifecycle Policies.\nVerify the Index Lifecycle Management policy exists and references the index template.\n\nGET /_ilm/policy/logstash-example-policy\n\nVerify the policy is referenced in the index template.\n\nGET /_index_template/ilm_logstash_index_template\n\nCreate a new index that matches the pattern logstash-*.\n\nPUT /logstash-index\n\nVerify the index has the policy in its definition.\n\nGET /logstash-index\n\n\n\n\nThe index template configures 1 shard and the ILM policy/alias for rollover.\nThe rollover action creates a new index when the max_age is reached.\nThe delete phase removes indices older than 30 days.\n\n\n\n\n\nDelete the index.\n\nDELETE logstash-index\n\nDelete the index template.\n\nDELETE /_index_template/ilm_logstash_index_template\n\nDelete the policy.\n\nDELETE /_ilm/policy/logstash-example-policy\n\n\n\n\nCreate Index API\nCreate or Update Index Template API\nILM Settings\nIndex Lifecycle Management\n\n\n\n\n\n\n\n\nThe policy should be named “logs-policy”.\nIt should have a hot phase with a duration of 7 days.\nIt should have a warm phase with a duration of 30 days.\nIt should have a cold phase with a duration of 90 days.\nIt should have a delete phase.\nThe policy should be assigned to indices matching the pattern “ilm_logs_*“.\n\nSteps using the Elastic/Kibana UI\n\nOpen the hamburger menu and click on Management → Life Cycle Policies.\nPress + Create New Policy.\nEnter the following:\n\nPolicy name: logs-policy.\nHot phase:\n\nPress the garbage can icon to the right to delete data after this phase.\n\nWarm phase:\n\nMove data into phase when: 7 days old.\nLeave Delete data after this phase.\n\nCold phase:\n\nMove data into phase when: 30 days old.\nLeave Delete data after this phase.\n\nDelete phase:\n\nMove data into phase when: 90 days old.\n\n\nPress Save Policy.\nOpen the Kibana Console or use a REST client.\nCreate an index template that will match on indices that match the pattern “ilm_logs_*“.\n\nPUT /_index_template/ilm_logs_index_template\n{\n  \"index_patterns\": [\"ilm_logs_*\"]\n}\n\nReturn to the Management → Life Cycle Policies page.\nPress the plus sign (+) to the right of logs_policy.\nThe Add Policy “logs-policy” to index template dialog opens.\nClick on the Index Template input field and type the first few letters of the index template created above.\nSelect the template created above (ilm_logs_index_template).\nPress Add Policy.\nOpen the Kibana Console or use a REST client.\nList ilm_logs_index_template. Notice the ILM policy is now part of the index template.\n\nGET /_index_template/ilm_logs_index_template\nOutput from the GET:\n{\n  \"index_templates\": [\n    {\n      \"name\": \"ilm_logs_index_template\",\n      \"index_template\": {\n        \"index_patterns\": [\"ilm_logs_*\"],\n        \"template\": {\n          \"settings\": {\n            \"index\": {\n              \"lifecycle\": {\n                \"name\": \"logs-policy\"\n              }\n            }\n          }\n        },\n        \"composed_of\": []\n      }\n    }\n  ]\n}\n\nList logs-policy.\n\nGET _ilm/policy/logs-policy\nIn the in_use_by node you will see:\n\"in_use_by\": {\n  \"indices\": [],\n  \"data_streams\": [],\n  \"composable_templates\": [\n    \"ilm_logs_index_template\"\n  ]\n}\nSteps Using the REST API\n\nOpen the Kibana Console or use a REST client.\nCreate the ILM policy.\n\nPUT _ilm/policy/logs-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"min_age\": \"0ms\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 100\n          }\n        }\n      },\n      \"warm\": {\n        \"min_age\": \"7d\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 50\n          }\n        }\n      },\n      \"cold\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 0\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"90d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n\nAssign the policy to the indices matching the pattern “logs_*“.\n\nPUT /_index_template/ilm_logs_index_template\n{\n  \"index_patterns\": [\"ilm_logs_*\"],\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"logs-policy\",\n      \"index.lifecycle.rollover_alias\": \"logs\"\n    }\n  }\n}\n\n\n\n\nVerify the ILM policy exists in Kibana under Management → Index Lifecycle Policies.\nVerify the Index Lifecycle Management policy exists and references the index template.\n\nGET /_ilm/policy/logs-policy\n\nVerify the policy is referenced in the index template.\n\nGET /_index_template/ilm_logs_index_template\n\nCreate a new index that matches the pattern ilm_logs_*.\n\nPUT /ilm_logs_index\n\nVerify the index has the policy in its definition.\n\nGET /ilm_logs_index\n\n\n\n\nThe ILM policy will manage the indices matching the pattern ilm_logs_*.\nThe hot phase will keep the data for 7 days with high priority and rollover.\nThe warm phase will keep the data for 30 days with medium priority.\nThe cold phase will keep the data for 90 days with low priority.\nThe ILM policy will automatically manage the indices based on their age and size.\nThe policy can be adjusted based on the needs of the application and the data.\n\n\n\n\n\nDelete the index.\n\nDELETE ilm_logs_index\n\nDelete the index template.\n\nDELETE _index_template/ilm_logs_index_template\n\nDelete the policy.\n\nDELETE _ilm/policy/logs-policy\n\n\n\n\nCreate Index API\nCreate or Update Index Template API\nILM Settings\nIndex Lifecycle Management\n\n\n\n\n\n\n\n\nCreate a new index every day for sensor data (e.g., sensor_data-{date}).\nAutomatically roll over to a new index when the current one reaches a specific size.\nDelete rolled over indices after one month.\n\nSteps using the Elastic/Kibana UI\n\nOpen the hamburger menu and click on Management → Life Cycle Policies.\nPress + Create New Policy.\nEnter the following:\n\nPolicy name: sensor-data-policy.\nHot phase:\n\nChange Keep Data in the Phase Forever (the infinity icon) to Delete Data After This Phase (the trashcan icon).\nClick Advanced Settings.\nUnselect Use Recommended Defaults.\nSet Maximum Age to 1.\nSet Maximum Index Size to 10.\n\nDelete phase:\n\nMove data into phase when: 30 days old.\n\n\nPress Save Policy.\nOpen the Kibana Console or use a REST client.\nCreate an index template that will match on indices that match the pattern “sensor_data-*“.\n\nPUT /_index_template/sensor_data_index_template\n{\n  \"index_patterns\": [\"sensor_data-*\"]\n}\n\nReturn to the Management → Life Cycle Policies page.\nPress the plus sign (+) to the right of sensor-data-policy.\nThe Add Policy “sensor-data-policy” to index template dialog opens.\n\nClick on the Index Template input field and type the first few letters of the index template created above.\nSelect the template created above (sensor_data_index_template).\nPress Add Policy.\n\n\n\n\nOpen the Kibana Console or use a REST client.\nList sensor_data_index_template. Notice the ILM policy is now part of the index template.\n\nGET /_index_template/sensor_data_index_template\nOutput from the GET:\n{\n  \"index_templates\": [\n    {\n      \"name\": \"sensor_data_index_template\",\n      \"index_template\": {\n        \"index_patterns\": [\"sensor_data-*\"],\n        \"template\": {\n          \"settings\": {\n            \"index\": {\n              \"lifecycle\": {\n                \"name\": \"sensor-data-policy\"\n              }\n            }\n          }\n        },\n        \"composed_of\": []\n      }\n    }\n  ]\n}\n\nList sensor-data-policy.\n\nGET /_ilm/policy/sensor-data-policy\nIn the in_use_by node you will see:\n\"in_use_by\": {\n  \"indices\": [],\n  \"data_streams\": [],\n  \"composable_templates\": [\n    \"sensor_data_index_template\"\n  ]\n}\nOR\nSteps Using the REST API\n\nOpen the Kibana Console or use a REST client.\nDefine the ILM policy.\n\nPUT _ilm/policy/sensor-data-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"min_age\": \"0ms\",\n        \"actions\": {\n          \"rollover\": {\n            \"max_age\": \"1d\",\n            \"max_size\": \"10gb\"\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n\nAssign the policy to the indices matching the pattern “sensor_data-*“.\n\nPUT /_index_template/sensor_data_index_template\n{\n  \"index_patterns\": [\"sensor_data-*\"],\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"sensor-data-policy\",\n      \"index.lifecycle.rollover_alias\": \"sensor\"\n    }\n  }\n}\n\n\n\n\nVerify the ILM policy exists in Kibana under Management → Index Lifecycle Policies.\nVerify the Index Lifecycle Management policy exists and references the index template.\n\nGET /_ilm/policy/sensor-data-policy\n\nVerify the policy is referenced in the index template.\n\nGET /_index_template/sensor_data_index_template\n\nCreate a new index that matches the pattern sensor_data-*.\n\nPUT /sensor_data-20240516\n\nVerify the index has the policy in its definition.\n\nGET /sensor_data-20240516\n\n\n\n\nThe hot phase size threshold determines the frequency of rollovers.\nThe delete phase retention period defines how long rolled over data is stored.\n\n\n\n\n\nDelete the index.\n\nDELETE sensor_data-20240516\n\nDelete the index template.\n\nDELETE /_index_template/sensor_data_index_template\n\nDelete the policy.\n\nDELETE /_ilm/policy/sensor-data-policy\n\n\n\n\nCreate Index API\nCreate or Update Index Template API\nILM Settings\nIndex Lifecycle Management\n\n\n\n\n\n\nData streams in Elasticsearch are used for managing time-series data such as logs, metrics, and events. They can handle large volumes of time-series data in an efficient and scalable manner.\n\n\n\n\n\nCreate a new data stream named “app-logs” to store application logs.\nAutomatically create new backing indices within the data stream as needed.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nDefine the index template that will be used by the data stream to create new backing indices.\n\nPUT _index_template/app_logs_index_template\n{\n  \"index_patterns\": [\"app_logs*\"],\n  \"data_stream\": {}\n}\n\n\n\n\nVerify the index template creation.\n\nGET _index_template/app_logs_index_template\n\nConfirm there are no indices named app_logs*.\n\nGET /_cat/indices\n\nMock sending streaming data by just pushing a few documents to the stream. When sending documents using _bulk, they must use create instead of index. In addition, the documents must have a @timestamp field.\n\nPOST app_logs/_bulk\n{ \"create\":{} }\n{ \"@timestamp\": \"2099-05-06T16:21:15.000Z\", \"message\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\" }\n{ \"create\":{} }\n{ \"@timestamp\": \"2099-05-06T16:25:42.000Z\", \"message\": \"192.0.2.255 - - [06/May/2099:16:25:42 +0000] \\\"GET /favicon.ico HTTP/1.0\\\" 200 3638\" }\nThe response will list the name of the automatically created index, which will look something like this:\n{\n  \"errors\": false,\n  \"took\": 8,\n  \"items\": [\n    {\n      \"create\": {\n        \"_index\": \".ds-app_logs-2099.05.06-000001\",\n        \"_id\": \"OOazyo8BAvAOn4WaAfdD\",\n        \"_version\": 1,\n        \"result\": \"created\",\n        \"_shards\": {\n          \"total\": 2,\n          \"successful\": 1,\n          \"failed\": 0\n        },\n        \"_seq_no\": 2,\n        \"_primary_term\": 1,\n        \"status\": 201\n      }\n    },\n    {\n      \"create\": {\n        \"_index\": \".ds-app_logs-2099.05.06-000001\",\n        \"_id\": \"Oeazyo8BAvAOn4WaAfdD\",\n        \"_version\": 1,\n        \"result\": \"created\",\n        \"_shards\": {\n          \"total\": 2,\n          \"successful\": 1,\n          \"failed\": 0\n        },\n        \"_seq_no\": 3,\n        \"_primary_term\": 1,\n        \"status\": 201\n      }\n    }\n  ]\n}\n\nRun:\n\nGET /_cat/indices\nYou will see the new index listed. This is the backing index created by the data stream.\n\nCheck for the app_logs data stream under Management → Index Management → Data Streams.\nVerify that the documents were indexed.\n\nGET .ds-app_logs-2099.05.06-000001\n\n\n\n\nData streams provide a more efficient way to handle continuously flowing data compared to daily indices. They are created implicitly through the use of index templates, and you must use the **_bulk API** (including the use of create) when streaming data.\nNew backing indices are automatically created within the data stream as needed.\nLifecycle management policies can be applied to data streams for automatic deletion of older backing indices.\n\n\n\n\n\nDelete the data stream (deleting the data stream will also delete the backing index).\n\nDELETE /_data_stream/app_logs\n\nDelete the index template.\n\nDELETE _index_template/app_logs_index_template\n\n\n\n\nIndex Templates\nSetting Up a Data Stream\n\n\n\n\n\n\n\n\nThe template should apply to any index matching the pattern logs*.\nThe template must create a data stream.\nThe template should define settings for two primary shards and one replica.\nThe template should include mappings for fields @timestamp, log_level, and message.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\n\nPUT _index_template/log_application_index_template\n{\n  \"index_patterns\": [\"logs*\"],\n  \"data_stream\": {},\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 2,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\"\n        },\n        \"log_level\": {\n          \"type\": \"keyword\"\n        },\n        \"message\": {\n          \"type\": \"text\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index template creation\n\nGET _index_template/log_application_index_template\n\nConfirm there are no indices named logs*\n\nGET /_cat/indices\n\nIndex documents into the data stream\n\nPOST /logs/_doc\n{\n  \"@timestamp\": \"2024-05-16T12:34:56\",\n  \"log_level\": \"info\",\n  \"message\": \"Test log message\"\n}\nThis will return a result with the name of the backing index\n{\n  \"_index\": \".ds-logs-2024.05.16-000001\", // yours will be different\n  \"_id\": \"PObWyo8BAvAOn4WaC_de\",\n  \"_version\": 1,\n  \"result\": \"created\",\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 1,\n    \"failed\": 0\n  },\n  \"_seq_no\": 0,\n  \"_primary_term\": 1\n}\nRun\nGET /_cat/indices\nThe index will be listed.\n\nConfirm the configuration of the backing index matches the index template (your backing index name will be different)\n\nGET .ds-logs-2024.05.16-000001\n\nRun a search for the document that was indexed\n\nGET .ds-logs-2024.05.16-000001/_search\n\n\n\n\nData streams provide a more efficient way to handle continuously flowing data compared to daily indices. They are created implicitly through the use of index templates, and you must use the **_bulk API** (including the use of create) when streaming data.\nNew backing indices are automatically created within the data stream as needed.\nLifecycle management policies can be applied to data streams for automatic deletion of older backing indices (not shown but there is an example at Set Up a Data Stream).\n\n\n\n\n\nDelete the data stream (deleting the data stream will also delete the backing index)\n\nDELETE _data_stream/logs\n\nDelete the index template\n\nDELETE _index_template/log_application_index_template\n\n\n\n\nIndex Templates\nSetting Up a Data Stream\n\n\n\n\n\n\n\n\nCreate an index template named “metrics_template”.\nThe template should create a new data stream named “metrics-{suffix}”.\nThe template should have one shard and one replica.\nThe template should have a mapping for the “metric” field as a keyword data type.\nThe template should have a mapping for the “value” field as a float data type.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index template.\n\nPUT _index_template/metrics_template\n{\n  \"index_patterns\": [\"metrics-*\"],\n  \"data_stream\": {},\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"metric\": {\n          \"type\": \"keyword\"\n        },\n        \"value\": {\n          \"type\": \"float\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index template creation.\n\nGET _index_template/metrics_template\n\nConfirm there are no indices named metrics-*.\n\nGET /_cat/indices\n\nIndex documents into the data stream.\n\nPOST /metrics-ds/_doc\n{\n  \"@timestamp\": \"2024-05-16T12:34:56\",\n  \"metric\": \"cpu\",\n  \"value\": 0.5\n}\nNotice the use of the @timestamp field. That is required for any documents going into a data stream.\n\nThis will return a result with the name of the backing index.\n\n{\n  \"_index\": \".ds-metrics-ds-2024.05.16-000001\", // yours will be different\n  \"_id\": \"P-YFy48BAvAOn4WaUvef\",\n  \"_version\": 1,\n  \"result\": \"created\",\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 1,\n    \"failed\": 0\n  },\n  \"_seq_no\": 1,\n  \"_primary_term\": 1\n}\n\nRun:\n\nGET /_cat/indices\n\nThe index will be listed.\nConfirm the configuration of the backing index matches the index template (your backing index name will be different).\n\nGET .ds-metrics-ds-2024.05.16-000001\n\nRun a search for the document that was indexed.\n\nGET .ds-metrics-ds-2024.05.16-000001/_search\n\n\n\n\nThe “keyword” data type is chosen for the “metric” field to enable exact matching and filtering.\nThe “float” data type is chosen for the “value” field to enable precise numerical calculations.\nOne shard and one replica are chosen for simplicity and development purposes; in a production environment, this would depend on the expected data volume and search traffic.\n\n\n\n\n\nDelete the data stream (deleting the data stream will also delete the backing index).\n\nDELETE _data_stream/metrics-ds\n\nDelete the index template.\n\nDELETE _index_template/metrics_template\n\n\n\n\nIndex templates\nData streams\nMapping types"
  },
  {
    "objectID": "1-data-management.html#task-define-an-index-that-satisfies-a-given-set-of-requirements",
    "href": "1-data-management.html#task-define-an-index-that-satisfies-a-given-set-of-requirements",
    "title": "Data Management",
    "section": "",
    "text": "The platform hosts articles, each with text content, a publication date, author details, and tags.\nArticles need to be searchable by content, title, and tags.\nThe application requires fast search responses and efficient storage.\nThe application should handle date-based queries efficiently.\nAuthor details are nested objects that include the author’s name and email.\n\n\n\n\n\nChoose Appropriate Settings:\n\nUse a custom analyzer for the articles’ content to improve text search.\nSet the number of shards to balance between write performance and search speed.\nConfigure replicas for high availability and read performance.\n\nDefine Mappings:\n\nContent and Title: Use the text data type with a custom analyzer.\nPublication Date: Use the date data type.\nTags: Use the keyword data type for exact matching.\nAuthor: Use a nested object to keep author details searchable and well-structured.\n\nOpen the Kibana Console or use a REST client.\nCreate the index\n\nPUT blog_articles\n{\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 1,\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_text_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_text_analyzer\"\n      },\n      \"content\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_text_analyzer\"\n      },\n      \"publication_date\": {\n        \"type\": \"date\"\n      },\n      \"author\": {\n        \"type\": \"nested\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"text\",\n            \"analyzer\": \"custom_text_analyzer\"\n          },\n          \"email\": {\n            \"type\": \"keyword\"\n          }\n        }\n      },\n      \"tags\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\nOr insert the settings and mappings separately.\nPUT blog_articles\n{\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 1,\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_text_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  }\n}\nAnd:\nPUT blog_articles/_mapping\n{\n  \"properties\": {\n    \"title\": {\n      \"type\": \"text\",\n      \"analyzer\": \"custom_text_analyzer\"\n    },\n    \"content\": {\n      \"type\": \"text\",\n      \"analyzer\": \"custom_text_analyzer\"\n    },\n    \"publication_date\": {\n      \"type\": \"date\"\n    },\n    \"author\": {\n      \"type\": \"nested\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"text\",\n          \"analyzer\": \"custom_text_analyzer\"\n        },\n        \"email\": {\n          \"type\": \"keyword\"\n        }\n      }\n    },\n    \"tags\": {\n      \"type\": \"text\"\n    }\n  }\n}\n\n\n\n\nVerify the index\n\nGET /_cat/indices\n\nVerify the mappings\n\nGET /blog_articles/_mapping\n\nIndex and search for a document\n\n# Index\nPOST /blog_articles/_doc\n{\n  \"title\" : \"My First Blog Post\",\n  \"content\" : \"What an interesting way to go...\",\n  \"publication_date\" : \"2024-05-15\",\n  \"tags\" : \"superb\",\n  \"author\" : {\n    \"name\" : \"John Doe\",\n    \"email\" : \"john@doe.com\"\n  }\n}\n# Search like this\nGET /blog_articles/_search\n# Or search like this\nGET /blog_articles/_search?q=tags:superb\n# Or search like this\nGET blog_articles/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"default_field\": \"tags\",\n      \"query\": \"superb\"\n    }\n  }\n}\n# Or search like this\nGET blog_articles/_search\n{\n  \"query\": {\n    \"nested\": {\n      \"path\": \"author\",\n      \"query\": {\n        \"match\": {\n          \"author.name\": \"john\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nAnalyzers: Custom analyzers are used for content and title to optimize for search effectiveness. While we create a custom analyzer it was not part of the requirements.\nShards and Replicas: Adjust these settings based on expected data volume and query load.\nNested Objects: These are crucial for maintaining the structure and searchability of complex data like author details.\n\n\n\n\n\nIn the console execute the following\n\nDELETE blog_articles\n\n\n\n\ncat indices API\nCreate Index API\nCustom Analyzer\nGet mapping API\nSearch API\n\n\n\n\n\n\n\n\nStore log data with a timestamp field\nEnable time-based data management\nUse a custom routing value based on the log source\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index\n\nPUT /log_data\n{\n  \"settings\": {\n    \"number_of_shards\": 3\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"@timestamp\": {\n        \"type\": \"date\"\n      },\n      \"log_source\": {\n        \"type\": \"keyword\"\n      },\n      \"message\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index creation\n\nGET /log_data\nOr\nGET /_cat/indices\n\nVerify the field mapping\n\nGET /log_data/_mapping\n\nIn order for the indexing and searching of example docs to work\nIndex and search for a sample document\n\nIndex\n\nPUT /log_data/_doc/1\n{\n  \"@timestamp\": \"2023-05-16T12:34:56Z\",\n  \"log_source\": \"web_server\",\n  \"message\": \"HTTP request received\"\n}\n\nSearch\n\nGET /log_data/_search\nThe response should show the indexed document.\n\n\n\n\n\nIn settings, number_of_replicas doesn’t appear as its default is set to one 1 which is sufficient. The field number_of_shards should be higher than 1 depending on the requirements for a log index. No, you do not need to have a settings block for the index to be created.\nThe @timestamp field is mapped as a date type for time-based data management.\nThe log_source field is mapped as a keyword type to enable custom routing based on its value.\nThere is no routing-specific configuration listed in this example. Routing configurations are often not explicitly defined in the index creation step because Elasticsearch uses default routing based on the document IDs. For specialized use cases, like logs, you might want to define custom routing to optimize query performance or to balance the load more effectively.\n\n\n\n\n\nIn the console execute the following\n\nDELETE log_data \n\n\n\n\ncat indices API\nCreate Index API\nGet mapping API\nMapping Routing Fields\nSearch API\n\n\n\n\n\n\n\n\nStore product information including name, description, category, price, and stock level.\nAllow filtering and searching based on product name, category, and price range.\nEnable aggregations to calculate average price per category.\n\n\n\n\n\nDefine mappings:\n\nUse the text data type for name and description to allow full-text search.\nUse the keyword data type for category to enable filtering by exact terms.\nUse the integer data type for price to allow for range queries and aggregations.\nUse the integer data type for stock_level for inventory management.\n\nOpen the Kibana Console or use a REST client.\nCreate the index\n\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": { \"type\": \"text\" },\n      \"description\": { \"type\": \"text\" },\n      \"category\": { \"type\": \"keyword\" },\n      \"price\": { \"type\": \"integer\" },\n      \"stock_level\": { \"type\": \"integer\" }\n    }\n  }\n}\n\nConfigure analyzers (optional):\n\nYou can define custom analyzers for name and description to handle special characters or stemming based on your needs. Notice two things:\n\nHow the custom_analyzer refers to the filter and tokenizer (both of which are optional).\nThe fields that will use custom_analyzer, name and description, have an analyzer reference to custom_analyzer.\n\n\n\nPUT /products\n{\n  \"settings\": {\n    \"analysis\": {\n      \"tokenizer\": {\n        \"custom_tokenizer\": {\n          \"type\": \"standard\"\n        }\n      },\n      \"filter\": {\n        \"custom_stemmer\": {\n          \"type\": \"stemmer\",\n          \"name\": \"english\"\n        },\n        \"custom_stop\": {\n          \"type\": \"stop\",\n          \"stopwords\": \"_english_\"\n        }\n      },\n      \"analyzer\": {\n        \"custom_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"custom_tokenizer\",\n          \"filter\": [\n            \"lowercase\",\n            \"custom_stop\",\n            \"custom_stemmer\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      },\n      \"description\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"integer\"\n      },\n      \"stock_level\": {\n        \"type\": \"integer\"\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index creation\n\nGET products\nOr\nGET /_cat/indices\n\nVerify the field mapping\n\nGET /products/_mapping\n\nIndex and search some sample product data\n\nIndex some products\n\n\nPOST /products/_bulk\n{ \"index\": { \"_index\": \"products\", \"_id\": \"1\" } }\n{ \"name\": \"Wireless Bluetooth Headphones\", \"description\": \"High-quality wireless Bluetooth headphones with noise-cancellation and long battery life.\", \"category\": \"electronics\", \"price\": 99, \"stock_level\": 250 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"2\" } }\n{ \"name\": \"Stainless Steel Water Bottle\", \"description\": \"Durable stainless steel water bottle, keeps drinks cold for 24 hours and hot for 12 hours.\", \"category\": \"home\", \"price\": 25, \"stock_level\": 500 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"3\" } }\n{ \"name\": \"Smartphone\", \"description\": \"Latest model smartphone with high-resolution display and fast processor.\", \"category\": \"electronics\", \"price\": 699, \"stock_level\": 150 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"4\" } }\n{ \"name\": \"LED Desk Lamp\", \"description\": \"Energy-efficient LED desk lamp with adjustable brightness and flexible neck.\", \"category\": \"home\", \"price\": 45, \"stock_level\": 300 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"5\" } }\n{ \"name\": \"4K Ultra HD TV\", \"description\": \"55-inch 4K Ultra HD TV with HDR support and smart features.\", \"category\": \"electronics\", \"price\": 499, \"stock_level\": 200 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"6\" } }\n{ \"name\": \"Vacuum Cleaner\", \"description\": \"High-suction vacuum cleaner with multiple attachments for versatile cleaning.\", \"category\": \"home\", \"price\": 120, \"stock_level\": 100 }\n\nSearch\n\nGET /products/_search?q=name:desk\n\nUse aggregations to calculate the average price per category.\n\nPOST /products/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"average_price_per_category\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nUsing the appropriate data types ensures efficient storage and querying capabilities.\nText fields allow full-text search, while keyword fields enable filtering by exact terms.\n\n\n\n\n\nIn the console execute the following\n\nDELETE products\n\n\n\n\nAggregations\ncat indices API\nCreate Index API\nGet mapping API\nSearch API"
  },
  {
    "objectID": "1-data-management.html#task-define-and-use-an-index-template-for-a-given-pattern-that-satisfies-a-given-set-of-requirements",
    "href": "1-data-management.html#task-define-and-use-an-index-template-for-a-given-pattern-that-satisfies-a-given-set-of-requirements",
    "title": "Data Management",
    "section": "",
    "text": "Create an index template named “user_profile_template”.\nThe template should apply to indices starting with “user_profile-”.\nThe template should have two shards and one replica.\nThe template should have a mapping for the “name” field as a text data type with an analyzer of “standard”.\nThe template should have a mapping for the “age” field as an integer data type.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\n\nPUT /_index_template/user_profile_template\n{\n  \"index_patterns\": [\"user_profile-*\"],\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 2,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"name\": {\n          \"type\": \"text\",\n          \"analyzer\": \"standard\"\n        },\n        \"age\": {\n          \"type\": \"integer\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index template was created\n\nGET _index_template/user_profile_template\n\nCreate an index named “user_profile-2024” using the REST API:\n\nPUT /user_profile_2024\n\nVerify that the index was created with the expected settings and mappings:\n\nGET /user_profile_2024/_settings\nGET /user_profile_2024/_mapping\n\n\n\n\nTwo shards are chosen to allow for parallel processing and improved search performance.\nOne replica is chosen for simplicity and development purposes; in a production environment, this would depend on the expected data volume and search traffic.\nThe “standard” analyzer is chosen for the “name” field to enable standard text analysis.\n\n\n\n\n\nIn the console execute the following\n\nDELETE /_index_template/user_profile_template\n\n\n\n\nAnalyzers\nCreate Index API\nIndex templates\n\n\n\n\n\n\n\n\nIndex name pattern: products-*\nIndex settings:\n\nNumber of shards: 3\nNumber of replicas: 2\n\nMapping:\n\nField name should be of type text\nField description should be of type text\nField price should be of type float\nField category should be of type keyword\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\n\nPUT _template/monthly_products\n{\n  \"index_patterns\": [\"products-*\"],\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 2\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index template was created\n\nGET _index_template/monthly_products\n\nCreate a new index matching the pattern (e.g., products-202305):\n\nPUT products-202305\n\nVerify that the index was created with the expected settings and mappings:\n\nGET /products-202305/_settings\nGET /products-202305/_mapping\n\nIndex a sample document and verify that the mapping is applied correctly:\n\nIndex\n\n\nPOST products-202305/_doc\n{\n  \"name\": \"Product A\",\n  \"description\": \"This is a sample product\",\n  \"price\": 19.99,\n  \"category\": \"Electronics\"\n}\n\nSearch\n\nGET products-202305/_search\nThe response should show the correct mapping for the fields specified in the index template.\n\n\n\n\nThe index_patterns field specifies the pattern for index names to which this template should be applied.\nThe number_of_shards and number_of_replicas settings are chosen based on the expected data volume and high availability requirements.\nThe text type is used for name and description fields to enable full-text search and analysis.\nThe float type is used for the price field to support decimal values.\nThe keyword type is used for the category field to prevent analysis and treat the values as exact matches.\n\n\n\n\n\nIn the console execute the following\n\nDELETE products-202305\nDELETE _template/monthly_products\n\n\n\n\nCreate Index API\nIndex templates\nSearch API\n\n\n\n\n\n\n\n\nThe template should apply to any index starting with logs-.\nThe template must define settings for three primary shards and one replica.\nThe template should include mappings for fields timestamp, log_level, and message.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\n\nPUT /_index_template/logs_template\n{\n  \"index_patterns\": [\"logs-*\"],\n  \"template\": {\n    \"settings\": {\n      \"index\": {\n        \"number_of_shards\": 3,\n        \"number_of_replicas\": 1\n      }\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"timestamp\": {\n          \"type\": \"date\"\n        },\n        \"log_level\": {\n          \"type\": \"keyword\"\n        },\n        \"message\": {\n          \"type\": \"text\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index template was created\n\nGET _index_template/logs_template\n\nCreate a new index matching the pattern (e.g., logs-202405)\n\nPUT logs-202405\n\nVerify that the index was created with the expected settings and mappings\n\nGET /logs-202405/_settings\nGET /logs-202405/_mapping\n\nIndex a sample document and verify that the mapping is applied correctly:\n\nIndex\n\n\nPOST logs-202405/_doc\n{\n  \"@timestamp\": \"2024-05-16T12:34:56Z\",\n  \"log_level\": \"ERROR\",\n  \"message\": \"Help!\"\n}\n\nSearch\n\nGET logs-202405/_search\nThe response should show the correct mapping for the fields specified in the index template.\n\n\n\n\nIndex Patterns: The template applies to any index starting with logs-, ensuring consistency across similar indices.\nNumber of Shards: Three shards provide a balance between performance and resource utilization.\nReplicas: A single replica ensures high availability and fault tolerance.\nMappings: Predefined mappings ensure that the fields are properly indexed and can be efficiently queried.\n\n\n\n\n\nIn the console execute the following\n\nDELETE logs-202405\nDELETE _index_template/logs_template\n\n\n\n\nCreate Index API\nLogstash: Event Dependent Configuration\nIndex templates\nSearch API"
  },
  {
    "objectID": "1-data-management.html#task-define-and-use-a-dynamic-template-that-satisfies-a-given-set-of-requirements",
    "href": "1-data-management.html#task-define-and-use-a-dynamic-template-that-satisfies-a-given-set-of-requirements",
    "title": "Data Management",
    "section": "",
    "text": "FYI: The difference between index templates and dynamic templates is: * An index template is a way to define settings, mappings, and other configurations that should be applied automatically to new indices when they are created. * A dynamic template is part of the mapping definition within an index template or index mapping that allows Elasticsearch to dynamically infer the mapping of fields based on field names, data patterns, or the data type detected.\nThere is one example per field mapping type. They all use an explicit dynamic template, but Exercise 1 also shows the use of a dynamic template embedded in the index definition.\n\n\n\n\n\nApply a specific text analysis to all fields that end with _log.\nUse a keyword type for all fields that start with status_.\nDefault to text with a standard analyzer for other string fields.\nDefine a custom log_analyzer for _log fields.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nDefine the dynamic template\n\n\nAs part of the index definition\n\nPUT /logs_index\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"log_fields\": {\n          \"match\": \"*_log\",\n          \"mapping\": {\n            \"type\": \"text\",\n            \"analyzer\": \"log_analyzer\"\n          }\n        }\n      },\n      {\n        \"status_fields\": {\n          \"match\": \"status_*\",\n          \"mapping\": {\n            \"type\": \"keyword\"\n          }\n        }\n      },\n      {\n        \"default_string\": {\n          \"match_mapping_type\": \"string\",\n          \"mapping\": {\n            \"type\": \"text\",\n            \"analyzer\": \"standard\"\n          }\n        }\n      }\n    ]\n  },\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"log_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\"lowercase\", \"stop\"]\n        }\n      }\n    }\n  }\n}\n\nor as a standalone definition to be added to indexes as needed using the index_pattern\n\nPUT /_index_template/logs_dyn_template\n{\n  \"index_patterns\": [\"logs_*\"],\n  \"template\": {\n    \"mappings\": {\n      \"dynamic_templates\": [\n        {\n          \"log_fields\": {\n            \"match\": \"*_log\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"log_analyzer\"\n            }\n          }\n        },\n        {\n          \"status_fields\": {\n            \"match\": \"status_*\",\n            \"mapping\": {\n              \"type\": \"keyword\"\n            }\n          }\n        },\n        {\n          \"default_string\": {\n            \"match_mapping_type\": \"string\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"standard\"\n            }\n          }\n        }\n      ]\n    },\n    \"settings\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"log_analyzer\": {\n            \"type\": \"custom\",\n            \"tokenizer\": \"standard\",\n            \"filter\": [\"lowercase\", \"stop\"]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the dynamic template was created\n\nIf you used the embedded version\n\n\nGET /logs_index/_mapping\n\nIf you used the standalone version\n\nGET /_index_template/logs_dyn_template\n\nCreate a new index matching the pattern (e.g., logs-202405)\n\nOptional if you used the embedded version\n\n\nPUT logs_index\n\nVerify that the created index has the expected settings and mappings\n\nEnsure error_log is of type text with log_analyzer\nEnsure status_code is of type keyword\nEnsure message is of type text with standard analyzer\n\n\nGET /logs_index/_mapping\n\nIndex a sample document and verify that the mapping is applied correctly\n\nPOST /logs_index/_doc/1\n{\n  \"error_log\": \"This is an error log message.\",\n  \"status_code\": \"200\",\n  \"message\": \"Regular log message.\"\n}\n\nPerform Searches:\n\nSearch within error_log and verify the custom analyzer is applied\n\n\nGET /logs_index/_search\n{\n  \"query\": {\n    \"match\": {\n      \"error_log\": \"error\"\n    }\n  }\n}\n\nCheck if status_code is searchable as a keyword\n\nGET /logs_index/_search\n{\n  \"query\": {\n    \"term\": {\n      \"status_code\": \"200\"\n    }\n  }\n}\n\n\n\n\nThe custom analyzer log_analyzer is used to provide specific tokenization and filtering for log fields.\nThe keyword type for status_* fields ensures they are treated as exact values, useful for status codes.\nThe default_string template ensures other string fields are analyzed with the standard analyzer, providing a balanced default.\n\n\n\n\n\nDelete the index\n\nDELETE logs_index\n\nDelete the dynamic template\n\nDELETE /_index_template/logs_dyn_template\n\n\n\n\nDynamic Templates\nCustom Analyzers\nPut Mapping API\nIndex API\nSearch API\n\n\n\n\n\n\n\n\nAll string fields should be treated as text with a standard analyzer.\nAll long fields should be treated as integer.\nAll date fields should use a specific date format.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nDefine the Dynamic Template\n\nPUT /_index_template/data_type_template\n{\n  \"index_patterns\": [\"data_type_*\"],\n  \"template\": {\n    \"mappings\": {\n      \"dynamic_templates\": [\n        {\n          \"strings_as_text\": {\n            \"match_mapping_type\": \"string\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"standard\"\n            }\n          }\n        },\n        {\n          \"longs_as_integer\": {\n            \"match_mapping_type\": \"long\",\n            \"mapping\": {\n              \"type\": \"integer\"\n            }\n          }\n        },\n        {\n          \"dates_with_format\": {\n            \"match_mapping_type\": \"date\",\n            \"mapping\": {\n              \"type\": \"date\",\n              \"format\": \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nVerify the dynamic template was created\n\nGET /_index_template/data_type_template\n\nCreate a new index matching the pattern\n\nPUT data_type_202405\n\nCheck the Field Types\n\nVerify that all string fields are mapped as text with the standard analyzer.\nVerify that all long fields are mapped as integer.\nVerify that all date fields are mapped with the correct format.\n\n\nGET /data_type_202405/_mapping\n\nInsert sample documents to ensure that the dynamic template is applied correctly\n\nPOST /data_type_202405/_bulk\n{ \"index\": { \"_index\": \"data_type_202405\", \"_id\": \"1\" } }\n{ \"name\": \"Wireless Bluetooth Headphones\", \"release_date\": \"2024-05-28T14:35:00.000Z\", \"price\": 99 }\n{ \"index\": { \"_index\": \"data_type_202405\", \"_id\": \"2\" } }\n{ \"description\": \"Durable stainless steel water bottle\", \"launch_date\": \"2024-05-28T15:00:00.000Z\", \"quantity\": 500 }\n\nPerform Searches\n\nSearch launch_date\n\n\nGET /data_type_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"launch_date:\\\"2024-05-28T15:00:00.000Z\\\"\"\n    }\n  }\n}\n\nCheck if price is searchable as a value\n\nGET /data_type_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"price: 99\"\n    }\n  }\n}\n\n\n\n\nDynamic Templates: Using dynamic templates based on data types allows for flexible and consistent field mappings without needing to know the exact field names in advance.\nData Types: Matching on data types (string, long, date) ensures that fields are mapped appropriately based on their content.\nDate Format: Specifying the date format ensures that date fields are parsed correctly, avoiding potential issues with date-time representation.\n\n\n\n\n\nDelete the index\n\nDELETE data_type_202405\n\nDelete the dynamic template\n\nDELETE /_index_template/data_type_template\n\n\n\n\nDynamic Templates\nMapping\nAnalyzers\n\n\n\n\n\n\n\n\nAutomatically map fields that end with “_ip” as IP type.\nMap fields that start with “timestamp_” as date type.\nMap any field containing the word “keyword” as a keyword type.\nUse a custom analyzer for fields ending with “_text”.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the dynamic template\n\nPUT /_index_template/logs_template\n{\n  \"index_patterns\": [\"logs*\"],\n  \"template\": {\n    \"settings\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"custom_analyzer\": {\n            \"type\": \"standard\",\n            \"stopwords\": \"_english_\"\n          }\n        }\n      }\n    },\n    \"mappings\": {\n      \"dynamic_templates\": [\n        {\n          \"ip_fields\": {\n            \"match\": \"*_ip\",\n            \"mapping\": {\n              \"type\": \"ip\"\n            }\n          }\n        },\n        {\n          \"date_fields\": {\n            \"match\": \"timestamp_*\",\n            \"mapping\": {\n              \"type\": \"date\"\n            }\n          }\n        },\n        {\n          \"keyword_fields\": {\n            \"match\": \"*keyword*\",\n            \"mapping\": {\n              \"type\": \"keyword\"\n            }\n          }\n        },\n        {\n          \"text_fields\": {\n            \"match\": \"*_text\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"custom_analyzer\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nVerify the dynamic template was created\n\nGET /_index_template/logs_template\n\nCreate a new index matching the pattern\n\nPUT logs_202405\n\nCheck the Field Types\n\nVerify that all _ip fields are mapped as ip\nVerify that all timestamp_* fields are mapped as date\nVerify that all keyword fields are mapped as keyword\n\n\nGET /logs_202405/_mapping\n\nInsert sample documents to ensure that the dynamic template is applied correctly\n\nPOST /logs_202405/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"source_ip\": \"192.168.1.1\", \"timestamp_event\": \"2024-05-28T12:00:00Z\", \"user_keyword\": \"elastic\", \"description_text\": \"This is a log entry.\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"destination_ip\": \"10.0.0.1\", \"timestamp_access\": \"2024-05-28T12:05:00Z\", \"log_keyword\": \"search\", \"details_text\": \"Another log entry.\" }\n\nPerform Searches\n\nSearch source_ip\n\n\nGET /logs_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"source_ip:\\\"192.168.1.1\\\"\"\n    }\n  }\n}\n\nCheck if timestamp_event is searchable as a date\n\nGET /logs_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"timestamp_event:\\\"2024-05-28T12:00:00Z\\\"\"\n    }\n  }\n}\n\n\n\n\nThe use of patterns in the dynamic template ensures that newly added fields matching the criteria are automatically mapped without the need for manual intervention.\nCustom analyzer configuration is critical for ensuring text fields are processed correctly, enhancing search capabilities.\n\n\n\n\n\nDelete the index\n\nDELETE logs_202405\n\nDelete the dynamic template\n\nDELETE /_index_template/logs_template\n\n\n\n\nDynamic Templates\nMapping Types\nAnalyzers"
  },
  {
    "objectID": "1-data-management.html#task-define-an-index-lifecycle-management-policy-for-a-timeseries-index",
    "href": "1-data-management.html#task-define-an-index-lifecycle-management-policy-for-a-timeseries-index",
    "title": "Data Management",
    "section": "",
    "text": "Indices are prefixed with “logstash-” and a datestamp (e.g. logstash-2024.05.16).\nIndices should be rolled over daily (create a new index every day).\nOld indices should be deleted after 30 days.\n\n\n\n\n\nOpen the hamburger menu and click on Management → Life Cycle Policies.\nPress + Create New Policy.\nEnter the following:\n\nPolicy name: logstash-example-policy.\nHot phase:\n\nChange Keep Data in the Phase Forever (the infinity icon) to Delete Data After This Phase (the trashcan icon).\nClick Advanced Settings.\nUnselect Use Recommended Defaults.\nSet Maximum Age to 1.\n\nDelete phase:\n\nMove data into phase when: 30 days old.\n\n\nPress Save Policy.\nOpen the Kibana Console or use a REST client.\nCreate an index template that will match on indices that match the pattern “logstash-*“.\n\nPUT /_index_template/ilm_logstash_index_template\n{\n  \"index_patterns\": [\"logstash-*\"]\n}\n\nReturn to the Management → Life Cycle Policies page.\nPress the plus sign (+) to the right of logstash-example-policy.\n\nThe Add Policy “logstash-example-policy” to index template dialog opens.\nClick on the Index Template input field and type the first few letters of the index template created above.\nSelect the template created above (ilm_logstash_index_template).\nPress Add Policy.\n\nOpen the Kibana Console or use a REST client.\nList ilm_logs_index_template. Notice the ILM policy is now part of the index template.\n\nGET /_index_template/ilm_logstash_index_template\nOutput from the GET:\n{\n  \"index_templates\": [\n    {\n      \"name\": \"ilm_logstash_index_template\",\n      \"index_template\": {\n        \"index_patterns\": [\"logstash-*\"],\n        \"template\": {\n          \"settings\": {\n            \"index\": {\n              \"lifecycle\": {\n                \"name\": \"logstash-example-policy\"\n              }\n            }\n          }\n        },\n        \"composed_of\": []\n      }\n    }\n  ]\n}\n\nCreate an index.\n\nPUT logstash-2024.05.16\n\nVerify the policy is there.\n\nGET logstash-2024.05.16\nThe output should look something like this:\n{\n  \"logstash-2024.05.16\": {\n    \"aliases\": {},\n    \"mappings\": {},\n    \"settings\": {\n      \"index\": {\n        \"lifecycle\": {\n          \"name\": \"logstash-example-policy\"\n        },\n        \"routing\": {\n          \"allocation\": {\n            \"include\": {\n              \"_tier_preference\": \"data_content\"\n            }\n          }\n        },\n        \"number_of_shards\": \"1\",\n        \"provided_name\": \"logstash-2024.05.16\",\n        \"creation_date\": \"1717024100387\",\n        \"priority\": \"100\",\n        \"number_of_replicas\": \"1\",\n        \"uuid\": \"mslAKuZGTpSDdFr4hSpAAA\",\n        \"version\": {\n          \"created\": \"8503000\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the ILM policy.\n\nPUT _ilm/policy/logstash-example-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_age\": \"1d\"\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n\nCreate an index template that includes the above policy. The two fields within settings are required.\n\nPUT /_index_template/ilm_logstash_index_template\n{\n  \"index_patterns\": [\"logstash-*\"],\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"logstash-example-policy\",\n      \"index.lifecycle.rollover_alias\": \"logstash\"\n    }\n  }\n}\n\n\n\n\nVerify the ILM policy exists in Kibana under Management → Index Lifecycle Policies.\nVerify the Index Lifecycle Management policy exists and references the index template.\n\nGET /_ilm/policy/logstash-example-policy\n\nVerify the policy is referenced in the index template.\n\nGET /_index_template/ilm_logstash_index_template\n\nCreate a new index that matches the pattern logstash-*.\n\nPUT /logstash-index\n\nVerify the index has the policy in its definition.\n\nGET /logstash-index\n\n\n\n\nThe index template configures 1 shard and the ILM policy/alias for rollover.\nThe rollover action creates a new index when the max_age is reached.\nThe delete phase removes indices older than 30 days.\n\n\n\n\n\nDelete the index.\n\nDELETE logstash-index\n\nDelete the index template.\n\nDELETE /_index_template/ilm_logstash_index_template\n\nDelete the policy.\n\nDELETE /_ilm/policy/logstash-example-policy\n\n\n\n\nCreate Index API\nCreate or Update Index Template API\nILM Settings\nIndex Lifecycle Management\n\n\n\n\n\n\n\n\nThe policy should be named “logs-policy”.\nIt should have a hot phase with a duration of 7 days.\nIt should have a warm phase with a duration of 30 days.\nIt should have a cold phase with a duration of 90 days.\nIt should have a delete phase.\nThe policy should be assigned to indices matching the pattern “ilm_logs_*“.\n\nSteps using the Elastic/Kibana UI\n\nOpen the hamburger menu and click on Management → Life Cycle Policies.\nPress + Create New Policy.\nEnter the following:\n\nPolicy name: logs-policy.\nHot phase:\n\nPress the garbage can icon to the right to delete data after this phase.\n\nWarm phase:\n\nMove data into phase when: 7 days old.\nLeave Delete data after this phase.\n\nCold phase:\n\nMove data into phase when: 30 days old.\nLeave Delete data after this phase.\n\nDelete phase:\n\nMove data into phase when: 90 days old.\n\n\nPress Save Policy.\nOpen the Kibana Console or use a REST client.\nCreate an index template that will match on indices that match the pattern “ilm_logs_*“.\n\nPUT /_index_template/ilm_logs_index_template\n{\n  \"index_patterns\": [\"ilm_logs_*\"]\n}\n\nReturn to the Management → Life Cycle Policies page.\nPress the plus sign (+) to the right of logs_policy.\nThe Add Policy “logs-policy” to index template dialog opens.\nClick on the Index Template input field and type the first few letters of the index template created above.\nSelect the template created above (ilm_logs_index_template).\nPress Add Policy.\nOpen the Kibana Console or use a REST client.\nList ilm_logs_index_template. Notice the ILM policy is now part of the index template.\n\nGET /_index_template/ilm_logs_index_template\nOutput from the GET:\n{\n  \"index_templates\": [\n    {\n      \"name\": \"ilm_logs_index_template\",\n      \"index_template\": {\n        \"index_patterns\": [\"ilm_logs_*\"],\n        \"template\": {\n          \"settings\": {\n            \"index\": {\n              \"lifecycle\": {\n                \"name\": \"logs-policy\"\n              }\n            }\n          }\n        },\n        \"composed_of\": []\n      }\n    }\n  ]\n}\n\nList logs-policy.\n\nGET _ilm/policy/logs-policy\nIn the in_use_by node you will see:\n\"in_use_by\": {\n  \"indices\": [],\n  \"data_streams\": [],\n  \"composable_templates\": [\n    \"ilm_logs_index_template\"\n  ]\n}\nSteps Using the REST API\n\nOpen the Kibana Console or use a REST client.\nCreate the ILM policy.\n\nPUT _ilm/policy/logs-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"min_age\": \"0ms\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 100\n          }\n        }\n      },\n      \"warm\": {\n        \"min_age\": \"7d\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 50\n          }\n        }\n      },\n      \"cold\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 0\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"90d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n\nAssign the policy to the indices matching the pattern “logs_*“.\n\nPUT /_index_template/ilm_logs_index_template\n{\n  \"index_patterns\": [\"ilm_logs_*\"],\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"logs-policy\",\n      \"index.lifecycle.rollover_alias\": \"logs\"\n    }\n  }\n}\n\n\n\n\nVerify the ILM policy exists in Kibana under Management → Index Lifecycle Policies.\nVerify the Index Lifecycle Management policy exists and references the index template.\n\nGET /_ilm/policy/logs-policy\n\nVerify the policy is referenced in the index template.\n\nGET /_index_template/ilm_logs_index_template\n\nCreate a new index that matches the pattern ilm_logs_*.\n\nPUT /ilm_logs_index\n\nVerify the index has the policy in its definition.\n\nGET /ilm_logs_index\n\n\n\n\nThe ILM policy will manage the indices matching the pattern ilm_logs_*.\nThe hot phase will keep the data for 7 days with high priority and rollover.\nThe warm phase will keep the data for 30 days with medium priority.\nThe cold phase will keep the data for 90 days with low priority.\nThe ILM policy will automatically manage the indices based on their age and size.\nThe policy can be adjusted based on the needs of the application and the data.\n\n\n\n\n\nDelete the index.\n\nDELETE ilm_logs_index\n\nDelete the index template.\n\nDELETE _index_template/ilm_logs_index_template\n\nDelete the policy.\n\nDELETE _ilm/policy/logs-policy\n\n\n\n\nCreate Index API\nCreate or Update Index Template API\nILM Settings\nIndex Lifecycle Management\n\n\n\n\n\n\n\n\nCreate a new index every day for sensor data (e.g., sensor_data-{date}).\nAutomatically roll over to a new index when the current one reaches a specific size.\nDelete rolled over indices after one month.\n\nSteps using the Elastic/Kibana UI\n\nOpen the hamburger menu and click on Management → Life Cycle Policies.\nPress + Create New Policy.\nEnter the following:\n\nPolicy name: sensor-data-policy.\nHot phase:\n\nChange Keep Data in the Phase Forever (the infinity icon) to Delete Data After This Phase (the trashcan icon).\nClick Advanced Settings.\nUnselect Use Recommended Defaults.\nSet Maximum Age to 1.\nSet Maximum Index Size to 10.\n\nDelete phase:\n\nMove data into phase when: 30 days old.\n\n\nPress Save Policy.\nOpen the Kibana Console or use a REST client.\nCreate an index template that will match on indices that match the pattern “sensor_data-*“.\n\nPUT /_index_template/sensor_data_index_template\n{\n  \"index_patterns\": [\"sensor_data-*\"]\n}\n\nReturn to the Management → Life Cycle Policies page.\nPress the plus sign (+) to the right of sensor-data-policy.\nThe Add Policy “sensor-data-policy” to index template dialog opens.\n\nClick on the Index Template input field and type the first few letters of the index template created above.\nSelect the template created above (sensor_data_index_template).\nPress Add Policy.\n\n\n\n\nOpen the Kibana Console or use a REST client.\nList sensor_data_index_template. Notice the ILM policy is now part of the index template.\n\nGET /_index_template/sensor_data_index_template\nOutput from the GET:\n{\n  \"index_templates\": [\n    {\n      \"name\": \"sensor_data_index_template\",\n      \"index_template\": {\n        \"index_patterns\": [\"sensor_data-*\"],\n        \"template\": {\n          \"settings\": {\n            \"index\": {\n              \"lifecycle\": {\n                \"name\": \"sensor-data-policy\"\n              }\n            }\n          }\n        },\n        \"composed_of\": []\n      }\n    }\n  ]\n}\n\nList sensor-data-policy.\n\nGET /_ilm/policy/sensor-data-policy\nIn the in_use_by node you will see:\n\"in_use_by\": {\n  \"indices\": [],\n  \"data_streams\": [],\n  \"composable_templates\": [\n    \"sensor_data_index_template\"\n  ]\n}\nOR\nSteps Using the REST API\n\nOpen the Kibana Console or use a REST client.\nDefine the ILM policy.\n\nPUT _ilm/policy/sensor-data-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"min_age\": \"0ms\",\n        \"actions\": {\n          \"rollover\": {\n            \"max_age\": \"1d\",\n            \"max_size\": \"10gb\"\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n\nAssign the policy to the indices matching the pattern “sensor_data-*“.\n\nPUT /_index_template/sensor_data_index_template\n{\n  \"index_patterns\": [\"sensor_data-*\"],\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"sensor-data-policy\",\n      \"index.lifecycle.rollover_alias\": \"sensor\"\n    }\n  }\n}\n\n\n\n\nVerify the ILM policy exists in Kibana under Management → Index Lifecycle Policies.\nVerify the Index Lifecycle Management policy exists and references the index template.\n\nGET /_ilm/policy/sensor-data-policy\n\nVerify the policy is referenced in the index template.\n\nGET /_index_template/sensor_data_index_template\n\nCreate a new index that matches the pattern sensor_data-*.\n\nPUT /sensor_data-20240516\n\nVerify the index has the policy in its definition.\n\nGET /sensor_data-20240516\n\n\n\n\nThe hot phase size threshold determines the frequency of rollovers.\nThe delete phase retention period defines how long rolled over data is stored.\n\n\n\n\n\nDelete the index.\n\nDELETE sensor_data-20240516\n\nDelete the index template.\n\nDELETE /_index_template/sensor_data_index_template\n\nDelete the policy.\n\nDELETE /_ilm/policy/sensor-data-policy\n\n\n\n\nCreate Index API\nCreate or Update Index Template API\nILM Settings\nIndex Lifecycle Management"
  },
  {
    "objectID": "1-data-management.html#task-define-an-index-template-that-creates-a-new-data-stream",
    "href": "1-data-management.html#task-define-an-index-template-that-creates-a-new-data-stream",
    "title": "Data Management",
    "section": "",
    "text": "Data streams in Elasticsearch are used for managing time-series data such as logs, metrics, and events. They can handle large volumes of time-series data in an efficient and scalable manner.\n\n\n\n\n\nCreate a new data stream named “app-logs” to store application logs.\nAutomatically create new backing indices within the data stream as needed.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nDefine the index template that will be used by the data stream to create new backing indices.\n\nPUT _index_template/app_logs_index_template\n{\n  \"index_patterns\": [\"app_logs*\"],\n  \"data_stream\": {}\n}\n\n\n\n\nVerify the index template creation.\n\nGET _index_template/app_logs_index_template\n\nConfirm there are no indices named app_logs*.\n\nGET /_cat/indices\n\nMock sending streaming data by just pushing a few documents to the stream. When sending documents using _bulk, they must use create instead of index. In addition, the documents must have a @timestamp field.\n\nPOST app_logs/_bulk\n{ \"create\":{} }\n{ \"@timestamp\": \"2099-05-06T16:21:15.000Z\", \"message\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\" }\n{ \"create\":{} }\n{ \"@timestamp\": \"2099-05-06T16:25:42.000Z\", \"message\": \"192.0.2.255 - - [06/May/2099:16:25:42 +0000] \\\"GET /favicon.ico HTTP/1.0\\\" 200 3638\" }\nThe response will list the name of the automatically created index, which will look something like this:\n{\n  \"errors\": false,\n  \"took\": 8,\n  \"items\": [\n    {\n      \"create\": {\n        \"_index\": \".ds-app_logs-2099.05.06-000001\",\n        \"_id\": \"OOazyo8BAvAOn4WaAfdD\",\n        \"_version\": 1,\n        \"result\": \"created\",\n        \"_shards\": {\n          \"total\": 2,\n          \"successful\": 1,\n          \"failed\": 0\n        },\n        \"_seq_no\": 2,\n        \"_primary_term\": 1,\n        \"status\": 201\n      }\n    },\n    {\n      \"create\": {\n        \"_index\": \".ds-app_logs-2099.05.06-000001\",\n        \"_id\": \"Oeazyo8BAvAOn4WaAfdD\",\n        \"_version\": 1,\n        \"result\": \"created\",\n        \"_shards\": {\n          \"total\": 2,\n          \"successful\": 1,\n          \"failed\": 0\n        },\n        \"_seq_no\": 3,\n        \"_primary_term\": 1,\n        \"status\": 201\n      }\n    }\n  ]\n}\n\nRun:\n\nGET /_cat/indices\nYou will see the new index listed. This is the backing index created by the data stream.\n\nCheck for the app_logs data stream under Management → Index Management → Data Streams.\nVerify that the documents were indexed.\n\nGET .ds-app_logs-2099.05.06-000001\n\n\n\n\nData streams provide a more efficient way to handle continuously flowing data compared to daily indices. They are created implicitly through the use of index templates, and you must use the **_bulk API** (including the use of create) when streaming data.\nNew backing indices are automatically created within the data stream as needed.\nLifecycle management policies can be applied to data streams for automatic deletion of older backing indices.\n\n\n\n\n\nDelete the data stream (deleting the data stream will also delete the backing index).\n\nDELETE /_data_stream/app_logs\n\nDelete the index template.\n\nDELETE _index_template/app_logs_index_template\n\n\n\n\nIndex Templates\nSetting Up a Data Stream\n\n\n\n\n\n\n\n\nThe template should apply to any index matching the pattern logs*.\nThe template must create a data stream.\nThe template should define settings for two primary shards and one replica.\nThe template should include mappings for fields @timestamp, log_level, and message.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\n\nPUT _index_template/log_application_index_template\n{\n  \"index_patterns\": [\"logs*\"],\n  \"data_stream\": {},\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 2,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\"\n        },\n        \"log_level\": {\n          \"type\": \"keyword\"\n        },\n        \"message\": {\n          \"type\": \"text\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index template creation\n\nGET _index_template/log_application_index_template\n\nConfirm there are no indices named logs*\n\nGET /_cat/indices\n\nIndex documents into the data stream\n\nPOST /logs/_doc\n{\n  \"@timestamp\": \"2024-05-16T12:34:56\",\n  \"log_level\": \"info\",\n  \"message\": \"Test log message\"\n}\nThis will return a result with the name of the backing index\n{\n  \"_index\": \".ds-logs-2024.05.16-000001\", // yours will be different\n  \"_id\": \"PObWyo8BAvAOn4WaC_de\",\n  \"_version\": 1,\n  \"result\": \"created\",\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 1,\n    \"failed\": 0\n  },\n  \"_seq_no\": 0,\n  \"_primary_term\": 1\n}\nRun\nGET /_cat/indices\nThe index will be listed.\n\nConfirm the configuration of the backing index matches the index template (your backing index name will be different)\n\nGET .ds-logs-2024.05.16-000001\n\nRun a search for the document that was indexed\n\nGET .ds-logs-2024.05.16-000001/_search\n\n\n\n\nData streams provide a more efficient way to handle continuously flowing data compared to daily indices. They are created implicitly through the use of index templates, and you must use the **_bulk API** (including the use of create) when streaming data.\nNew backing indices are automatically created within the data stream as needed.\nLifecycle management policies can be applied to data streams for automatic deletion of older backing indices (not shown but there is an example at Set Up a Data Stream).\n\n\n\n\n\nDelete the data stream (deleting the data stream will also delete the backing index)\n\nDELETE _data_stream/logs\n\nDelete the index template\n\nDELETE _index_template/log_application_index_template\n\n\n\n\nIndex Templates\nSetting Up a Data Stream\n\n\n\n\n\n\n\n\nCreate an index template named “metrics_template”.\nThe template should create a new data stream named “metrics-{suffix}”.\nThe template should have one shard and one replica.\nThe template should have a mapping for the “metric” field as a keyword data type.\nThe template should have a mapping for the “value” field as a float data type.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index template.\n\nPUT _index_template/metrics_template\n{\n  \"index_patterns\": [\"metrics-*\"],\n  \"data_stream\": {},\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"metric\": {\n          \"type\": \"keyword\"\n        },\n        \"value\": {\n          \"type\": \"float\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index template creation.\n\nGET _index_template/metrics_template\n\nConfirm there are no indices named metrics-*.\n\nGET /_cat/indices\n\nIndex documents into the data stream.\n\nPOST /metrics-ds/_doc\n{\n  \"@timestamp\": \"2024-05-16T12:34:56\",\n  \"metric\": \"cpu\",\n  \"value\": 0.5\n}\nNotice the use of the @timestamp field. That is required for any documents going into a data stream.\n\nThis will return a result with the name of the backing index.\n\n{\n  \"_index\": \".ds-metrics-ds-2024.05.16-000001\", // yours will be different\n  \"_id\": \"P-YFy48BAvAOn4WaUvef\",\n  \"_version\": 1,\n  \"result\": \"created\",\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 1,\n    \"failed\": 0\n  },\n  \"_seq_no\": 1,\n  \"_primary_term\": 1\n}\n\nRun:\n\nGET /_cat/indices\n\nThe index will be listed.\nConfirm the configuration of the backing index matches the index template (your backing index name will be different).\n\nGET .ds-metrics-ds-2024.05.16-000001\n\nRun a search for the document that was indexed.\n\nGET .ds-metrics-ds-2024.05.16-000001/_search\n\n\n\n\nThe “keyword” data type is chosen for the “metric” field to enable exact matching and filtering.\nThe “float” data type is chosen for the “value” field to enable precise numerical calculations.\nOne shard and one replica are chosen for simplicity and development purposes; in a production environment, this would depend on the expected data volume and search traffic.\n\n\n\n\n\nDelete the data stream (deleting the data stream will also delete the backing index).\n\nDELETE _data_stream/metrics-ds\n\nDelete the index template.\n\nDELETE _index_template/metrics_template\n\n\n\n\nIndex templates\nData streams\nMapping types"
  },
  {
    "objectID": "0.2-intro.html",
    "href": "0.2-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This study guide will go over the specific study areas as listed by Elastic on their website for the Elastic Certified Engineer Exam v8.1 (true as of XXX NN, 2024):\nData Management\n\nDefine an index that satisfies a given set of requirements\nDefine and use an index template for a given pattern that satisfies a given set of requirements\nDefine and use a dynamic template that satisfies a given set of requirements\nDefine an Index Lifecycle Management policy for a time-series index\nDefine an index template that creates a new data stream\n\nSearching Data\n\nWrite and execute a search query for terms and/or phrases in one or more fields of an index\nWrite and execute a search query that is a Boolean combination of multiple queries and filters\nWrite an asynchronous search\nWrite and execute metric and bucket aggregations\nWrite and execute aggregations that contain sub-aggregations\nWrite and execute a query that searches across multiple clusters\nWrite and execute a search that utilizes a runtime field\n\nDeveloping Search Applications\n\nHighlight the search terms in the response of a query\nSort the results of a query by a given set of requirements\nImplement pagination of the results of a search query\nDefine and use index aliases\nDefine and use a search template\n\nData Processing\n\nDefine a mapping that satisfies a given set of requirements\nDefine and use a custom analyzer that satisfies a given set of requirements\nDefine and use multi-fields with different data types and/or analyzers\nUse the Reindex API and Update By Query API to reindex and/or update documents\nDefine and use an ingest pipeline that satisfies a given set of requirements, including the use of Painless to modify documents\nDefine runtime fields to retrieve custom values using Painless scripting\n\nCluster Management\n\nDiagnose shard issues and repair a cluster’s health\nBackup and restore a cluster and/or specific indices\nConfigure a snapshot to be searchable\nConfigure a cluster for cross-cluster search\nImplement cross-cluster replication\n\n\n\nThis study guide makes a number of assumptions about the reader; not the least of which is your understanding of search and search technologies. This guide is:\n\nnot an introductory text on search and search technologies\nnot an Elastic or Kibana tutorial\nnot a JSON tutorial\n\nThe examples present the answers as REST API calls in JSON for the Elastic Kibana Console. In the Things You Must Do section we will show you how to translate the REST API calls into curl calls. This is done specifically so you understand how to execute the calls both ways, but will not be used in the examples.\n\n\n\nRegardless of where you are running Elastic (locally or from the Elastic cloud) you are going to need two bits of information to access your deployment:\n\nusername\npassword\n\nHow and where you get those two will depend on whether you have a local instance of Elasticsearch/Kibana or if you are using an Elastic Cloud deployment.\nThe installation of a local instance of Elasticsearch/Kibana can be found in the Appendix along with basic instructions on how you get the username/password when you create an Elastic cloud deployment. These same instructions can be found in the Elastic documentation.\nIf you decide to run these examples from the command line using curl you must:\n\nhave curl installed\nhave your Elasticsearch username (elastic) and password handy. The password will vary based on whether you are running your own local copy of the Elastic Stack or using the Elastic Cloud.\nIf you are running curl against your local container instance your command line should look like this:\n\ncurl --cacert http_ca.crt -u elastic:[container instance password here] -X [desired HTTP command] \"https://[elastic endpoint here]/[API path as appropriate]\"\nThe http_ca.crt file was something you should have extracted from the container while you were deploying it. If you haven’t then execute this first in the location where you are doing your certification work (assuming you have called your Elasticsearch container es01):\ndocker cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .\nKeep that file secret. Keep that file safe.\nAs a test you should be able to run:\ncurl --cacert http_ca.crt -u elastic:[container instance password here] https://localhost:9200/\nYou should get some reasonable output.\n\nIf you are running curl against the Elastic Cloud your command line should look like this:\n\ncurl -u elastic:[elastic cloud deployment password here] -X [desired HTTP command] \"https://[elastic endpoint here]/[API path as appropriate]\"\nThe only thing different with the above and the local command is you don’t need the certificate file if you are running curl against the Elastic Cloud.\nOriginally, the examples had both the REST API calls and the curl calls. Since the curl calls are slightly different between the local instance the the Elastic Cloud instance I have left them out. If you want to run curl using the Elastic REST API then remember that the REST API looks like this in the Elastic cloud console:\nGET / \nor\nPUT /example_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\nThe associated curl command for would like like this:\n\nLocal\n\nThe first call:\ncurl --cacert http_ca.crt -u elastic:[container instance password here] -X GET https://localhost:9200/\nor for the second call (note the use of single quotes around the JSON):\ncurl --cacert http_ca.crt -u elastic:[container instance password here] -X PUT https://localhost:9200/example_index\" -H 'Content-Type: application/json' -d'\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}'\n\nElastic Cloud\n\nThe first call:\ncurl -u elastic:[elastic cloud deployment password here] -X GET \"https://[elastic endpoint here]/\"\nor for the second call (note the use of single quotes around the JSON):\ncurl -u elastic:[elastic cloud deployment password here] -X PUT \"[elastic endpoint here]\" -H 'Content-Type: application/json' -d'\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}'\n\n\n\nDo the examples and more. Be over-prepared.\nGo through the documentation so you know where to look when a task shows up in the certification exam and you are not sure what the syntax or format of a given JSON might be. This is an open book test, but the only book you can use is the Elastic documentation.\nLearn the basics. The Elastic console has code completion so you don’t have to remember everything…just what might be the appropriate JSON elements for the solution you are implementing.\nLearn the query syntax of Query DSL. This is a search engine after all and knowledge of querying is fundamental to the certification exam.\nWhile there is no magic bullet the exam should not be that hard if you already have knowledge of:\n\nsearch and search technologies (preferably hands-on)\nJSON\n\nThings that will trip you up:\n\nsyntax\ndepth of various JSON nodes\n\n\n\n\nThis book will not teach you Elastic search or Kibana or any of the other Elastic products available in the first half of 2024. It assumes you have an understanding of various search topics, JSON, REST APIs, and areas like regular expressions, and web technologies. The examples are valid with the current documentation and were all confirmed as working in the same time period.\nIf you run into any problems with the examples, please send an email to support@brushedsteelconsulting.com. When in doubt, asking one of the many public LLMs available should help as well:\n\nChatGPT\nClaude.ai\nGemini\nMeta.ai\nPerplexity.ai\n\nAll of the examples were originally generated by the various LLMs above sites with many changes made by an actual human as the examples and the generated content left much to be desired.\nDisclaimer: this book was written with the assistance of various tools include a host of LLMs, but always under the guidance of the author. Make of that what you will."
  },
  {
    "objectID": "0.2-intro.html#assumptions",
    "href": "0.2-intro.html#assumptions",
    "title": "Introduction",
    "section": "",
    "text": "This study guide makes a number of assumptions about the reader; not the least of which is your understanding of search and search technologies. This guide is:\n\nnot an introductory text on search and search technologies\nnot an Elastic or Kibana tutorial\nnot a JSON tutorial\n\nThe examples present the answers as REST API calls in JSON for the Elastic Kibana Console. In the Things You Must Do section we will show you how to translate the REST API calls into curl calls. This is done specifically so you understand how to execute the calls both ways, but will not be used in the examples."
  },
  {
    "objectID": "0.2-intro.html#things-you-must-do",
    "href": "0.2-intro.html#things-you-must-do",
    "title": "Introduction",
    "section": "",
    "text": "Regardless of where you are running Elastic (locally or from the Elastic cloud) you are going to need two bits of information to access your deployment:\n\nusername\npassword\n\nHow and where you get those two will depend on whether you have a local instance of Elasticsearch/Kibana or if you are using an Elastic Cloud deployment.\nThe installation of a local instance of Elasticsearch/Kibana can be found in the Appendix along with basic instructions on how you get the username/password when you create an Elastic cloud deployment. These same instructions can be found in the Elastic documentation.\nIf you decide to run these examples from the command line using curl you must:\n\nhave curl installed\nhave your Elasticsearch username (elastic) and password handy. The password will vary based on whether you are running your own local copy of the Elastic Stack or using the Elastic Cloud.\nIf you are running curl against your local container instance your command line should look like this:\n\ncurl --cacert http_ca.crt -u elastic:[container instance password here] -X [desired HTTP command] \"https://[elastic endpoint here]/[API path as appropriate]\"\nThe http_ca.crt file was something you should have extracted from the container while you were deploying it. If you haven’t then execute this first in the location where you are doing your certification work (assuming you have called your Elasticsearch container es01):\ndocker cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .\nKeep that file secret. Keep that file safe.\nAs a test you should be able to run:\ncurl --cacert http_ca.crt -u elastic:[container instance password here] https://localhost:9200/\nYou should get some reasonable output.\n\nIf you are running curl against the Elastic Cloud your command line should look like this:\n\ncurl -u elastic:[elastic cloud deployment password here] -X [desired HTTP command] \"https://[elastic endpoint here]/[API path as appropriate]\"\nThe only thing different with the above and the local command is you don’t need the certificate file if you are running curl against the Elastic Cloud.\nOriginally, the examples had both the REST API calls and the curl calls. Since the curl calls are slightly different between the local instance the the Elastic Cloud instance I have left them out. If you want to run curl using the Elastic REST API then remember that the REST API looks like this in the Elastic cloud console:\nGET / \nor\nPUT /example_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\nThe associated curl command for would like like this:\n\nLocal\n\nThe first call:\ncurl --cacert http_ca.crt -u elastic:[container instance password here] -X GET https://localhost:9200/\nor for the second call (note the use of single quotes around the JSON):\ncurl --cacert http_ca.crt -u elastic:[container instance password here] -X PUT https://localhost:9200/example_index\" -H 'Content-Type: application/json' -d'\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}'\n\nElastic Cloud\n\nThe first call:\ncurl -u elastic:[elastic cloud deployment password here] -X GET \"https://[elastic endpoint here]/\"\nor for the second call (note the use of single quotes around the JSON):\ncurl -u elastic:[elastic cloud deployment password here] -X PUT \"[elastic endpoint here]\" -H 'Content-Type: application/json' -d'\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}'"
  },
  {
    "objectID": "0.2-intro.html#how-to-pass-the-test",
    "href": "0.2-intro.html#how-to-pass-the-test",
    "title": "Introduction",
    "section": "",
    "text": "Do the examples and more. Be over-prepared.\nGo through the documentation so you know where to look when a task shows up in the certification exam and you are not sure what the syntax or format of a given JSON might be. This is an open book test, but the only book you can use is the Elastic documentation.\nLearn the basics. The Elastic console has code completion so you don’t have to remember everything…just what might be the appropriate JSON elements for the solution you are implementing.\nLearn the query syntax of Query DSL. This is a search engine after all and knowledge of querying is fundamental to the certification exam.\nWhile there is no magic bullet the exam should not be that hard if you already have knowledge of:\n\nsearch and search technologies (preferably hands-on)\nJSON\n\nThings that will trip you up:\n\nsyntax\ndepth of various JSON nodes"
  },
  {
    "objectID": "0.2-intro.html#summary",
    "href": "0.2-intro.html#summary",
    "title": "Introduction",
    "section": "",
    "text": "This book will not teach you Elastic search or Kibana or any of the other Elastic products available in the first half of 2024. It assumes you have an understanding of various search topics, JSON, REST APIs, and areas like regular expressions, and web technologies. The examples are valid with the current documentation and were all confirmed as working in the same time period.\nIf you run into any problems with the examples, please send an email to support@brushedsteelconsulting.com. When in doubt, asking one of the many public LLMs available should help as well:\n\nChatGPT\nClaude.ai\nGemini\nMeta.ai\nPerplexity.ai\n\nAll of the examples were originally generated by the various LLMs above sites with many changes made by an actual human as the examples and the generated content left much to be desired.\nDisclaimer: this book was written with the assistance of various tools include a host of LLMs, but always under the guidance of the author. Make of that what you will."
  },
  {
    "objectID": "3-developing-search-applications.html",
    "href": "3-developing-search-applications.html",
    "title": "Developing Search Applications",
    "section": "",
    "text": "Create an index and populate it with example documents.\nPerform a search query on the index.\nHighlight the search terms in the response.\nUse the Kibana Console or a REST client for all steps.\n\n\n\n\n\nOpen the Kibana Console or Use a REST Client\nCreate and Populate the Index\n\nPOST /blog_posts/_bulk\n\n{ \"index\": { \"_id\": \"1\" } }\n{ \"title\": \"Introduction to Elasticsearch\", \"content\": \"Elasticsearch is a powerful search engine.\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"title\": \"Advanced Elasticsearch Techniques\", \"content\": \"This guide covers advanced features of Elasticsearch.\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"title\": \"Elasticsearch Performance Tuning\", \"content\": \"Learn how to optimize Elasticsearch for better performance.\" }\n\nPerform a Search Query with Highlighting\n\nGET /blog_posts/_search\n\n{\n  \"query\": {\n    \"match\": {\n      \"content\": \"elasticsearch\"\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"content\": {}\n    }\n  }\n}\n\n\n\n\nConfirm the index exists\n\nGET /blog_posts\n\nExecute the query and confirm that the content field has highlighting\n\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_id\": \"1\",\n        \"_source\": {\n          \"title\": \"Introduction to Elasticsearch\",\n          \"content\": \"Elasticsearch is a powerful search engine.\"\n        },\n        \"highlight\": {\n          \"content\": [\n            \"&lt;em&gt;Elasticsearch&lt;/em&gt; is a powerful search engine.\"\n          ]\n        }\n      }\n      // Additional documents...\n    ]\n  }\n}\n\n\n\n\nField Selection: The highlight field in the search request specifies which fields to highlight. In this example, we highlight the content field.\nPerformance: Highlighting can impact search performance, especially on large datasets. It is essential to balance the need for highlighting with performance considerations.\nHighlight Configuration: Additional configurations, such as pre-tags and post-tags, can customize the highlighting format.\n\n\n\n\n\nDelete the index\n\nDELETE blog_posts\n\n\n\n\nHighlighting\nMatch Query\nBulk API\n\n\n\n\n\n\n\n\nAn index (orders) with documents containing customer order information (customer_name, order_date, products, total_price)\nA search query to retrieve orders with specific products and a total price range\nHighlighting the search terms in the response, including nested objects (products)\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the orders index by indexing some documents\n\nPOST /orders/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"customer_name\": \"John Doe\", \"order_date\": \"2022-01-01\", \"products\": [{ \"name\": \"Product A\", \"price\": 10.99 },{ \"name\": \"Product B\", \"price\": 5.99 }], \"total_price\": 16.98 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"customer_name\": \"Jane Smith\", \"order_date\": \"2022-01-15\", \"products\": [{ \"name\": \"Product B\", \"price\": 5.99 },{ \"name\": \"Product C\", \"price\": 7.99 }], \"total_price\": 13.98 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"customer_name\": \"Bob Johnson\", \"order_date\": \"2022-02-01\", \"products\": [{ \"name\": \"Product A\", \"price\": 10.99 },{ \"name\": \"Product C\", \"price\": 7.99 }], \"total_price\": 18.98 }\n\nExecute a search query with highlighting\n\nPOST /orders/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"match\": { \"products.name\": \"Product A\" } },\n        { \"range\": { \"total_price\": { \"gte\": 15, \"lte\": 20 } } }\n      ]\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"products.name\": {},\n      \"products.price\": {}\n    },\n    \"pre_tags\": [\"&lt;b&gt;\"],\n    \"post_tags\": [\"&lt;/b&gt;\"]\n  }\n}\n\n\n\n\nConfirm the index exists\n\nGET /orders\n\nExecute the query and confirm that products.name has highlighting\n\n{\n  \"hits\": [\n    {\n      \"_index\": \"orders\",\n      \"_id\": \"1\",\n      \"_score\": 1.6536093,\n      \"_source\": {\n        \"customer_name\": \"John Doe\",\n        \"order_date\": \"2022-01-01\",\n        \"products\": [\n          { \"name\": \"Product A\", \"price\": 10.99 },\n          { \"name\": \"Product B\", \"price\": 5.99 }\n        ],\n        \"total_price\": 16.98\n      },\n      \"highlight\": {\n        \"products.name\": [\n          \"&lt;b&gt;Product&lt;/b&gt; &lt;b&gt;A&lt;/b&gt;\",\n          \"&lt;b&gt;Product&lt;/b&gt; B\"\n        ]\n      }\n    }\n    // Additional documents...\n  ]\n}\n\n\n\n\nHighlighting is used to emphasize the search terms in the response, making it easier to see why a document matched the query.\nThe highlight section in the search query specifies which fields to highlight and how to format the highlighted text.\nNested objects (products) are highlighted using the fields section with dot notation (products.name, products.price).\n\n\n\n\n\nDelete the index\n\nDELETE orders\n\n\n\n\nHighlighting\nMatch Query\nBulk API\nElasticsearch Search API\nElasticsearch Nested Objects\n\n\n\n\n\n\n\n\n\n\n\nSearch for e-commerce product data in an index named products.\nSort the results by two criteria:\n\nPrimary Sort: In descending order by product price (highest to lowest).\nSecondary Sort: In ascending order by product name (alphabetically).\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the products index\n\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      }\n    }\n  }\n}\n\nIndex some documents\n\nPUT /products/_bulk\n{\"index\":{},\"action\":\"index\",\"_id\":\"1\"}\n{\"name\":\"Headphones\",\"price\":79.99}\n{\"index\":{},\"action\":\"index\",\"_id\":\"2\"}\n{\"name\":\"Smartwatch\",\"price\":249.99}\n{\"index\":{},\"action\":\"index\",\"_id\":\"3\"}\n{\"name\":\"Laptop\",\"price\":1299.99}\n{\"index\":{},\"action\":\"index\",\"_id\":\"4\"}\n{\"name\":\"Wireless Speaker\",\"price\":99.99}\n\nDefine a query to perform the primary sort\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"desc\"\n      }\n    }\n  ]\n}\n\nDefine a query to perform the secondary sort\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"name\": {\n        \"order\": \"asc\"\n      }\n    }\n  ]\n}\n\nCombine the two sorts and their impact on the results\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"desc\"\n      }\n    },\n    {\n      \"name\": {\n        \"order\": \"asc\"\n      }\n    }\n  ]\n}\n\n\n\n\nConfirm the index exists\n\nGET /products\n\nRun the search queries and examine the response\n\n\n\n\n\nThe sort clause defines the sorting criteria.\nAn array of sort definitions is specified, prioritizing them from top to bottom.\nIn this example, price is sorted in descending order (desc), while name is sorted in ascending order (asc).\n\n\n\n\n\nDelete the index\n\nDELETE products\n\n\n\n\nSort Options\n\n\n\n\n\n\nThere is only one example here as pagination is rather simple with very few configuration options.\n\n\n\n\n\nAn index named products with documents containing fields like name, price, category, description, etc.\nImplement pagination to retrieve search results in batches of 10 documents at a time.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex sample products documents\n\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Product A\",\"price\":99.99,\"category\":\"Electronics\",\"description\":\"High-quality product\"}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Product B\",\"price\":49.99,\"category\":\"Books\",\"description\":\"Best-selling novel\"}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Product C\",\"price\":149.99,\"category\":\"Electronics\",\"description\":\"Top-rated gadget\"}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Product D\",\"price\":29.99,\"category\":\"Clothing\",\"description\":\"Stylish t-shirt\"}\n{\"index\":{\"_id\":5}}\n{\"name\":\"Product E\",\"price\":19.99,\"category\":\"Books\",\"description\":\"Classic literature\"}\n\nDefine the initial search query with pagination\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    \"_doc\"\n  ],\n  \"from\": 0,\n  \"size\": 2\n}\nThis query will retrieve the first 2 documents sorted by the _doc field (document ID).\n\nTo retrieve the next page of results, use one of two methods:\n\nUpdate the from field with the document count to proceed from (not the document id)\n\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    \"_doc\"\n  ],\n  \"from\": 2,\n  \"size\": 2\n}\n\nUse the search_after parameter along with the sort values from the last hit in the previous page\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    \"_doc\"\n  ],\n  \"size\": 2,\n  \"search_after\": [1]\n}\n\n\n\n\nConfirm the index exists\n\nGET /products\n\nExecute the initial search query to retrieve the first 2 documents\nInspect the sort values in the response for the last hit\nExamine the results using both of the queries to return the next page of results\n\n\n\n\n\nThe sort parameter is used to ensure consistent ordering of results across pages.\nThe _doc field is used as a tiebreaker to ensure a stable sort order.\nThe size parameter specifies the number of documents to retrieve per page.\nThe from parameter is used for the initial query to start from the beginning.\nThe search_after parameter can be used for subsequent queries to retrieve the next page of results based on the sort values from the last hit or simply update the from parameter to start with the next group starting from a certain number of items in the search results.\n\n\n\n\n\nDelete the index\n\nDELETE products\n\n\n\n\nElasticsearch Pagination\nSearch After API\nSort Search Results\n\n\n\n\n\n\n\n\nThis is an example of the simplest kind of alias.\n\n\n\nCreate multiple indices for customer data (e.g., customers-2024-01, customers-2024-02).\nCreate an alias that points to these indices.\nUse the alias to perform search operations across all customer indices.\n\n\n\n\n\nOpen the Kibana Console or use a REST Client.\nCreate the 2 indices as a side-effect of indexing sample documents\n\nPOST /customers-2024-01/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"John Doe\", \"email\": \"john.doe@example.com\", \"signup_date\": \"2024-01-15\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Jane Smith\", \"email\": \"jane.smith@example.com\", \"signup_date\": \"2024-01-20\" }\nPOST /customers-2024-02/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"Alice Johnson\", \"email\": \"alice.johnson@example.com\", \"signup_date\": \"2024-02-05\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Bob Brown\", \"email\": \"bob.brown@example.com\", \"signup_date\": \"2024-02-10\" }\n\nCreate an alias for the two indices\n\nPOST /_aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"customers-2024-01\",\n        \"alias\": \"customers-current\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"customers-2024-02\",\n        \"alias\": \"customers-current\"\n      }\n    }\n  ]\n}\n\nExecute a search query using the alias and confirm 4 documents returned\n\nGET /customers-current/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nVerify the alias was created\n\nGET /_alias/customers-current\n\nConfirm 4 documents returned when executing the test query\n\nGET /customers-current/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\nOr just\nGET /customers-current/_search\n\n\n\n\nAlias Flexibility: Using an alias allows for flexibility in managing indices. The alias can point to multiple indices, making it easier to manage and query data across time-based indices.\nIndex Patterns: Ensure that the alias name (customers-current) is descriptive and clearly indicates its purpose.\nPerformance: Searching using an alias is efficient and does not introduce significant overhead compared to searching directly on indices.\n\n\n\n\n\nDelete the aliases\n\nDELETE customers-2024-01/_alias/customers\nDELETE customers-2024-02/_alias/customers\n\nDelete the 2 indices\n\nDELETE customers-2024-01\nDELETE customers-2024-02\n\n\n\n\nBulk API\nDelete Aliases\nIndex Aliases\nSearch API\n\n\n\n\n\nThis is a slightly more complex use of an index alias. It includes a custom configuration for each index defined in the alias and any custom filtering and/or routing that is required.\n\n\n\nThree indices (logs_2022, logs_2023, and logs_2024) with documents containing log data (message, level, timestamp)\nAn index alias (logs) that points to all three indices with filtering and routing based on the log level\nA search query against the message field to retrieve documents from the alias\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the logs_2022, logs_2023, and logs_2024 indices\n\nPUT logs_2022\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"level\": {\n        \"type\": \"keyword\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\nPUT logs_2023\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"level\": {\n        \"type\": \"keyword\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\nPUT logs_2024\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"level\": {\n        \"type\": \"keyword\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n\nCreate an index alias (logs) with filtering and routing (this must be done before indexing any documents)\n\nPOST /_aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"logs_2022\",\n        \"alias\": \"logs\",\n        \"filter\": {\n          \"term\": {\n            \"level\": \"ERROR\"\n          }\n        },\n        \"routing\": \"error\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"logs_2023\",\n        \"alias\": \"logs\",\n        \"filter\": {\n          \"term\": {\n            \"level\": \"INFO\"\n          }\n        },\n        \"routing\": \"info\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"logs_2024\",\n        \"alias\": \"logs\",\n        \"filter\": {\n          \"term\": {\n            \"level\": \"DEBUG\"\n          }\n        },\n        \"routing\": \"debug\"\n      }\n    }\n  ]\n}\n\nIndex sample documents\n\nPOST /logs_2022/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"message\": \"Error occurred\", \"level\": \"ERROR\", \"timestamp\": \"2022-01-01T12:00:00Z\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"message\": \"Error occurred\", \"level\": \"ERROR\", \"timestamp\": \"2022-01-01T12:00:00Z\" }\nPOST /logs_2023/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"message\": \"Info message\", \"level\": \"INFO\", \"timestamp\": \"2023-01-01T12:00:01Z\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"message\": \"Info message\", \"level\": \"INFO\", \"timestamp\": \"2023-01-01T12:00:01Z\" }\nPOST /logs_2024/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"message\": \"Debug message\", \"level\": \"DEBUG\", \"timestamp\": \"2024-01-01T12:00:01Z\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"message\": \"Debug message\", \"level\": \"DEBUG\", \"timestamp\": \"2024-01-01T12:00:01Z\" }\n\nExecute a search query using the logs alias\n\nGET /logs/_search\n{\n  \"query\": {\n    \"terms\": {\n      \"message\": [\n        \"error\",\n        \"info\"\n      ]\n    }\n  }\n}\n\n\n\n\nVerify the alias was created\n\nGET /_alias/logs\n\nConfirm 4 documents returned when executing the test query: 2 from logs_2022 and 2 from logs_2023\n\n\n\n\n\nThe index must be set up in the proper order for the query using the alias with filtering and routing to work:\n\ncreate the index\ncreate the alias using filtering and/or routing\nindex the documents\n\nIndex aliases with filtering and routing allow you to control which documents are included in the alias based on specific criteria.\nIn this example, we created an alias that points to three indices with filtering based on the log level and routing to separate indices.\n\n\n\n\n\nDelete the aliases\n\nDELETE logs_2022/_alias/logs\nDELETE logs_2023/_alias/logs\nDELETE logs_2024/_alias/logs\n\nDelete the indices\n\nDELETE logs_2022\nDELETE logs_2023\nDELETE logs_2024\n\n\n\n\nBulk API\nDelete Aliases\nIndex Aliases\nSearch API\n\n\n\n\n\n\n\n\n\n\n\nCreate an index and populate it with example product documents\nDefine a search template for querying products based on a keyword\n\n\n\n\n\nOpen the Kibana Console or use a REST Client\nCreate the index as a side-effect of indexing sample documents\n\nPOST /products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"Laptop\", \"description\": \"A high-performance laptop\", \"price\": 999.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Smartphone\", \"description\": \"A latest model smartphone\", \"price\": 799.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"name\": \"Tablet\", \"description\": \"A new tablet with excellent features\", \"price\": 499.99 }\n\nDefine a search template\n\nPOST /_scripts/product_search\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"match\": {\n          \"description\": \"{{query_string}}\"\n        }\n      }\n    }\n  }\n}\n\nUse the search template\n\nGET products/_search/template\n{\n  \"id\": \"product_search_template\",\n  \"params\": {\n    \"query_string\": \"laptop\"\n  }\n}\nOr\nPOST _search/template\n{\n  \"id\": \"product_search_template\",\n  \"params\": {\n    \"query_string\": \"laptop\"\n  }\n}\n\n\n\n\nVerify the documents are indexed\n\nGET products/_search\n\nVerify the template was created\n\nGET _scripts/product_search_template\n\nPerform a search using the template (results below)\n\n{\n  \"hits\": {\n    \"total\": {\n      \"value\": 1,\n      \"relation\": \"eq\"\n    },\n    \"hits\": [\n      {\n        \"_index\": \"products\",\n        \"_id\": \"1\",\n        \"_source\": {\n          \"name\": \"Laptop\",\n          \"description\": \"A high-performance laptop\",\n          \"price\": 999.99\n        }\n      }\n    ]\n  }\n}\n\n\n\n\nTemplate Flexibility: Using a search template allows for reusable and parameterized queries, reducing the need to write complex queries multiple times.\nPerformance: Search templates can improve performance by reusing the query logic and reducing the overhead of constructing queries dynamically.\nTemplate Language: Mustache is used as the templating language, providing a simple and powerful way to define dynamic queries.\n\n\n\n\n\nDelete the search template\n\nDELETE _scripts/product_search_template\n\nDelete the index\n\nDELETE products\n\n\n\n\nBulk API\nMustache\nSearch API\nSearch Templates\n\n\n\n\n\n\n\n\nAn Elasticsearch index named products with documents containing fields like name, price, category, description, rating, etc.\nDefine a search template to search for products based on a user-provided query string, category filter, sort order, and pagination.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex sample product documents using the /_bulk endpoint:\n\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Product A\", \"price\":99.99, \"category\":\"Electronics\", \"description\":\"High-quality product\", \"rating\":4.2}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Product B\", \"price\":49.99, \"category\":\"Books\", \"description\":\"Best-selling novel\", \"rating\":4.5}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Product C\", \"price\":149.99, \"category\":\"Electronics\", \"description\":\"Top-rated gadget\", \"rating\":3.8}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Product D\", \"price\":29.99, \"category\":\"Clothing\", \"description\":\"Stylish t-shirt\", \"rating\":4.1}\n\nDefine the search template:\n\nPUT _scripts/product_search_template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"term\": {\n                \"name\": {\n                  \"value\": \"{{product}}\"\n                }\n              }\n            }\n          ],\n          \"filter\": [\n            {\n              \"term\": {\n                \"category\": \"{{category}}\"\n              }\n            }\n          ]\n        }\n      },\n      \"sort\": [\n        {\n          \"{{sort_field}}\": {\n            \"order\": \"asc\"\n          }\n        }\n      ],\n      \"from\": \"{{from}}\",\n      \"size\": \"{{size}}\"\n    }\n  }\n}\n\nUse the search template with sorting and pagination:\n\nGET products/_search/template\n{\n  \"id\": \"product_search_template\",\n  \"params\": {\n    \"product\" : \"product\",\n    \"category\" : \"electronics\",\n    \"sort_field\" : \"price\",\n    \"from\": 0,\n    \"size\" : 2\n  }\n}\n\n\n\n\nVerify the documents are indexed\n\nGET products/_search\n\nVerify the template is created\n\nGET _scripts/product_search_template\n\nExecute a query using the search template, and it should return the first 2 documents matching the provided query string (“product”), category filter (“Electronics”), sorted by price in descending order.\nTo retrieve the next page of results, update the from parameter in the params object and execute the query again.\n\n\n\n\n\nThe search template includes sorting and pagination parameters (sort_field, sort_order, from, size).\nThe sort parameter in the template specifies the field and order for sorting the results.\nThe from and size parameters control the pagination of the results.\nThe params object in the search template request provides the values for all placeholders in the template.\n\n\n\n\n\nDelete the search template\n\nDELETE _scripts/product_search_template\n\nDelete the index\n\nDELETE products\n\n\n\n\nElasticsearch Search Template\nMustache Language\nPaginate Search Results\nScripting in Elasticsearch\nSort Search Results\n\n\n\n\n\n\n\n\nAn Elasticsearch index named products with documents containing fields like name, price, category, description, rating, tags, specifications (nested object), etc.\nDefine a search template to search for products based on a user-provided query string, category filter, tag filter, sort order, pagination, and include aggregations for faceted search.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index with two fields (category and tags) of type keyword for use in the aggregation\n\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"price\" : {\n        \"type\": \"float\"\n      },\n      \"category\" : {\n        \"type\": \"keyword\"\n      },\n      \"description\" : {\n        \"type\": \"text\"\n      },\n      \"rating\" : {\n        \"type\": \"float\"\n      },\n      \"tags\" : {\n        \"type\": \"keyword\"\n      },\n      \"specifications\" : {\n        \"properties\": {\n          \"ram\" : {\n            \"type\" : \"text\"\n          },\n          \"storage\" : {\n            \"type\" : \"text\"\n          }\n        }\n      }\n    }\n  }\n}\n\nIndex sample products documents\n\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Product A\", \"price\":99.99, \"category\":\"Electronics\", \"description\":\"High-quality product\", \"rating\":4.2, \"tags\":[\"electronics\", \"gadget\"], \"specifications\":{\"ram\":\"8GB\", \"storage\":\"256GB\"}}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Product B\", \"price\":49.99, \"category\":\"Books\", \"description\":\"Best-selling novel\", \"rating\":4.5, \"tags\":[\"book\", \"fiction\"]}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Product C\", \"price\":149.99, \"category\":\"Electronics\", \"description\":\"Top-rated gadget\", \"rating\":3.8, \"tags\":[\"electronics\", \"laptop\"], \"specifications\":{\"ram\":\"16GB\", \"storage\":\"512GB\"}}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Product D\", \"price\":29.99, \"category\":\"Clothing\", \"description\":\"Stylish t-shirt\", \"rating\":4.1, \"tags\":[\"clothing\", \"tshirt\"]}\n\nDefine the search template\n\nPUT _scripts/products_search_template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"query_string\": {\n                \"default_field\": \"{{default_field}}\",\n                \"query\": \"{{query}}\"\n              }\n            }\n          ],\n          \"filter\": [\n            {\n              \"term\": {\n                \"category\": \"{{category_filter}}\"\n              }\n            },\n            {\n              \"term\": {\n                \"tags\": \"{{tags_filter}}\"\n              }\n            }\n          ]\n        }\n      },\n      \"sort\": [\n        {\n          \"{{sort_field}}\": {\n            \"order\": \"{{sort_order}}\"\n          }\n        }\n      ],\n      \"from\": \"{{from}}\",\n      \"size\": \"{{page_size}}\",\n      \"aggs\": {\n        \"category_agg\": {\n          \"terms\": {\n            \"field\": \"category\"\n          }\n        },\n        \"tags_agg\": {\n          \"terms\": {\n            \"field\": \"tags\"\n          }\n        },\n        \"price_range\": {\n          \"range\": {\n            \"field\": \"price\",\n            \"ranges\": [\n              {\n                \"from\": 50,\n                \"to\": 200\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nUse the search template with sorting, pagination, and aggregations:\n\nGET /products/_search/template\n{\n  \"id\": \"products_search_template\",\n  \"params\": {\n    \"default_field\": \"description\",\n    \"query\": \"*\",\n    \"category_filter\": \"Electronics\",\n    \"tags_filter\": \"electronics\",\n    \"sort_field\": \"price\",\n    \"sort_order\": \"desc\",\n    \"from\": 0,\n    \"page_size\": 2\n  }\n}\n\n\n\n\nVerify the index is created\n\nGET products\n\nVerify the documents are indexed\n\nGET products/_search\n\nVerify the template is created\n\nGET _scripts/product_search_template\n\nExecute a search using the search template query, and it should return the first 2 documents matching the provided query string (“*“), category filter (”Electronics”), tag filter (“electronics”), sorted by price in descending order.\n\nThe response should also include aggregations for category, tags, and price.\n\nTo retrieve the next page of results, update the from parameter in the params object and execute the query again.\n\n\n\n\n\nThe search template includes nested queries, filters, sorting, pagination, and aggregations.\nThe tags filter uses a terms query to match documents with any of the specified tags.\nThe specifications aggregation is a nested aggregation that aggregates the nested specifications object.\nThe aggs section in the template defines the aggregations to be included in the search results.\nThe params object in the search template request provides the values for all placeholders in the template.\n\n\n\n\n\nDelete the search template\n\nDELETE _scripts/product_search_template\n\nDelete the index\n\nDELETE products\n\n\n\n\nAggregations\nMustache Language\nNested Aggregations\nPaginate Search Results\nScripting in Elasticsearch\nSearch Template\nSort Search Results"
  },
  {
    "objectID": "3-developing-search-applications.html#task-highlight-the-search-terms-in-the-response-of-a-query",
    "href": "3-developing-search-applications.html#task-highlight-the-search-terms-in-the-response-of-a-query",
    "title": "Developing Search Applications",
    "section": "",
    "text": "Create an index and populate it with example documents.\nPerform a search query on the index.\nHighlight the search terms in the response.\nUse the Kibana Console or a REST client for all steps.\n\n\n\n\n\nOpen the Kibana Console or Use a REST Client\nCreate and Populate the Index\n\nPOST /blog_posts/_bulk\n\n{ \"index\": { \"_id\": \"1\" } }\n{ \"title\": \"Introduction to Elasticsearch\", \"content\": \"Elasticsearch is a powerful search engine.\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"title\": \"Advanced Elasticsearch Techniques\", \"content\": \"This guide covers advanced features of Elasticsearch.\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"title\": \"Elasticsearch Performance Tuning\", \"content\": \"Learn how to optimize Elasticsearch for better performance.\" }\n\nPerform a Search Query with Highlighting\n\nGET /blog_posts/_search\n\n{\n  \"query\": {\n    \"match\": {\n      \"content\": \"elasticsearch\"\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"content\": {}\n    }\n  }\n}\n\n\n\n\nConfirm the index exists\n\nGET /blog_posts\n\nExecute the query and confirm that the content field has highlighting\n\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_id\": \"1\",\n        \"_source\": {\n          \"title\": \"Introduction to Elasticsearch\",\n          \"content\": \"Elasticsearch is a powerful search engine.\"\n        },\n        \"highlight\": {\n          \"content\": [\n            \"&lt;em&gt;Elasticsearch&lt;/em&gt; is a powerful search engine.\"\n          ]\n        }\n      }\n      // Additional documents...\n    ]\n  }\n}\n\n\n\n\nField Selection: The highlight field in the search request specifies which fields to highlight. In this example, we highlight the content field.\nPerformance: Highlighting can impact search performance, especially on large datasets. It is essential to balance the need for highlighting with performance considerations.\nHighlight Configuration: Additional configurations, such as pre-tags and post-tags, can customize the highlighting format.\n\n\n\n\n\nDelete the index\n\nDELETE blog_posts\n\n\n\n\nHighlighting\nMatch Query\nBulk API\n\n\n\n\n\n\n\n\nAn index (orders) with documents containing customer order information (customer_name, order_date, products, total_price)\nA search query to retrieve orders with specific products and a total price range\nHighlighting the search terms in the response, including nested objects (products)\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the orders index by indexing some documents\n\nPOST /orders/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"customer_name\": \"John Doe\", \"order_date\": \"2022-01-01\", \"products\": [{ \"name\": \"Product A\", \"price\": 10.99 },{ \"name\": \"Product B\", \"price\": 5.99 }], \"total_price\": 16.98 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"customer_name\": \"Jane Smith\", \"order_date\": \"2022-01-15\", \"products\": [{ \"name\": \"Product B\", \"price\": 5.99 },{ \"name\": \"Product C\", \"price\": 7.99 }], \"total_price\": 13.98 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"customer_name\": \"Bob Johnson\", \"order_date\": \"2022-02-01\", \"products\": [{ \"name\": \"Product A\", \"price\": 10.99 },{ \"name\": \"Product C\", \"price\": 7.99 }], \"total_price\": 18.98 }\n\nExecute a search query with highlighting\n\nPOST /orders/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"match\": { \"products.name\": \"Product A\" } },\n        { \"range\": { \"total_price\": { \"gte\": 15, \"lte\": 20 } } }\n      ]\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"products.name\": {},\n      \"products.price\": {}\n    },\n    \"pre_tags\": [\"&lt;b&gt;\"],\n    \"post_tags\": [\"&lt;/b&gt;\"]\n  }\n}\n\n\n\n\nConfirm the index exists\n\nGET /orders\n\nExecute the query and confirm that products.name has highlighting\n\n{\n  \"hits\": [\n    {\n      \"_index\": \"orders\",\n      \"_id\": \"1\",\n      \"_score\": 1.6536093,\n      \"_source\": {\n        \"customer_name\": \"John Doe\",\n        \"order_date\": \"2022-01-01\",\n        \"products\": [\n          { \"name\": \"Product A\", \"price\": 10.99 },\n          { \"name\": \"Product B\", \"price\": 5.99 }\n        ],\n        \"total_price\": 16.98\n      },\n      \"highlight\": {\n        \"products.name\": [\n          \"&lt;b&gt;Product&lt;/b&gt; &lt;b&gt;A&lt;/b&gt;\",\n          \"&lt;b&gt;Product&lt;/b&gt; B\"\n        ]\n      }\n    }\n    // Additional documents...\n  ]\n}\n\n\n\n\nHighlighting is used to emphasize the search terms in the response, making it easier to see why a document matched the query.\nThe highlight section in the search query specifies which fields to highlight and how to format the highlighted text.\nNested objects (products) are highlighted using the fields section with dot notation (products.name, products.price).\n\n\n\n\n\nDelete the index\n\nDELETE orders\n\n\n\n\nHighlighting\nMatch Query\nBulk API\nElasticsearch Search API\nElasticsearch Nested Objects"
  },
  {
    "objectID": "3-developing-search-applications.html#task-sort-the-results-of-a-query-by-a-given-set-of-requirements",
    "href": "3-developing-search-applications.html#task-sort-the-results-of-a-query-by-a-given-set-of-requirements",
    "title": "Developing Search Applications",
    "section": "",
    "text": "Search for e-commerce product data in an index named products.\nSort the results by two criteria:\n\nPrimary Sort: In descending order by product price (highest to lowest).\nSecondary Sort: In ascending order by product name (alphabetically).\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the products index\n\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      }\n    }\n  }\n}\n\nIndex some documents\n\nPUT /products/_bulk\n{\"index\":{},\"action\":\"index\",\"_id\":\"1\"}\n{\"name\":\"Headphones\",\"price\":79.99}\n{\"index\":{},\"action\":\"index\",\"_id\":\"2\"}\n{\"name\":\"Smartwatch\",\"price\":249.99}\n{\"index\":{},\"action\":\"index\",\"_id\":\"3\"}\n{\"name\":\"Laptop\",\"price\":1299.99}\n{\"index\":{},\"action\":\"index\",\"_id\":\"4\"}\n{\"name\":\"Wireless Speaker\",\"price\":99.99}\n\nDefine a query to perform the primary sort\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"desc\"\n      }\n    }\n  ]\n}\n\nDefine a query to perform the secondary sort\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"name\": {\n        \"order\": \"asc\"\n      }\n    }\n  ]\n}\n\nCombine the two sorts and their impact on the results\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"desc\"\n      }\n    },\n    {\n      \"name\": {\n        \"order\": \"asc\"\n      }\n    }\n  ]\n}\n\n\n\n\nConfirm the index exists\n\nGET /products\n\nRun the search queries and examine the response\n\n\n\n\n\nThe sort clause defines the sorting criteria.\nAn array of sort definitions is specified, prioritizing them from top to bottom.\nIn this example, price is sorted in descending order (desc), while name is sorted in ascending order (asc).\n\n\n\n\n\nDelete the index\n\nDELETE products\n\n\n\n\nSort Options"
  },
  {
    "objectID": "3-developing-search-applications.html#task-implement-pagination-of-the-results-of-a-search-query",
    "href": "3-developing-search-applications.html#task-implement-pagination-of-the-results-of-a-search-query",
    "title": "Developing Search Applications",
    "section": "",
    "text": "There is only one example here as pagination is rather simple with very few configuration options.\n\n\n\n\n\nAn index named products with documents containing fields like name, price, category, description, etc.\nImplement pagination to retrieve search results in batches of 10 documents at a time.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex sample products documents\n\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Product A\",\"price\":99.99,\"category\":\"Electronics\",\"description\":\"High-quality product\"}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Product B\",\"price\":49.99,\"category\":\"Books\",\"description\":\"Best-selling novel\"}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Product C\",\"price\":149.99,\"category\":\"Electronics\",\"description\":\"Top-rated gadget\"}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Product D\",\"price\":29.99,\"category\":\"Clothing\",\"description\":\"Stylish t-shirt\"}\n{\"index\":{\"_id\":5}}\n{\"name\":\"Product E\",\"price\":19.99,\"category\":\"Books\",\"description\":\"Classic literature\"}\n\nDefine the initial search query with pagination\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    \"_doc\"\n  ],\n  \"from\": 0,\n  \"size\": 2\n}\nThis query will retrieve the first 2 documents sorted by the _doc field (document ID).\n\nTo retrieve the next page of results, use one of two methods:\n\nUpdate the from field with the document count to proceed from (not the document id)\n\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    \"_doc\"\n  ],\n  \"from\": 2,\n  \"size\": 2\n}\n\nUse the search_after parameter along with the sort values from the last hit in the previous page\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    \"_doc\"\n  ],\n  \"size\": 2,\n  \"search_after\": [1]\n}\n\n\n\n\nConfirm the index exists\n\nGET /products\n\nExecute the initial search query to retrieve the first 2 documents\nInspect the sort values in the response for the last hit\nExamine the results using both of the queries to return the next page of results\n\n\n\n\n\nThe sort parameter is used to ensure consistent ordering of results across pages.\nThe _doc field is used as a tiebreaker to ensure a stable sort order.\nThe size parameter specifies the number of documents to retrieve per page.\nThe from parameter is used for the initial query to start from the beginning.\nThe search_after parameter can be used for subsequent queries to retrieve the next page of results based on the sort values from the last hit or simply update the from parameter to start with the next group starting from a certain number of items in the search results.\n\n\n\n\n\nDelete the index\n\nDELETE products\n\n\n\n\nElasticsearch Pagination\nSearch After API\nSort Search Results"
  },
  {
    "objectID": "3-developing-search-applications.html#task-define-and-use-index-aliases",
    "href": "3-developing-search-applications.html#task-define-and-use-index-aliases",
    "title": "Developing Search Applications",
    "section": "",
    "text": "This is an example of the simplest kind of alias.\n\n\n\nCreate multiple indices for customer data (e.g., customers-2024-01, customers-2024-02).\nCreate an alias that points to these indices.\nUse the alias to perform search operations across all customer indices.\n\n\n\n\n\nOpen the Kibana Console or use a REST Client.\nCreate the 2 indices as a side-effect of indexing sample documents\n\nPOST /customers-2024-01/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"John Doe\", \"email\": \"john.doe@example.com\", \"signup_date\": \"2024-01-15\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Jane Smith\", \"email\": \"jane.smith@example.com\", \"signup_date\": \"2024-01-20\" }\nPOST /customers-2024-02/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"Alice Johnson\", \"email\": \"alice.johnson@example.com\", \"signup_date\": \"2024-02-05\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Bob Brown\", \"email\": \"bob.brown@example.com\", \"signup_date\": \"2024-02-10\" }\n\nCreate an alias for the two indices\n\nPOST /_aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"customers-2024-01\",\n        \"alias\": \"customers-current\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"customers-2024-02\",\n        \"alias\": \"customers-current\"\n      }\n    }\n  ]\n}\n\nExecute a search query using the alias and confirm 4 documents returned\n\nGET /customers-current/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nVerify the alias was created\n\nGET /_alias/customers-current\n\nConfirm 4 documents returned when executing the test query\n\nGET /customers-current/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\nOr just\nGET /customers-current/_search\n\n\n\n\nAlias Flexibility: Using an alias allows for flexibility in managing indices. The alias can point to multiple indices, making it easier to manage and query data across time-based indices.\nIndex Patterns: Ensure that the alias name (customers-current) is descriptive and clearly indicates its purpose.\nPerformance: Searching using an alias is efficient and does not introduce significant overhead compared to searching directly on indices.\n\n\n\n\n\nDelete the aliases\n\nDELETE customers-2024-01/_alias/customers\nDELETE customers-2024-02/_alias/customers\n\nDelete the 2 indices\n\nDELETE customers-2024-01\nDELETE customers-2024-02\n\n\n\n\nBulk API\nDelete Aliases\nIndex Aliases\nSearch API\n\n\n\n\n\nThis is a slightly more complex use of an index alias. It includes a custom configuration for each index defined in the alias and any custom filtering and/or routing that is required.\n\n\n\nThree indices (logs_2022, logs_2023, and logs_2024) with documents containing log data (message, level, timestamp)\nAn index alias (logs) that points to all three indices with filtering and routing based on the log level\nA search query against the message field to retrieve documents from the alias\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the logs_2022, logs_2023, and logs_2024 indices\n\nPUT logs_2022\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"level\": {\n        \"type\": \"keyword\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\nPUT logs_2023\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"level\": {\n        \"type\": \"keyword\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\nPUT logs_2024\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"level\": {\n        \"type\": \"keyword\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n\nCreate an index alias (logs) with filtering and routing (this must be done before indexing any documents)\n\nPOST /_aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"logs_2022\",\n        \"alias\": \"logs\",\n        \"filter\": {\n          \"term\": {\n            \"level\": \"ERROR\"\n          }\n        },\n        \"routing\": \"error\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"logs_2023\",\n        \"alias\": \"logs\",\n        \"filter\": {\n          \"term\": {\n            \"level\": \"INFO\"\n          }\n        },\n        \"routing\": \"info\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"logs_2024\",\n        \"alias\": \"logs\",\n        \"filter\": {\n          \"term\": {\n            \"level\": \"DEBUG\"\n          }\n        },\n        \"routing\": \"debug\"\n      }\n    }\n  ]\n}\n\nIndex sample documents\n\nPOST /logs_2022/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"message\": \"Error occurred\", \"level\": \"ERROR\", \"timestamp\": \"2022-01-01T12:00:00Z\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"message\": \"Error occurred\", \"level\": \"ERROR\", \"timestamp\": \"2022-01-01T12:00:00Z\" }\nPOST /logs_2023/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"message\": \"Info message\", \"level\": \"INFO\", \"timestamp\": \"2023-01-01T12:00:01Z\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"message\": \"Info message\", \"level\": \"INFO\", \"timestamp\": \"2023-01-01T12:00:01Z\" }\nPOST /logs_2024/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"message\": \"Debug message\", \"level\": \"DEBUG\", \"timestamp\": \"2024-01-01T12:00:01Z\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"message\": \"Debug message\", \"level\": \"DEBUG\", \"timestamp\": \"2024-01-01T12:00:01Z\" }\n\nExecute a search query using the logs alias\n\nGET /logs/_search\n{\n  \"query\": {\n    \"terms\": {\n      \"message\": [\n        \"error\",\n        \"info\"\n      ]\n    }\n  }\n}\n\n\n\n\nVerify the alias was created\n\nGET /_alias/logs\n\nConfirm 4 documents returned when executing the test query: 2 from logs_2022 and 2 from logs_2023\n\n\n\n\n\nThe index must be set up in the proper order for the query using the alias with filtering and routing to work:\n\ncreate the index\ncreate the alias using filtering and/or routing\nindex the documents\n\nIndex aliases with filtering and routing allow you to control which documents are included in the alias based on specific criteria.\nIn this example, we created an alias that points to three indices with filtering based on the log level and routing to separate indices.\n\n\n\n\n\nDelete the aliases\n\nDELETE logs_2022/_alias/logs\nDELETE logs_2023/_alias/logs\nDELETE logs_2024/_alias/logs\n\nDelete the indices\n\nDELETE logs_2022\nDELETE logs_2023\nDELETE logs_2024\n\n\n\n\nBulk API\nDelete Aliases\nIndex Aliases\nSearch API"
  },
  {
    "objectID": "3-developing-search-applications.html#task-define-and-use-a-search-template",
    "href": "3-developing-search-applications.html#task-define-and-use-a-search-template",
    "title": "Developing Search Applications",
    "section": "",
    "text": "Create an index and populate it with example product documents\nDefine a search template for querying products based on a keyword\n\n\n\n\n\nOpen the Kibana Console or use a REST Client\nCreate the index as a side-effect of indexing sample documents\n\nPOST /products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"Laptop\", \"description\": \"A high-performance laptop\", \"price\": 999.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Smartphone\", \"description\": \"A latest model smartphone\", \"price\": 799.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"name\": \"Tablet\", \"description\": \"A new tablet with excellent features\", \"price\": 499.99 }\n\nDefine a search template\n\nPOST /_scripts/product_search\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"match\": {\n          \"description\": \"{{query_string}}\"\n        }\n      }\n    }\n  }\n}\n\nUse the search template\n\nGET products/_search/template\n{\n  \"id\": \"product_search_template\",\n  \"params\": {\n    \"query_string\": \"laptop\"\n  }\n}\nOr\nPOST _search/template\n{\n  \"id\": \"product_search_template\",\n  \"params\": {\n    \"query_string\": \"laptop\"\n  }\n}\n\n\n\n\nVerify the documents are indexed\n\nGET products/_search\n\nVerify the template was created\n\nGET _scripts/product_search_template\n\nPerform a search using the template (results below)\n\n{\n  \"hits\": {\n    \"total\": {\n      \"value\": 1,\n      \"relation\": \"eq\"\n    },\n    \"hits\": [\n      {\n        \"_index\": \"products\",\n        \"_id\": \"1\",\n        \"_source\": {\n          \"name\": \"Laptop\",\n          \"description\": \"A high-performance laptop\",\n          \"price\": 999.99\n        }\n      }\n    ]\n  }\n}\n\n\n\n\nTemplate Flexibility: Using a search template allows for reusable and parameterized queries, reducing the need to write complex queries multiple times.\nPerformance: Search templates can improve performance by reusing the query logic and reducing the overhead of constructing queries dynamically.\nTemplate Language: Mustache is used as the templating language, providing a simple and powerful way to define dynamic queries.\n\n\n\n\n\nDelete the search template\n\nDELETE _scripts/product_search_template\n\nDelete the index\n\nDELETE products\n\n\n\n\nBulk API\nMustache\nSearch API\nSearch Templates\n\n\n\n\n\n\n\n\nAn Elasticsearch index named products with documents containing fields like name, price, category, description, rating, etc.\nDefine a search template to search for products based on a user-provided query string, category filter, sort order, and pagination.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex sample product documents using the /_bulk endpoint:\n\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Product A\", \"price\":99.99, \"category\":\"Electronics\", \"description\":\"High-quality product\", \"rating\":4.2}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Product B\", \"price\":49.99, \"category\":\"Books\", \"description\":\"Best-selling novel\", \"rating\":4.5}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Product C\", \"price\":149.99, \"category\":\"Electronics\", \"description\":\"Top-rated gadget\", \"rating\":3.8}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Product D\", \"price\":29.99, \"category\":\"Clothing\", \"description\":\"Stylish t-shirt\", \"rating\":4.1}\n\nDefine the search template:\n\nPUT _scripts/product_search_template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"term\": {\n                \"name\": {\n                  \"value\": \"{{product}}\"\n                }\n              }\n            }\n          ],\n          \"filter\": [\n            {\n              \"term\": {\n                \"category\": \"{{category}}\"\n              }\n            }\n          ]\n        }\n      },\n      \"sort\": [\n        {\n          \"{{sort_field}}\": {\n            \"order\": \"asc\"\n          }\n        }\n      ],\n      \"from\": \"{{from}}\",\n      \"size\": \"{{size}}\"\n    }\n  }\n}\n\nUse the search template with sorting and pagination:\n\nGET products/_search/template\n{\n  \"id\": \"product_search_template\",\n  \"params\": {\n    \"product\" : \"product\",\n    \"category\" : \"electronics\",\n    \"sort_field\" : \"price\",\n    \"from\": 0,\n    \"size\" : 2\n  }\n}\n\n\n\n\nVerify the documents are indexed\n\nGET products/_search\n\nVerify the template is created\n\nGET _scripts/product_search_template\n\nExecute a query using the search template, and it should return the first 2 documents matching the provided query string (“product”), category filter (“Electronics”), sorted by price in descending order.\nTo retrieve the next page of results, update the from parameter in the params object and execute the query again.\n\n\n\n\n\nThe search template includes sorting and pagination parameters (sort_field, sort_order, from, size).\nThe sort parameter in the template specifies the field and order for sorting the results.\nThe from and size parameters control the pagination of the results.\nThe params object in the search template request provides the values for all placeholders in the template.\n\n\n\n\n\nDelete the search template\n\nDELETE _scripts/product_search_template\n\nDelete the index\n\nDELETE products\n\n\n\n\nElasticsearch Search Template\nMustache Language\nPaginate Search Results\nScripting in Elasticsearch\nSort Search Results\n\n\n\n\n\n\n\n\nAn Elasticsearch index named products with documents containing fields like name, price, category, description, rating, tags, specifications (nested object), etc.\nDefine a search template to search for products based on a user-provided query string, category filter, tag filter, sort order, pagination, and include aggregations for faceted search.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index with two fields (category and tags) of type keyword for use in the aggregation\n\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"price\" : {\n        \"type\": \"float\"\n      },\n      \"category\" : {\n        \"type\": \"keyword\"\n      },\n      \"description\" : {\n        \"type\": \"text\"\n      },\n      \"rating\" : {\n        \"type\": \"float\"\n      },\n      \"tags\" : {\n        \"type\": \"keyword\"\n      },\n      \"specifications\" : {\n        \"properties\": {\n          \"ram\" : {\n            \"type\" : \"text\"\n          },\n          \"storage\" : {\n            \"type\" : \"text\"\n          }\n        }\n      }\n    }\n  }\n}\n\nIndex sample products documents\n\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Product A\", \"price\":99.99, \"category\":\"Electronics\", \"description\":\"High-quality product\", \"rating\":4.2, \"tags\":[\"electronics\", \"gadget\"], \"specifications\":{\"ram\":\"8GB\", \"storage\":\"256GB\"}}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Product B\", \"price\":49.99, \"category\":\"Books\", \"description\":\"Best-selling novel\", \"rating\":4.5, \"tags\":[\"book\", \"fiction\"]}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Product C\", \"price\":149.99, \"category\":\"Electronics\", \"description\":\"Top-rated gadget\", \"rating\":3.8, \"tags\":[\"electronics\", \"laptop\"], \"specifications\":{\"ram\":\"16GB\", \"storage\":\"512GB\"}}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Product D\", \"price\":29.99, \"category\":\"Clothing\", \"description\":\"Stylish t-shirt\", \"rating\":4.1, \"tags\":[\"clothing\", \"tshirt\"]}\n\nDefine the search template\n\nPUT _scripts/products_search_template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"query_string\": {\n                \"default_field\": \"{{default_field}}\",\n                \"query\": \"{{query}}\"\n              }\n            }\n          ],\n          \"filter\": [\n            {\n              \"term\": {\n                \"category\": \"{{category_filter}}\"\n              }\n            },\n            {\n              \"term\": {\n                \"tags\": \"{{tags_filter}}\"\n              }\n            }\n          ]\n        }\n      },\n      \"sort\": [\n        {\n          \"{{sort_field}}\": {\n            \"order\": \"{{sort_order}}\"\n          }\n        }\n      ],\n      \"from\": \"{{from}}\",\n      \"size\": \"{{page_size}}\",\n      \"aggs\": {\n        \"category_agg\": {\n          \"terms\": {\n            \"field\": \"category\"\n          }\n        },\n        \"tags_agg\": {\n          \"terms\": {\n            \"field\": \"tags\"\n          }\n        },\n        \"price_range\": {\n          \"range\": {\n            \"field\": \"price\",\n            \"ranges\": [\n              {\n                \"from\": 50,\n                \"to\": 200\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nUse the search template with sorting, pagination, and aggregations:\n\nGET /products/_search/template\n{\n  \"id\": \"products_search_template\",\n  \"params\": {\n    \"default_field\": \"description\",\n    \"query\": \"*\",\n    \"category_filter\": \"Electronics\",\n    \"tags_filter\": \"electronics\",\n    \"sort_field\": \"price\",\n    \"sort_order\": \"desc\",\n    \"from\": 0,\n    \"page_size\": 2\n  }\n}\n\n\n\n\nVerify the index is created\n\nGET products\n\nVerify the documents are indexed\n\nGET products/_search\n\nVerify the template is created\n\nGET _scripts/product_search_template\n\nExecute a search using the search template query, and it should return the first 2 documents matching the provided query string (“*“), category filter (”Electronics”), tag filter (“electronics”), sorted by price in descending order.\n\nThe response should also include aggregations for category, tags, and price.\n\nTo retrieve the next page of results, update the from parameter in the params object and execute the query again.\n\n\n\n\n\nThe search template includes nested queries, filters, sorting, pagination, and aggregations.\nThe tags filter uses a terms query to match documents with any of the specified tags.\nThe specifications aggregation is a nested aggregation that aggregates the nested specifications object.\nThe aggs section in the template defines the aggregations to be included in the search results.\nThe params object in the search template request provides the values for all placeholders in the template.\n\n\n\n\n\nDelete the search template\n\nDELETE _scripts/product_search_template\n\nDelete the index\n\nDELETE products\n\n\n\n\nAggregations\nMustache Language\nNested Aggregations\nPaginate Search Results\nScripting in Elasticsearch\nSearch Template\nSort Search Results"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever."
  },
  {
    "objectID": "4-data-processing.html",
    "href": "4-data-processing.html",
    "title": "Data Processing",
    "section": "",
    "text": "Define fields for product ID, name, description, price, and availability status.\nEnsure the price field is a numeric type.\nUse a text type for description with a keyword sub-field for exact matches.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index with mappings:\n\nPUT /product_catalog\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nCreate sample documents using the _bulk endpoint:\n\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 19.99, \"availability_status\": true }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 29.99, \"availability_status\": false }\n\n\n\n\nRetrieve the mappings to verify:\n\nGET /product_catalog/_mapping\n\nSearch for documents to confirm they are indexed correctly:\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nThe price field is set to double to handle decimal values.\nThe description field includes a keyword sub-field for exact match searches.\n\n\n\n\n\nDelete the index (which will also delete the mapping)\n\nDELETE product_catalog\n\n\n\n\nBulk API\nElasticsearch Index Mappings\n\n\n\n\n\n\n\n\nThe mapping should have a field called username of type keyword\nThe mapping should have a field called email of type keyword\nThe mapping should have a field called posts of type array containing object values\nThe posts array should have a property called content of type text\nThe posts array should have a property called likes of type integer\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate an index with the desired mapping:\n\nPUT /users\n{\n  \"mappings\": {\n    \"properties\": {\n      \"username\": {\n        \"type\": \"keyword\"\n      },\n      \"email\": {\n        \"type\": \"keyword\"\n      },\n      \"posts\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"content\": {\n            \"type\": \"text\"\n          },\n          \"likes\": {\n            \"type\": \"integer\"\n          }\n        }\n      }\n    }\n  }\n}\n\nIndex a document:\n\nPOST /users/_doc\n{\n  \"username\": \"john_doe\",\n  \"email\": \"john.doe@example.com\",\n  \"posts\": [\n    {\n      \"content\": \"Hello World!\",\n      \"likes\": 10\n    },\n    {\n      \"content\": \"This is my second post\",\n      \"likes\": 5\n    }\n  ]\n}\n\n\n\n\nUse the _search API to verify that the mapping is correct and the data is indexed:\n\nGET /users/_search\n{\n  \"query\": {\n    \"match\": {\n      \"username\": \"john_doe\"\n    }\n  }\n}\n\n\n\n\nThe username and email fields are of type keyword to enable exact matching.\nThe posts field is of type array with object values to enable storing multiple posts per user.\nThe content field is of type text to enable full-text search.\nThe likes field is of type integer to enable aggregations and sorting.\n\n\n\n\n\nDelete the index (which will also delete the mapping)\n\nDELETE users\n\n\n\n\nElasticsearch Index Mappings\n\n\n\n\n\n\n\n\nDefine a mapping for an index named restaurants.\nThe mapping should include fields for:\n\nname (text field for restaurant name)\ndescription (text field for restaurant description)\nlocation (geolocation field for restaurant location)\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the mapping using a REST API call:\n\nPUT /restaurants\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"location\": {\n        \"type\": \"geo_point\"\n      }\n    }\n  }\n}\n\n\n\n\nVerify that the mapping is created successfully by using the following API call:\n\nGET /restaurants/_mapping\n\nTry indexing a sample document with the defined fields:\n\nPUT /restaurants/_doc/1\n{\n  \"name\": \"Pizza Palace\",\n  \"description\": \"Delicious pizzas and Italian cuisine\",\n  \"location\": {\n    \"lat\": 40.7128,\n    \"lon\": -74.0059\n  }\n}\n\nUse search queries to test text search on name and description fields, and utilize geoqueries to search based on the location field.\n\nGET /restaurants/_search\n{\n  \"query\": {\n    \"match\": {\n      \"name\": \"Pizza Palace\"\n    }\n  }\n}\nGET /restaurants/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"Italian cuisine\"\n    }\n  }\n}\nGET /restaurants/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": {\n        \"geo_distance\": {\n          \"distance\": \"5km\",\n          \"location\": {\n            \"lat\": 40.7128,\n            \"lon\": -74.0059\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\ntext is a generic field type suitable for textual data like names and descriptions.\ngeo_point is a specialized field type for storing and searching geospatial data like latitude and longitude coordinates.\n\n\n\n\n\nDelete the index (which will also delete the mapping)\n\nDELETE restaurants\n\n\n\n\nData Types\nGeolocation in Elasticsearch\n\n\n\n\n\n\n\n\n\n\n\nCreate a custom analyzer named custom_review_analyzer.\nThe analyzer should:\n\nUse the standard tokenizer.\nInclude a lowercase filter.\nInclude a stop filter to remove common English stop words.\nInclude a synonym filter to handle common synonyms.\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate the index with a custom analyzer defined in the index settings.\n\nPUT /restaurant_reviews\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_review_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"stop\",\n            \"synonym\"\n          ]\n        }\n      },\n      \"filter\": {\n        \"synonym\": {\n          \"type\": \"synonym\",\n          \"synonyms\": [\n            \"delicious, tasty\",\n            \"restaurant, eatery\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"review_id\": {\n        \"type\": \"keyword\"\n      },\n      \"restaurant_name\": {\n        \"type\": \"text\"\n      },\n      \"review_text\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_review_analyzer\"\n      },\n      \"rating\": {\n        \"type\": \"integer\"\n      },\n      \"review_date\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n\nAdd some sample documents to the index to test the custom analyzer\n\nPOST /restaurant_reviews/_bulk\n{ \"index\": {} }\n{ \"review_id\": \"1\", \"restaurant_name\": \"Pizza Palace\", \"review_text\": \"The pizza was delicious and the service was excellent.\", \"rating\": 5, \"review_date\": \"2024-07-01\" }\n{ \"index\": {} }\n{ \"review_id\": \"2\", \"restaurant_name\": \"Burger Haven\", \"review_text\": \"Tasty burgers and friendly staff.\", \"rating\": 4, \"review_date\": \"2024-07-02\" }\n\nPerform a search query to verify the custom analyzer is working as expected.\n\nGET /restaurant_reviews/_search\n{\n  \"query\": {\n    \"match\": {\n      \"review_text\": \"tasty\"\n    }\n  }\n}\n\n\n\n\nStandard Tokenizer: Chosen for its ability to handle most text inputs effectively.\nLowercase Filter: Ensures case-insensitive search.\nStop Filter: Removes common stop words to improve search relevance.\nSynonym Filter: Handles common synonyms to enhance search matching.\n\n\n\n\n\nVerify the analyzer was created\n\nGET /restaurant_reviews/_settings\n\nVerify the custom analyzer configuration using the _analyze API to test the custom analyzer directly.\n\nGET /restaurant_reviews/_analyze\n{\n  \"analyzer\": \"custom_review_analyzer\",\n  \"text\": \"The pizza was delicious and the service was excellent.\"\n}\n\nPerform a search queries to ensure the custom analyzer processes the text as expected.\n\nGET /restaurant_reviews/_search\n{\n  \"query\": {\n    \"match\": {\n      \"review_text\": \"tasty\"\n    }\n  }\n}\n\n\n\n\nDelete the Index\n\nDELETE /restaurant_reviews\n\n\n\n\nAnalyzers\nCustom Analyzers\nIndex Settings\n\n\n\n\n\n\n\n\nAn index named products with a description field containing product descriptions\nThe custom analyzer should:\n\nLowercase all text\nRemove stop words (common words like the, and, a, etc.)\nSplit text into individual words (tokenize)\nStem words (reduce words to their root form, e.g., running - run)\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate the products index with a custom analyzer for the description field:\n\nPUT /products\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"product_description_analyzer\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"stop\",\n            \"stemmer\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"description\": {\n        \"type\": \"text\",\n        \"analyzer\": \"product_description_analyzer\"\n      }\n    }\n  }\n}\n\nIndex some sample documents using the _bulk endpoint:\n\nPOST /products/_bulk\n{ \"index\": { \"_id\": 1 } }\n{ \"description\": \"The quick brown fox jumps over the lazy dog.\" }\n{ \"index\": { \"_id\": 2 } }\n{ \"description\": \"A high-quality product for running enthusiasts.\" }\n\n\n\n\nSearch for documents containing the term “run”:\n\nGET /products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"run\"\n    }\n  }\n}\nThis should return the document with _id 2, as the custom analyzer has stemmed “running” to “run”.\n\nSearch for documents containing the term “the”:\n\nGET /products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"the\"\n    }\n  }\n}\nThis should not return any documents, as the custom analyzer has removed stop words like “the”.\n\n\n\n\nThe custom analyzer is defined in the index settings using the analysis section.\nThe tokenizer parameter specifies how the text should be split into tokens (individual words).\nThe filter parameter specifies the filters to be applied to the tokens, such as lowercasing, stop word removal, and stemming.\nThe custom analyzer is applied to the description field by specifying it in the field mapping.\n\n\n\n\n\nDelete the Index\n\nDELETE /products\n\n\n\n\nElasticsearch Analyzers\nElasticsearch Custom Analyzers\nElasticsearch Tokenizers\nElasticsearch Token Filters\n\n\n\n\n\n\n\n\nUse a custom tokenizer that splits text on non-letter characters.\nInclude a lowercase filter to normalize text.\nAdd a stopword filter to remove common English stopwords.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the custom analyzer in the index settings:\n\nPUT /product_catalog\n{\n  \"settings\": {\n    \"analysis\": {\n      \"tokenizer\": {\n        \"custom_tokenizer\": {\n          \"type\": \"pattern\",\n          \"pattern\": \"\\\\W+\"\n        }\n      },\n      \"filter\": {\n        \"custom_stop\": {\n          \"type\": \"stop\",\n          \"stopwords\": \"_english_\"\n        }\n      },\n      \"analyzer\": {\n        \"custom_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"custom_tokenizer\",\n          \"filter\": [\n            \"lowercase\",\n            \"custom_stop\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"description\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      }\n    }\n  }\n}\n\nCreate sample documents using the _bulk endpoint:\n\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"description\": \"This is a great product! It works perfectly.\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"description\": \"An amazing gadget, with excellent features.\" }\n\n\n\n\nAnalyze a sample text to verify the custom analyzer:\n\nGET /product_catalog/_analyze\n{\n  \"analyzer\": \"custom_analyzer\",\n  \"text\": \"This is a great product! It works perfectly.\"\n}\n\nSearch for documents to confirm they are indexed correctly:\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"great product\"\n    }\n  }\n}\n\n\n\n\nThe custom tokenizer splits text on non-letter characters, ensuring that punctuation does not affect tokenization.\nThe lowercase filter normalizes text to lower case, providing case-insensitive searches.\nThe custom_stop stopword filter removes common English stopwords, improving search relevance by ignoring less important words.\n\n\n\n\n\nDelete the Index\n\nDELETE /product_catalog\n\n\n\n\nBulk API\nCustom Analyzers\n\n\n\n\n\n\n\n\n\n\n\nDefine a field with a text type for full-text search.\nInclude a keyword sub-field for exact matches.\nAdd a custom analyzer to the text field to normalize the text.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the multi-fields in the index mappings\n\nPUT /product_catalog\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_name\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      }\n    }\n  }\n}\n\nCreate sample documents using the _bulk endpoint:\n\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_name\": \"Deluxe Toaster\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_name\": \"Premium Coffee Maker\" }\n\n\n\n\nRetrieve the index configuration to verify the custom analyzer and the sub-field:\n\nGET product_catalog\n\nSearch for documents using the text field:\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match\": {\n      \"product_name\": \"deluxe\"\n    }\n  }\n}\n\nSearch for documents using the keyword sub-field:\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"term\": {\n      \"product_name.keyword\": \"Deluxe Toaster\"\n    }\n  }\n}\n\n\n\n\nThe custom analyzer includes the lowercase filter for case-insensitive searches and the asciifolding filter to normalize text by removing accents and other diacritics.\nThe keyword sub-field allows for exact matches, which is useful for aggregations and sorting.\n\n\n\n\n\nDelete the Index\n\nDELETE /product_catalog\n\n\n\n\nBulk API\nCustom Analyzers in Elasticsearch\nMulti-fields in Elasticsearch\n\n\n\n\n\n\n\n\nThe title field should have a sub-field for exact matching (keyword)\nThe title field should have a sub-field for full-text search (text) with standard analyzer\nThe title field should have a sub-field for full-text search (text) with english analyzer\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate an index with the desired mapping:\n\nPUT /myindex\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"exact\": {\n            \"type\": \"keyword\"\n          },\n          \"std\": {\n            \"type\": \"text\",\n            \"analyzer\": \"standard\"\n          },\n          \"english\": {\n            \"type\": \"text\",\n            \"analyzer\": \"english\"\n          }\n        }\n      }\n    }\n  }\n}\n\nAdd documents using the appropriate endpoint:\n\nPOST /myindex/_bulk\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"title\": \"The Quick Brown Fox\" }\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"title\": \"The Quick Brown Fox Jumps\" }\n\n\n\n\nVerify the index was created with its associated multi-fields\n\nGET myindex\n\nUse the _search API to verify that the multi-field is working correctly:\n\nGET /myindex/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title.exact\": \"The Quick Brown Fox\"\n    }\n  }\n}\n\nGET /myindex/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title.std\": \"Quick Brown\"\n    }\n  }\n}\n\nGET /myindex/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title.english\": \"Quick Brown\"\n    }\n  }\n}\n\n\n\n\nThe title.exact sub-field is used for exact matching.\nThe title.std sub-field is used for full-text search with the standard analyzer.\nThe title.english sub-field is used for full-text search with the English analyzer.\n\n\n\n\n\nDelete the Index\n\nDELETE /myindex\n\n\n\n\nBulk API\nMulti-Field\n\n\n\n\n\n\n\n\nStore the original text data for display purposes\nAnalyze the text data for full-text search\nAnalyze the text data for filtering and aggregations\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the multi-fields in the index mapping:\n\nPUT /text_data\n{\n  \"mappings\": {\n    \"properties\": {\n      \"content\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"raw\": {\n            \"type\": \"keyword\"\n          },\n          \"analyzed\": {\n            \"type\": \"text\",\n            \"analyzer\": \"english\"\n          },\n          \"ngram\": {\n            \"type\": \"text\",\n            \"analyzer\": \"ngram_analyzer\"\n          }\n        }\n      }\n    }\n  },\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"ngram_analyzer\": {\n          \"tokenizer\": \"ngram_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"ngram_tokenizer\": {\n          \"type\": \"ngram\",\n          \"min_gram\": 2,\n          \"max_gram\": 3\n        }\n      }\n    }\n  }\n}\n\nIndex some documents using the text_data index:\n\nPOST /text_data/_bulk\n{ \"index\": {} }\n{ \"content\": \"This is a sample text for analyzing.\" }\n{ \"index\": {} }\n{ \"content\": \"Another example of text data.\" }\n\n\n\n\nVerify the index was created with its associated multi-fields\n\nGET text_data\n\nTest the multi-fields by querying and aggregating the data:\n\nGET /text_data/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content.analyzed\": \"sample\"\n    }\n  },\n  \"aggs\": {\n    \"filter_agg\": {\n      \"filter\": {\n        \"term\": {\n          \"content.ngram\": \"ex\"\n        }\n      }\n    }\n  }\n}\nThe output should show a sinlge document in the search results matching the analyzed text and the aggregation results based on the ngram analysis.\nThe following:\nGET /text_data/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content.ngram\": \"ex\"\n    }\n  },\n  \"aggs\": {\n    \"filter_agg\": {\n      \"filter\": {\n        \"term\": {\n          \"content.ngram\": \"ex\"\n        }\n      }\n    }\n  }\n}\nwill show 2 documents as the search is looking for the substring “ex” which can be found in both documents, but only if you search against content.ngram.\n\n\n\n\nThe content field has multiple sub-fields: raw (keyword), analyzed (text with English analyzer), and ngram (text with ngram analyzer).\nThe raw sub-field is used for storing the original text data without analysis.\nThe analyzed sub-field is used for full-text search using the English analyzer.\nThe ngram sub-field is used for filtering and aggregations based on ngram analysis.\n\n\n\n\n\nDelete the Index\n\nDELETE text_data\n\n\n\n\nAnalyzers\nMulti-fields\nNgram Tokenizer\n\n\n\n\n\n\n\n\n\n\n\nReindex data from an existing index named products_old to a new index named products_new.\nDuring the reindexing process, add a new field named stock_level with a default value of 10 for each product.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate the indices (notice that they both look identical)\n\nPUT /products_old\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\nPUT /products_new\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nAdd products to products_old\n\nPOST /products_old/_bulk\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"1\" } }\n{ \"product_id\": \"1\", \"name\": \"Wireless Mouse\", \"description\": \"A high-quality wireless mouse with ergonomic design.\", \"price\": 29.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"2\" } }\n{ \"product_id\": \"2\", \"name\": \"Gaming Keyboard\", \"description\": \"Mechanical gaming keyboard with customizable RGB lighting.\", \"price\": 79.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"3\" } }\n{ \"product_id\": \"3\", \"name\": \"USB-C Hub\", \"description\": \"A versatile USB-C hub with multiple ports.\", \"price\": 49.99, \"availability_status\": true }\n\nUse the Reindex API with a script to update documents during the copy process:\n\nPOST /_reindex\n{\n  \"source\": {\n    \"index\": \"products_old\"\n  },\n  \"dest\": {\n    \"index\": \"products_new\"\n  },\n  \"script\": {\n    \"source\": \"ctx._source.stock_level = 10\"\n  }\n}\n\nWait for the reindexing or update operation to complete by monitoring the task status through the API or Kibana.\n\n\n\n\n\nVerify that the data is successfully migrated to the products_new index with the addition of stock_level\nGET /products_new/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\nVerify that the documents from products_old do not contain stock_level\nGET /products_old/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\n\nThe Reindex API with a script allows copying data and applying transformations during the process.\n\n\n\n\n\nDelete the two indices\n\nDELETE products_old\nDELETE products_new\n\n\n\n\nReindex API\nUpdate By Query API\n\n\n\n\n\n\n\n\nReindex product data from an old index to a new index with an updated mapping\nUpdate the in_stock field for products with a low inventory count\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate the old index with some sample data:\n\nPUT /products_old\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      },\n      \"inventory_count\": {\n        \"type\": \"integer\"\n      }\n    }\n  }\n}\nPOST /products_old/_bulk\n{ \"index\": {} }\n{ \"name\": \"Product A\", \"price\": 19.99, \"inventory_count\": 10 }\n{ \"index\": {} }\n{ \"name\": \"Product B\", \"price\": 29.99, \"inventory_count\": 5 }\n{ \"index\": {} }\n{ \"name\": \"Product C\", \"price\": 39.99, \"inventory_count\": 20 }\n\nCreate the new index with an updated mapping:\n\nPUT /products_new\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      },\n      \"inventory_count\": {\n        \"type\": \"integer\"\n      },\n      \"in_stock\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nReindex the data from the old index to the new index:\n\nPOST /_reindex\n{\n  \"source\": {\n    \"index\": \"products_old\"\n  },\n  \"dest\": {\n    \"index\": \"products_new\"\n  },\n  \"script\": {\n    \"source\": \"\"\"\n      if (ctx._source.inventory_count &lt; 10) {\n        ctx._source.in_stock = false;\n      } else {\n        ctx._source.in_stock = true;\n      }\n    \"\"\"\n  }\n}\n\nUpdate the “in_stock” field for products with low inventory:\n\nPOST /products_new/_update_by_query\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock = false\"\n  },\n  \"query\": {\n    \"range\": {\n      \"inventory_count\": {\n        \"lt\": 10\n      }\n    }\n  }\n}\n\n\n\n\nSearch the new index to verify the reindexed data and updated in_stock field\n\nGET /products_new/_search\nThe response should show the reindexed products with the “in_stock” field set correctly based on the inventory count.\n\nSearch the old index to verify the original data and the absence of in_stock\n\nGET /products_old/_search\n\n\n\n\nThe Reindex API is used to copy data from the old index to the new index while applying a script to set the “in_stock” field based on the inventory count.\nThe Update By Query API is used to update the in_stock field for products with an inventory count lower than 10.\n\n\n\n\n\nDelete the two indices\n\nDELETE products_old\nDELETE products_new\n\n\n\n\nReindex API\nUpdate By Query API\nScripting\n\n\n\n\n\n\n\n\nCreate the products_old index and add sample products.\nReindex documents from products_old to products_new.\nUpdate the mappings in products_new.\nIncrease the price of all products in products_new by 10%.\n\n\n\n\n\nCreate the products_old index and add sample products\nPUT /products_old\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nPOST /products_old/_bulk\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"1\" } }\n{ \"product_id\": \"1\", \"name\": \"Wireless Mouse\", \"description\": \"A high-quality wireless mouse with ergonomic design.\", \"price\": 29.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"2\" } }\n{ \"product_id\": \"2\", \"name\": \"Gaming Keyboard\", \"description\": \"Mechanical gaming keyboard with customizable RGB lighting.\", \"price\": 79.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"3\" } }\n{ \"product_id\": \"3\", \"name\": \"USB-C Hub\", \"description\": \"A versatile USB-C hub with multiple ports.\", \"price\": 49.99, \"availability_status\": true }\nCreate the new index with updated mappings\n\nDefine the new index products_new with the desired mappings.\n\nPUT /products_new\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\nReindex Documents from products_old to products_new\n\nUse the Reindex API to copy documents from the old index to the new index.\nPOST /_reindex\n{\n \"source\": {\n \"index\": \"products_old\"\n },\n \"dest\": {\n \"index\": \"products_new\"\n }\n}\n\nUpdate prices in the new index using the Update By Query API to increase the price of all products in products_new by 10%.\n\nPOST /products_new/_update_by_query\n{\n  \"script\": {\n    \"source\": \"ctx._source.price *= 1.10\",\n    \"lang\": \"painless\"\n  },\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nVerify the reindexing\n\nGET /products_old/_count\nGET /products_new/_count\n\nVerify the price update\n\nGET /products_new/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nMappings Update: Ensure the new index products_new has the updated mappings to accommodate any changes in the document structure.\nPrice Update Script: The script in the Update By Query API uses the painless language to increase the price by 10%. This is a simple and efficient way to update document fields.\n\n\n\n\n\nDelete the indices\n\nDELETE /products_old\nDELETE /products_new\n\n\n\n\nIndex Settings\nPainless Scripting Language\nReindex API\nUpdate By Query API\n\n\n\n\n\n\n\n\n\n\n\nUse an ingest pipeline to process incoming documents.\nApply a Painless script to modify specific fields.\nEnrich the data by adding a timestamp and converting the price to a different currency.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the ingest pipeline with a Painless script and additional processors:\n\nPUT /_ingest/pipeline/product_pipeline\n{\n  \"processors\": [\n    {\n      \"script\": {\n        \"lang\": \"painless\",\n        \"source\": \"\"\"\n          if (ctx.price != null) {\n            ctx.price_usd = ctx.price * 1.1; // Convert to USD assuming 1.1 is the conversion rate\n          }\n        \"\"\"\n      }\n    },\n    {\n      \"set\": {\n        \"field\": \"timestamp\",\n        \"value\": \"{{_ingest.timestamp}}\"\n      }\n    }\n  ]\n}\n\nCreate the index\n\nPUT /product_catalog\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"price_usd\": {\n        \"type\": \"double\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n\nIndex documents using the ingest pipeline\n\nPOST /product_catalog/_bulk?pipeline=product_pipeline\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 20.0 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 30.0 }\n\n\n\n\nVerify the ingest pipeline configuration:\n\nGET /_ingest/pipeline/product_pipeline\n\nSearch the indexed documents to ensure the modifications have been applied:\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nThe Painless script modifies the price field to convert it to USD and stores it in a new field price_usd.\nThe set processor adds a timestamp to each document to track when it was ingested.\nEnsure the pipeline processes all incoming documents to maintain data consistency.\n\n\n\n\n\nDelete the index\n\nDELETE product_catalog\n\nDelete the pipeline\n\nDELETE _ingest/pipeline/product_pipeline\n\n\n\n\nBulk API\nIngest Node Pipelines\nPainless Scripting Language\n\n\n\n\n\nThis example creates another ingest pipeline, but this time adds it directly into the index definition.\n\n\n\nExtract the log level (DEBUG, INFO, WARNING, ERROR) from the log message.\nExtract the log timestamp in ISO format.\nAdd a new field log_level_tag with a value based on the log level (e.g. DEBUG -&gt; DEBUG_LOG).\nAdd a new field log_timestamp_in_seconds with the timestamp in seconds.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate an ingest pipeline:\n\nPUT /_ingest/pipeline/logging-pipeline\n{\n  \"description\": \"Extract and transform log data\",\n  \"processors\": [\n    {\n      \"grok\": {\n        \"field\": \"message\",\n        \"patterns\": [\"%{LOGLEVEL:log_level} %{TIMESTAMP_ISO8601:log_timestamp} %{GREEDYDATA:message}\"]\n      }\n    },\n    {\n      \"script\": {\n        \"source\": \"\"\"\n          ctx.log_level_tag = ctx.log_level.toUpperCase() + '_LOG';\n          ctx.log_timestamp_in_seconds = ZonedDateTime.parse(ctx.log_timestamp).toEpochSecond();\n        \"\"\",\n        \"lang\": \"painless\"\n      }\n    }\n  ]\n}\n\nCreate an index with the ingest pipeline:\n\nPUT /logging-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"log_level\": {\n        \"type\": \"keyword\"\n      },\n      \"log_timestamp\": {\n        \"type\": \"date\"\n      },\n      \"log_level_tag\": {\n        \"type\": \"keyword\"\n      },\n      \"log_timestamp_in_seconds\": {\n        \"type\": \"long\"\n      }\n    }\n  },\n  \"settings\": {\n    \"index\": {\n      \"default_pipeline\": \"logging-pipeline\"\n    }\n  }\n}\n\nAdd documents to the index:\n\nPOST /logging-index/_bulk\n{ \"index\": { \"_index\": \"logging-index\" } }\n{ \"message\": \"DEBUG 2022-05-25T14:30:00.000Z This is a debug message\" }\n{ \"index\": { \"_index\": \"logging-index\" } }\n{ \"message\": \"INFO 2022-05-25T14:30:00.000Z This is an info message\" }\n\n\n\n\nVerify that the documents have been processed correctly:\n\nGET /logging-index/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nThe ingest pipeline uses the Grok processor to extract the log level and timestamp from the log message.\nThe Painless script processor is used to transform the log level and timestamp into new fields.\n\n\n\n\n\nDelete the index\n\nDELETE logging-index\n\nDelete the pipeline\n\nDELETE _ingest/pipeline/logging-pipeline\n\n\n\n\nIngest Node Pipelines\nPainless Scripting Language\n\n\n\n\n\n\n\n\nAn index named products with fields like name, price, category, description, etc.\nPreprocess incoming product data using an ingest pipeline:\n\nLowercase the name and category fields\nRemove HTML tags from the description field\nCalculate a discounted_price field based on the price field and a discount percentage stored in a pipeline variable\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the ingest pipeline:\n\nPUT /_ingest/pipeline/product_pipeline\n{\n  \"description\": \"Ingest pipeline for product data\",\n  \"processors\": [\n    {\n      \"lowercase\": {\n        \"field\": \"name\"\n      }\n    },\n    {\n      \"lowercase\": {\n        \"field\": \"category\"\n      }\n    },\n    {\n      \"script\": {\n        \"lang\": \"painless\",\n        \"source\": \"\"\"\n          if (ctx.description != null) {\n            Pattern pattern = /&lt;[^&gt;]+&gt;/;\n            Matcher matcher = pattern.matcher(ctx.description);\n            ctx.description = matcher.replaceAll('');\n          }\n        \"\"\"\n      }\n    },\n    {\n      \"script\": {\n        \"lang\": \"painless\",\n        \"source\": \"\"\"\n          double discount_percentage = 0.2;\n          double discounted_price = ctx.price * (1 - discount_percentage);\n          ctx.discounted_price = discounted_price;\n        \"\"\"\n      }\n    }\n  ]\n}\n\nIndex a sample document using the ingest pipeline:\n\nPUT /products/_doc/1?pipeline=product_pipeline\n{\n  \"name\": \"Product A\",\n  \"price\": 99.99,\n  \"category\": \"Electronics\",\n  \"description\": \"A &lt;b&gt;high-quality&lt;/b&gt; product for running enthusiasts.\"\n}\n\n\n\n\nSearch the products index and verify that the document has been processed by the ingest pipeline:\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\nThe response should show:\n\nname and category fields in lowercase\ndescription field without HTML tags\ndiscounted_price field calculated based on the price field and the discount percentage\n\n\n\n\n\nThe ingest pipeline is defined with a list of processors that perform specific operations on incoming documents.\nThe lowercase processor lowercases the name and category fields.\nThe remove processor removes HTML tags from the description field using a regular expression.\nThe script processor uses the Painless scripting language to calculate the discounted_price field based on the price field and a discount percentage variable.\n\n\n\n\n\nDelete the index\n\nDELETE products\n\nDelete the pipeline\n\nDELETE _ingest/pipeline/product_pipeline\n\n\n\n\nIngest Node\nIngest Pipelines\nIngest Processors\nPainless Scripting Language\n\n\n\n\n\n\n\n\n\n\n\nUse a runtime field to calculate a discount on product prices.\nApply a Painless script to dynamically compute the discounted price.\nEnsure the runtime field is available for queries and aggregations.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the index with appropriate mappings:\n\nPUT /product_catalog\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    },\n    \"runtime\": {\n      \"discounted_price\": {\n        \"type\": \"double\",\n        \"script\": {\n          \"source\": \"\"\"\n            if (doc['price'].size() != 0) {\n              emit(doc['price'].value * 0.9);\n            } else {\n              emit(Double.NaN);\n            }\n          \"\"\"\n        }\n      }\n    }\n  }\n}\n\nIndex sample documents using the _bulk endpoint:\n\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 20.0 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 30.0 }\n\n\n\n\nSearch the indexed documents and retrieve the runtime field\n\nGET /product_catalog/_search\n{\n  \"_source\": [\"name\", \"price\"],\n  \"fields\": [\"discounted_price\"],\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\nVerify the discounted price in the search results\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"script_fields\": {\n    \"discounted_price\": {\n      \"script\": {\n        \"source\": \"doc['price'].value * 0.9\"\n      }\n    }\n  }\n}\n\n\n\n\nThe Painless script calculates a 10% discount on the price.\nRuntime fields are defined in the index mappings and can be used for querying and aggregations without being stored in the index.\n\n\n\n\n\nDelete the index\n\nDELETE product_catalog\n\n\n\n\nBulk API\nPainless Scripting Language\nRuntime Fields\n\n\n\n\n\n\n\n\nExtract the domain from a URL field.\nUse Painless scripting to define the runtime field.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate an index with a URL field:\n\nPUT /myindex\n{\n  \"mappings\": {\n    \"properties\": {\n      \"url\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\nDefine a runtime field to extract the domain:\n\nPUT /myindex/_mapping\n{\n  \"runtime\": {\n    \"domain\": {\n      \"type\": \"keyword\",\n      \"script\": {\n        \"source\": \"\"\"\n          if (doc['url'].size() != 0) {\n            String url = doc['url'].value;\n            int startIndex = url.indexOf(\"://\") + 3;\n            int endIndex = url.indexOf(\"/\", startIndex);\n            if (endIndex == -1) {\n              endIndex = url.length();\n            }\n            emit(url.substring(startIndex, endIndex));\n          }\n        \"\"\"\n      }\n    }\n  }\n}\n\nAdd documents to the index:\n\nPOST /myindex/_bulk\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"url\": \"https://www.example.com/path/to/page\" }\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"url\": \"http://sub.example.com/other/page\" }\n\n\n\n\nVerify that the runtime field is working correctly:\n\nGET /myindex/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\"domain\"]\n}\n\n\n\n\nThe runtime field uses Painless scripting to extract the domain from the URL field.\nThe script splits the URL into components and returns the domain.\n\n\n\n\n\nDelete the index\n\nDELETE myindex\n\n\n\n\nPainless Scripting\nRuntime Fields\n\n\n\n\n\n\n\n\nDefine a search query that utilizes a runtime field to calculate the age difference in years between two date fields (date_of_birth and current_date) within the search results.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate the Index\n\nPUT /people\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"date_of_birth\": {\n        \"type\": \"date\"\n      },\n      \"current_date\": {\n        \"type\": \"date\"\n      }\n    },\n    \"runtime\": {\n      \"age_difference\": {\n        \"type\": \"long\",\n        \"script\": {\n          \"source\": \"\"\"\n            if (doc['date_of_birth'].size() != 0 && doc['current_date'].size() != 0) {\n              ZonedDateTime dob = ZonedDateTime.parse(doc['date_of_birth'].value.toString());\n              ZonedDateTime current = ZonedDateTime.parse(doc['current_date'].value.toString());\n              long yearsBetween = ChronoUnit.YEARS.between(dob, current);\n              emit(yearsBetween);\n            }\n          \"\"\"\n        }\n      }\n    }\n  }\n}\n\nIndex sample documents\n\nPOST /people/_bulk\n{ \"index\": { \"_index\": \"people\", \"_id\": \"1\" } }\n{ \"name\": \"Alice\", \"date_of_birth\": \"1990-01-01\", \"current_date\": \"2024-07-08\" }\n{ \"index\": { \"_index\": \"people\", \"_id\": \"2\" } }\n{ \"name\": \"Bob\", \"date_of_birth\": \"1985-05-15\", \"current_date\": \"2024-07-08\" }\n{ \"index\": { \"_index\": \"people\", \"_id\": \"3\" } }\n{ \"name\": \"Charlie\", \"date_of_birth\": \"2000-12-25\", \"current_date\": \"2024-07-08\" }\n\nConstruct a search query with a runtime field:\n\nGET /people/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"script_fields\": {\n    \"age_difference\": {\n      \"script\": {\n        \"source\": \"\"\"\n          if (doc['date_of_birth'].size() != 0 && doc['current_date'].size() != 0) {\n            ZonedDateTime dob = ZonedDateTime.parse(doc['date_of_birth'].value.toString());\n            ZonedDateTime current = ZonedDateTime.parse(doc['current_date'].value.toString());\n            long yearsBetween = ChronoUnit.YEARS.between(dob, current);\n            return yearsBetween;\n          } else {\n            return null;\n          }\n        \"\"\"\n      }\n    }\n  }\n}\n\n\n\n\nEnsure the documents in your index have date_of_birth and current_date fields in a compatible date format (e.g., milliseconds since epoch).\nRun the search query and examine the response. The results should include an additional field named age representing the calculated age difference in years for each document.\n\n\n\n\n\nThe runtime field definition utilizes Painless scripting to perform the age calculation.\nThe script calculates the difference in milliseconds between current_date and date_of_birth, then divides by the conversion factor for milliseconds in a year (considering leap years).\nThe Math.floor function ensures the age is a whole number of years.\n\n\n\n\n\nDelete the index\n\nDELETE people\n\n\n\n\nPainless Scripting\nRuntime Fields"
  },
  {
    "objectID": "4-data-processing.html#task-define-a-mapping-that-satisfies-a-given-set-of-requirements",
    "href": "4-data-processing.html#task-define-a-mapping-that-satisfies-a-given-set-of-requirements",
    "title": "Data Processing",
    "section": "",
    "text": "Define fields for product ID, name, description, price, and availability status.\nEnsure the price field is a numeric type.\nUse a text type for description with a keyword sub-field for exact matches.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the index with mappings:\n\nPUT /product_catalog\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nCreate sample documents using the _bulk endpoint:\n\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 19.99, \"availability_status\": true }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 29.99, \"availability_status\": false }\n\n\n\n\nRetrieve the mappings to verify:\n\nGET /product_catalog/_mapping\n\nSearch for documents to confirm they are indexed correctly:\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nThe price field is set to double to handle decimal values.\nThe description field includes a keyword sub-field for exact match searches.\n\n\n\n\n\nDelete the index (which will also delete the mapping)\n\nDELETE product_catalog\n\n\n\n\nBulk API\nElasticsearch Index Mappings\n\n\n\n\n\n\n\n\nThe mapping should have a field called username of type keyword\nThe mapping should have a field called email of type keyword\nThe mapping should have a field called posts of type array containing object values\nThe posts array should have a property called content of type text\nThe posts array should have a property called likes of type integer\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate an index with the desired mapping:\n\nPUT /users\n{\n  \"mappings\": {\n    \"properties\": {\n      \"username\": {\n        \"type\": \"keyword\"\n      },\n      \"email\": {\n        \"type\": \"keyword\"\n      },\n      \"posts\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"content\": {\n            \"type\": \"text\"\n          },\n          \"likes\": {\n            \"type\": \"integer\"\n          }\n        }\n      }\n    }\n  }\n}\n\nIndex a document:\n\nPOST /users/_doc\n{\n  \"username\": \"john_doe\",\n  \"email\": \"john.doe@example.com\",\n  \"posts\": [\n    {\n      \"content\": \"Hello World!\",\n      \"likes\": 10\n    },\n    {\n      \"content\": \"This is my second post\",\n      \"likes\": 5\n    }\n  ]\n}\n\n\n\n\nUse the _search API to verify that the mapping is correct and the data is indexed:\n\nGET /users/_search\n{\n  \"query\": {\n    \"match\": {\n      \"username\": \"john_doe\"\n    }\n  }\n}\n\n\n\n\nThe username and email fields are of type keyword to enable exact matching.\nThe posts field is of type array with object values to enable storing multiple posts per user.\nThe content field is of type text to enable full-text search.\nThe likes field is of type integer to enable aggregations and sorting.\n\n\n\n\n\nDelete the index (which will also delete the mapping)\n\nDELETE users\n\n\n\n\nElasticsearch Index Mappings\n\n\n\n\n\n\n\n\nDefine a mapping for an index named restaurants.\nThe mapping should include fields for:\n\nname (text field for restaurant name)\ndescription (text field for restaurant description)\nlocation (geolocation field for restaurant location)\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the mapping using a REST API call:\n\nPUT /restaurants\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"location\": {\n        \"type\": \"geo_point\"\n      }\n    }\n  }\n}\n\n\n\n\nVerify that the mapping is created successfully by using the following API call:\n\nGET /restaurants/_mapping\n\nTry indexing a sample document with the defined fields:\n\nPUT /restaurants/_doc/1\n{\n  \"name\": \"Pizza Palace\",\n  \"description\": \"Delicious pizzas and Italian cuisine\",\n  \"location\": {\n    \"lat\": 40.7128,\n    \"lon\": -74.0059\n  }\n}\n\nUse search queries to test text search on name and description fields, and utilize geoqueries to search based on the location field.\n\nGET /restaurants/_search\n{\n  \"query\": {\n    \"match\": {\n      \"name\": \"Pizza Palace\"\n    }\n  }\n}\nGET /restaurants/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"Italian cuisine\"\n    }\n  }\n}\nGET /restaurants/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": {\n        \"geo_distance\": {\n          \"distance\": \"5km\",\n          \"location\": {\n            \"lat\": 40.7128,\n            \"lon\": -74.0059\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\ntext is a generic field type suitable for textual data like names and descriptions.\ngeo_point is a specialized field type for storing and searching geospatial data like latitude and longitude coordinates.\n\n\n\n\n\nDelete the index (which will also delete the mapping)\n\nDELETE restaurants\n\n\n\n\nData Types\nGeolocation in Elasticsearch"
  },
  {
    "objectID": "4-data-processing.html#task-define-and-use-a-custom-analyzer-that-satisfies-a-given-set-of-requirements",
    "href": "4-data-processing.html#task-define-and-use-a-custom-analyzer-that-satisfies-a-given-set-of-requirements",
    "title": "Data Processing",
    "section": "",
    "text": "Create a custom analyzer named custom_review_analyzer.\nThe analyzer should:\n\nUse the standard tokenizer.\nInclude a lowercase filter.\nInclude a stop filter to remove common English stop words.\nInclude a synonym filter to handle common synonyms.\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate the index with a custom analyzer defined in the index settings.\n\nPUT /restaurant_reviews\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_review_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"stop\",\n            \"synonym\"\n          ]\n        }\n      },\n      \"filter\": {\n        \"synonym\": {\n          \"type\": \"synonym\",\n          \"synonyms\": [\n            \"delicious, tasty\",\n            \"restaurant, eatery\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"review_id\": {\n        \"type\": \"keyword\"\n      },\n      \"restaurant_name\": {\n        \"type\": \"text\"\n      },\n      \"review_text\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_review_analyzer\"\n      },\n      \"rating\": {\n        \"type\": \"integer\"\n      },\n      \"review_date\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n\nAdd some sample documents to the index to test the custom analyzer\n\nPOST /restaurant_reviews/_bulk\n{ \"index\": {} }\n{ \"review_id\": \"1\", \"restaurant_name\": \"Pizza Palace\", \"review_text\": \"The pizza was delicious and the service was excellent.\", \"rating\": 5, \"review_date\": \"2024-07-01\" }\n{ \"index\": {} }\n{ \"review_id\": \"2\", \"restaurant_name\": \"Burger Haven\", \"review_text\": \"Tasty burgers and friendly staff.\", \"rating\": 4, \"review_date\": \"2024-07-02\" }\n\nPerform a search query to verify the custom analyzer is working as expected.\n\nGET /restaurant_reviews/_search\n{\n  \"query\": {\n    \"match\": {\n      \"review_text\": \"tasty\"\n    }\n  }\n}\n\n\n\n\nStandard Tokenizer: Chosen for its ability to handle most text inputs effectively.\nLowercase Filter: Ensures case-insensitive search.\nStop Filter: Removes common stop words to improve search relevance.\nSynonym Filter: Handles common synonyms to enhance search matching.\n\n\n\n\n\nVerify the analyzer was created\n\nGET /restaurant_reviews/_settings\n\nVerify the custom analyzer configuration using the _analyze API to test the custom analyzer directly.\n\nGET /restaurant_reviews/_analyze\n{\n  \"analyzer\": \"custom_review_analyzer\",\n  \"text\": \"The pizza was delicious and the service was excellent.\"\n}\n\nPerform a search queries to ensure the custom analyzer processes the text as expected.\n\nGET /restaurant_reviews/_search\n{\n  \"query\": {\n    \"match\": {\n      \"review_text\": \"tasty\"\n    }\n  }\n}\n\n\n\n\nDelete the Index\n\nDELETE /restaurant_reviews\n\n\n\n\nAnalyzers\nCustom Analyzers\nIndex Settings\n\n\n\n\n\n\n\n\nAn index named products with a description field containing product descriptions\nThe custom analyzer should:\n\nLowercase all text\nRemove stop words (common words like the, and, a, etc.)\nSplit text into individual words (tokenize)\nStem words (reduce words to their root form, e.g., running - run)\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate the products index with a custom analyzer for the description field:\n\nPUT /products\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"product_description_analyzer\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"stop\",\n            \"stemmer\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"description\": {\n        \"type\": \"text\",\n        \"analyzer\": \"product_description_analyzer\"\n      }\n    }\n  }\n}\n\nIndex some sample documents using the _bulk endpoint:\n\nPOST /products/_bulk\n{ \"index\": { \"_id\": 1 } }\n{ \"description\": \"The quick brown fox jumps over the lazy dog.\" }\n{ \"index\": { \"_id\": 2 } }\n{ \"description\": \"A high-quality product for running enthusiasts.\" }\n\n\n\n\nSearch for documents containing the term “run”:\n\nGET /products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"run\"\n    }\n  }\n}\nThis should return the document with _id 2, as the custom analyzer has stemmed “running” to “run”.\n\nSearch for documents containing the term “the”:\n\nGET /products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"the\"\n    }\n  }\n}\nThis should not return any documents, as the custom analyzer has removed stop words like “the”.\n\n\n\n\nThe custom analyzer is defined in the index settings using the analysis section.\nThe tokenizer parameter specifies how the text should be split into tokens (individual words).\nThe filter parameter specifies the filters to be applied to the tokens, such as lowercasing, stop word removal, and stemming.\nThe custom analyzer is applied to the description field by specifying it in the field mapping.\n\n\n\n\n\nDelete the Index\n\nDELETE /products\n\n\n\n\nElasticsearch Analyzers\nElasticsearch Custom Analyzers\nElasticsearch Tokenizers\nElasticsearch Token Filters\n\n\n\n\n\n\n\n\nUse a custom tokenizer that splits text on non-letter characters.\nInclude a lowercase filter to normalize text.\nAdd a stopword filter to remove common English stopwords.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the custom analyzer in the index settings:\n\nPUT /product_catalog\n{\n  \"settings\": {\n    \"analysis\": {\n      \"tokenizer\": {\n        \"custom_tokenizer\": {\n          \"type\": \"pattern\",\n          \"pattern\": \"\\\\W+\"\n        }\n      },\n      \"filter\": {\n        \"custom_stop\": {\n          \"type\": \"stop\",\n          \"stopwords\": \"_english_\"\n        }\n      },\n      \"analyzer\": {\n        \"custom_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"custom_tokenizer\",\n          \"filter\": [\n            \"lowercase\",\n            \"custom_stop\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"description\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      }\n    }\n  }\n}\n\nCreate sample documents using the _bulk endpoint:\n\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"description\": \"This is a great product! It works perfectly.\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"description\": \"An amazing gadget, with excellent features.\" }\n\n\n\n\nAnalyze a sample text to verify the custom analyzer:\n\nGET /product_catalog/_analyze\n{\n  \"analyzer\": \"custom_analyzer\",\n  \"text\": \"This is a great product! It works perfectly.\"\n}\n\nSearch for documents to confirm they are indexed correctly:\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"great product\"\n    }\n  }\n}\n\n\n\n\nThe custom tokenizer splits text on non-letter characters, ensuring that punctuation does not affect tokenization.\nThe lowercase filter normalizes text to lower case, providing case-insensitive searches.\nThe custom_stop stopword filter removes common English stopwords, improving search relevance by ignoring less important words.\n\n\n\n\n\nDelete the Index\n\nDELETE /product_catalog\n\n\n\n\nBulk API\nCustom Analyzers"
  },
  {
    "objectID": "4-data-processing.html#task-define-and-use-multi-fields-with-different-data-types-andor-analyzers",
    "href": "4-data-processing.html#task-define-and-use-multi-fields-with-different-data-types-andor-analyzers",
    "title": "Data Processing",
    "section": "",
    "text": "Define a field with a text type for full-text search.\nInclude a keyword sub-field for exact matches.\nAdd a custom analyzer to the text field to normalize the text.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the multi-fields in the index mappings\n\nPUT /product_catalog\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_name\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      }\n    }\n  }\n}\n\nCreate sample documents using the _bulk endpoint:\n\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_name\": \"Deluxe Toaster\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_name\": \"Premium Coffee Maker\" }\n\n\n\n\nRetrieve the index configuration to verify the custom analyzer and the sub-field:\n\nGET product_catalog\n\nSearch for documents using the text field:\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match\": {\n      \"product_name\": \"deluxe\"\n    }\n  }\n}\n\nSearch for documents using the keyword sub-field:\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"term\": {\n      \"product_name.keyword\": \"Deluxe Toaster\"\n    }\n  }\n}\n\n\n\n\nThe custom analyzer includes the lowercase filter for case-insensitive searches and the asciifolding filter to normalize text by removing accents and other diacritics.\nThe keyword sub-field allows for exact matches, which is useful for aggregations and sorting.\n\n\n\n\n\nDelete the Index\n\nDELETE /product_catalog\n\n\n\n\nBulk API\nCustom Analyzers in Elasticsearch\nMulti-fields in Elasticsearch\n\n\n\n\n\n\n\n\nThe title field should have a sub-field for exact matching (keyword)\nThe title field should have a sub-field for full-text search (text) with standard analyzer\nThe title field should have a sub-field for full-text search (text) with english analyzer\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate an index with the desired mapping:\n\nPUT /myindex\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"exact\": {\n            \"type\": \"keyword\"\n          },\n          \"std\": {\n            \"type\": \"text\",\n            \"analyzer\": \"standard\"\n          },\n          \"english\": {\n            \"type\": \"text\",\n            \"analyzer\": \"english\"\n          }\n        }\n      }\n    }\n  }\n}\n\nAdd documents using the appropriate endpoint:\n\nPOST /myindex/_bulk\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"title\": \"The Quick Brown Fox\" }\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"title\": \"The Quick Brown Fox Jumps\" }\n\n\n\n\nVerify the index was created with its associated multi-fields\n\nGET myindex\n\nUse the _search API to verify that the multi-field is working correctly:\n\nGET /myindex/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title.exact\": \"The Quick Brown Fox\"\n    }\n  }\n}\n\nGET /myindex/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title.std\": \"Quick Brown\"\n    }\n  }\n}\n\nGET /myindex/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title.english\": \"Quick Brown\"\n    }\n  }\n}\n\n\n\n\nThe title.exact sub-field is used for exact matching.\nThe title.std sub-field is used for full-text search with the standard analyzer.\nThe title.english sub-field is used for full-text search with the English analyzer.\n\n\n\n\n\nDelete the Index\n\nDELETE /myindex\n\n\n\n\nBulk API\nMulti-Field\n\n\n\n\n\n\n\n\nStore the original text data for display purposes\nAnalyze the text data for full-text search\nAnalyze the text data for filtering and aggregations\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the multi-fields in the index mapping:\n\nPUT /text_data\n{\n  \"mappings\": {\n    \"properties\": {\n      \"content\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"raw\": {\n            \"type\": \"keyword\"\n          },\n          \"analyzed\": {\n            \"type\": \"text\",\n            \"analyzer\": \"english\"\n          },\n          \"ngram\": {\n            \"type\": \"text\",\n            \"analyzer\": \"ngram_analyzer\"\n          }\n        }\n      }\n    }\n  },\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"ngram_analyzer\": {\n          \"tokenizer\": \"ngram_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"ngram_tokenizer\": {\n          \"type\": \"ngram\",\n          \"min_gram\": 2,\n          \"max_gram\": 3\n        }\n      }\n    }\n  }\n}\n\nIndex some documents using the text_data index:\n\nPOST /text_data/_bulk\n{ \"index\": {} }\n{ \"content\": \"This is a sample text for analyzing.\" }\n{ \"index\": {} }\n{ \"content\": \"Another example of text data.\" }\n\n\n\n\nVerify the index was created with its associated multi-fields\n\nGET text_data\n\nTest the multi-fields by querying and aggregating the data:\n\nGET /text_data/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content.analyzed\": \"sample\"\n    }\n  },\n  \"aggs\": {\n    \"filter_agg\": {\n      \"filter\": {\n        \"term\": {\n          \"content.ngram\": \"ex\"\n        }\n      }\n    }\n  }\n}\nThe output should show a sinlge document in the search results matching the analyzed text and the aggregation results based on the ngram analysis.\nThe following:\nGET /text_data/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content.ngram\": \"ex\"\n    }\n  },\n  \"aggs\": {\n    \"filter_agg\": {\n      \"filter\": {\n        \"term\": {\n          \"content.ngram\": \"ex\"\n        }\n      }\n    }\n  }\n}\nwill show 2 documents as the search is looking for the substring “ex” which can be found in both documents, but only if you search against content.ngram.\n\n\n\n\nThe content field has multiple sub-fields: raw (keyword), analyzed (text with English analyzer), and ngram (text with ngram analyzer).\nThe raw sub-field is used for storing the original text data without analysis.\nThe analyzed sub-field is used for full-text search using the English analyzer.\nThe ngram sub-field is used for filtering and aggregations based on ngram analysis.\n\n\n\n\n\nDelete the Index\n\nDELETE text_data\n\n\n\n\nAnalyzers\nMulti-fields\nNgram Tokenizer"
  },
  {
    "objectID": "4-data-processing.html#task-use-the-reindex-api-and-update-by-query-api-to-reindex-andor-update-documents",
    "href": "4-data-processing.html#task-use-the-reindex-api-and-update-by-query-api-to-reindex-andor-update-documents",
    "title": "Data Processing",
    "section": "",
    "text": "Reindex data from an existing index named products_old to a new index named products_new.\nDuring the reindexing process, add a new field named stock_level with a default value of 10 for each product.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate the indices (notice that they both look identical)\n\nPUT /products_old\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\nPUT /products_new\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nAdd products to products_old\n\nPOST /products_old/_bulk\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"1\" } }\n{ \"product_id\": \"1\", \"name\": \"Wireless Mouse\", \"description\": \"A high-quality wireless mouse with ergonomic design.\", \"price\": 29.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"2\" } }\n{ \"product_id\": \"2\", \"name\": \"Gaming Keyboard\", \"description\": \"Mechanical gaming keyboard with customizable RGB lighting.\", \"price\": 79.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"3\" } }\n{ \"product_id\": \"3\", \"name\": \"USB-C Hub\", \"description\": \"A versatile USB-C hub with multiple ports.\", \"price\": 49.99, \"availability_status\": true }\n\nUse the Reindex API with a script to update documents during the copy process:\n\nPOST /_reindex\n{\n  \"source\": {\n    \"index\": \"products_old\"\n  },\n  \"dest\": {\n    \"index\": \"products_new\"\n  },\n  \"script\": {\n    \"source\": \"ctx._source.stock_level = 10\"\n  }\n}\n\nWait for the reindexing or update operation to complete by monitoring the task status through the API or Kibana.\n\n\n\n\n\nVerify that the data is successfully migrated to the products_new index with the addition of stock_level\nGET /products_new/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\nVerify that the documents from products_old do not contain stock_level\nGET /products_old/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\n\nThe Reindex API with a script allows copying data and applying transformations during the process.\n\n\n\n\n\nDelete the two indices\n\nDELETE products_old\nDELETE products_new\n\n\n\n\nReindex API\nUpdate By Query API\n\n\n\n\n\n\n\n\nReindex product data from an old index to a new index with an updated mapping\nUpdate the in_stock field for products with a low inventory count\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate the old index with some sample data:\n\nPUT /products_old\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      },\n      \"inventory_count\": {\n        \"type\": \"integer\"\n      }\n    }\n  }\n}\nPOST /products_old/_bulk\n{ \"index\": {} }\n{ \"name\": \"Product A\", \"price\": 19.99, \"inventory_count\": 10 }\n{ \"index\": {} }\n{ \"name\": \"Product B\", \"price\": 29.99, \"inventory_count\": 5 }\n{ \"index\": {} }\n{ \"name\": \"Product C\", \"price\": 39.99, \"inventory_count\": 20 }\n\nCreate the new index with an updated mapping:\n\nPUT /products_new\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      },\n      \"inventory_count\": {\n        \"type\": \"integer\"\n      },\n      \"in_stock\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nReindex the data from the old index to the new index:\n\nPOST /_reindex\n{\n  \"source\": {\n    \"index\": \"products_old\"\n  },\n  \"dest\": {\n    \"index\": \"products_new\"\n  },\n  \"script\": {\n    \"source\": \"\"\"\n      if (ctx._source.inventory_count &lt; 10) {\n        ctx._source.in_stock = false;\n      } else {\n        ctx._source.in_stock = true;\n      }\n    \"\"\"\n  }\n}\n\nUpdate the “in_stock” field for products with low inventory:\n\nPOST /products_new/_update_by_query\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock = false\"\n  },\n  \"query\": {\n    \"range\": {\n      \"inventory_count\": {\n        \"lt\": 10\n      }\n    }\n  }\n}\n\n\n\n\nSearch the new index to verify the reindexed data and updated in_stock field\n\nGET /products_new/_search\nThe response should show the reindexed products with the “in_stock” field set correctly based on the inventory count.\n\nSearch the old index to verify the original data and the absence of in_stock\n\nGET /products_old/_search\n\n\n\n\nThe Reindex API is used to copy data from the old index to the new index while applying a script to set the “in_stock” field based on the inventory count.\nThe Update By Query API is used to update the in_stock field for products with an inventory count lower than 10.\n\n\n\n\n\nDelete the two indices\n\nDELETE products_old\nDELETE products_new\n\n\n\n\nReindex API\nUpdate By Query API\nScripting\n\n\n\n\n\n\n\n\nCreate the products_old index and add sample products.\nReindex documents from products_old to products_new.\nUpdate the mappings in products_new.\nIncrease the price of all products in products_new by 10%.\n\n\n\n\n\nCreate the products_old index and add sample products\nPUT /products_old\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nPOST /products_old/_bulk\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"1\" } }\n{ \"product_id\": \"1\", \"name\": \"Wireless Mouse\", \"description\": \"A high-quality wireless mouse with ergonomic design.\", \"price\": 29.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"2\" } }\n{ \"product_id\": \"2\", \"name\": \"Gaming Keyboard\", \"description\": \"Mechanical gaming keyboard with customizable RGB lighting.\", \"price\": 79.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"3\" } }\n{ \"product_id\": \"3\", \"name\": \"USB-C Hub\", \"description\": \"A versatile USB-C hub with multiple ports.\", \"price\": 49.99, \"availability_status\": true }\nCreate the new index with updated mappings\n\nDefine the new index products_new with the desired mappings.\n\nPUT /products_new\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\nReindex Documents from products_old to products_new\n\nUse the Reindex API to copy documents from the old index to the new index.\nPOST /_reindex\n{\n \"source\": {\n \"index\": \"products_old\"\n },\n \"dest\": {\n \"index\": \"products_new\"\n }\n}\n\nUpdate prices in the new index using the Update By Query API to increase the price of all products in products_new by 10%.\n\nPOST /products_new/_update_by_query\n{\n  \"script\": {\n    \"source\": \"ctx._source.price *= 1.10\",\n    \"lang\": \"painless\"\n  },\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nVerify the reindexing\n\nGET /products_old/_count\nGET /products_new/_count\n\nVerify the price update\n\nGET /products_new/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nMappings Update: Ensure the new index products_new has the updated mappings to accommodate any changes in the document structure.\nPrice Update Script: The script in the Update By Query API uses the painless language to increase the price by 10%. This is a simple and efficient way to update document fields.\n\n\n\n\n\nDelete the indices\n\nDELETE /products_old\nDELETE /products_new\n\n\n\n\nIndex Settings\nPainless Scripting Language\nReindex API\nUpdate By Query API"
  },
  {
    "objectID": "4-data-processing.html#task-define-and-use-an-ingest-pipeline-that-satisfies-a-given-set-of-requirements-including-the-use-of-painless-to-modify-documents",
    "href": "4-data-processing.html#task-define-and-use-an-ingest-pipeline-that-satisfies-a-given-set-of-requirements-including-the-use-of-painless-to-modify-documents",
    "title": "Data Processing",
    "section": "",
    "text": "Use an ingest pipeline to process incoming documents.\nApply a Painless script to modify specific fields.\nEnrich the data by adding a timestamp and converting the price to a different currency.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the ingest pipeline with a Painless script and additional processors:\n\nPUT /_ingest/pipeline/product_pipeline\n{\n  \"processors\": [\n    {\n      \"script\": {\n        \"lang\": \"painless\",\n        \"source\": \"\"\"\n          if (ctx.price != null) {\n            ctx.price_usd = ctx.price * 1.1; // Convert to USD assuming 1.1 is the conversion rate\n          }\n        \"\"\"\n      }\n    },\n    {\n      \"set\": {\n        \"field\": \"timestamp\",\n        \"value\": \"{{_ingest.timestamp}}\"\n      }\n    }\n  ]\n}\n\nCreate the index\n\nPUT /product_catalog\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"price_usd\": {\n        \"type\": \"double\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n\nIndex documents using the ingest pipeline\n\nPOST /product_catalog/_bulk?pipeline=product_pipeline\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 20.0 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 30.0 }\n\n\n\n\nVerify the ingest pipeline configuration:\n\nGET /_ingest/pipeline/product_pipeline\n\nSearch the indexed documents to ensure the modifications have been applied:\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nThe Painless script modifies the price field to convert it to USD and stores it in a new field price_usd.\nThe set processor adds a timestamp to each document to track when it was ingested.\nEnsure the pipeline processes all incoming documents to maintain data consistency.\n\n\n\n\n\nDelete the index\n\nDELETE product_catalog\n\nDelete the pipeline\n\nDELETE _ingest/pipeline/product_pipeline\n\n\n\n\nBulk API\nIngest Node Pipelines\nPainless Scripting Language\n\n\n\n\n\nThis example creates another ingest pipeline, but this time adds it directly into the index definition.\n\n\n\nExtract the log level (DEBUG, INFO, WARNING, ERROR) from the log message.\nExtract the log timestamp in ISO format.\nAdd a new field log_level_tag with a value based on the log level (e.g. DEBUG -&gt; DEBUG_LOG).\nAdd a new field log_timestamp_in_seconds with the timestamp in seconds.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate an ingest pipeline:\n\nPUT /_ingest/pipeline/logging-pipeline\n{\n  \"description\": \"Extract and transform log data\",\n  \"processors\": [\n    {\n      \"grok\": {\n        \"field\": \"message\",\n        \"patterns\": [\"%{LOGLEVEL:log_level} %{TIMESTAMP_ISO8601:log_timestamp} %{GREEDYDATA:message}\"]\n      }\n    },\n    {\n      \"script\": {\n        \"source\": \"\"\"\n          ctx.log_level_tag = ctx.log_level.toUpperCase() + '_LOG';\n          ctx.log_timestamp_in_seconds = ZonedDateTime.parse(ctx.log_timestamp).toEpochSecond();\n        \"\"\",\n        \"lang\": \"painless\"\n      }\n    }\n  ]\n}\n\nCreate an index with the ingest pipeline:\n\nPUT /logging-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"log_level\": {\n        \"type\": \"keyword\"\n      },\n      \"log_timestamp\": {\n        \"type\": \"date\"\n      },\n      \"log_level_tag\": {\n        \"type\": \"keyword\"\n      },\n      \"log_timestamp_in_seconds\": {\n        \"type\": \"long\"\n      }\n    }\n  },\n  \"settings\": {\n    \"index\": {\n      \"default_pipeline\": \"logging-pipeline\"\n    }\n  }\n}\n\nAdd documents to the index:\n\nPOST /logging-index/_bulk\n{ \"index\": { \"_index\": \"logging-index\" } }\n{ \"message\": \"DEBUG 2022-05-25T14:30:00.000Z This is a debug message\" }\n{ \"index\": { \"_index\": \"logging-index\" } }\n{ \"message\": \"INFO 2022-05-25T14:30:00.000Z This is an info message\" }\n\n\n\n\nVerify that the documents have been processed correctly:\n\nGET /logging-index/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nThe ingest pipeline uses the Grok processor to extract the log level and timestamp from the log message.\nThe Painless script processor is used to transform the log level and timestamp into new fields.\n\n\n\n\n\nDelete the index\n\nDELETE logging-index\n\nDelete the pipeline\n\nDELETE _ingest/pipeline/logging-pipeline\n\n\n\n\nIngest Node Pipelines\nPainless Scripting Language\n\n\n\n\n\n\n\n\nAn index named products with fields like name, price, category, description, etc.\nPreprocess incoming product data using an ingest pipeline:\n\nLowercase the name and category fields\nRemove HTML tags from the description field\nCalculate a discounted_price field based on the price field and a discount percentage stored in a pipeline variable\n\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the ingest pipeline:\n\nPUT /_ingest/pipeline/product_pipeline\n{\n  \"description\": \"Ingest pipeline for product data\",\n  \"processors\": [\n    {\n      \"lowercase\": {\n        \"field\": \"name\"\n      }\n    },\n    {\n      \"lowercase\": {\n        \"field\": \"category\"\n      }\n    },\n    {\n      \"script\": {\n        \"lang\": \"painless\",\n        \"source\": \"\"\"\n          if (ctx.description != null) {\n            Pattern pattern = /&lt;[^&gt;]+&gt;/;\n            Matcher matcher = pattern.matcher(ctx.description);\n            ctx.description = matcher.replaceAll('');\n          }\n        \"\"\"\n      }\n    },\n    {\n      \"script\": {\n        \"lang\": \"painless\",\n        \"source\": \"\"\"\n          double discount_percentage = 0.2;\n          double discounted_price = ctx.price * (1 - discount_percentage);\n          ctx.discounted_price = discounted_price;\n        \"\"\"\n      }\n    }\n  ]\n}\n\nIndex a sample document using the ingest pipeline:\n\nPUT /products/_doc/1?pipeline=product_pipeline\n{\n  \"name\": \"Product A\",\n  \"price\": 99.99,\n  \"category\": \"Electronics\",\n  \"description\": \"A &lt;b&gt;high-quality&lt;/b&gt; product for running enthusiasts.\"\n}\n\n\n\n\nSearch the products index and verify that the document has been processed by the ingest pipeline:\n\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\nThe response should show:\n\nname and category fields in lowercase\ndescription field without HTML tags\ndiscounted_price field calculated based on the price field and the discount percentage\n\n\n\n\n\nThe ingest pipeline is defined with a list of processors that perform specific operations on incoming documents.\nThe lowercase processor lowercases the name and category fields.\nThe remove processor removes HTML tags from the description field using a regular expression.\nThe script processor uses the Painless scripting language to calculate the discounted_price field based on the price field and a discount percentage variable.\n\n\n\n\n\nDelete the index\n\nDELETE products\n\nDelete the pipeline\n\nDELETE _ingest/pipeline/product_pipeline\n\n\n\n\nIngest Node\nIngest Pipelines\nIngest Processors\nPainless Scripting Language"
  },
  {
    "objectID": "4-data-processing.html#task-define-runtime-fields-to-retrieve-custom-values-using-painless-scripting",
    "href": "4-data-processing.html#task-define-runtime-fields-to-retrieve-custom-values-using-painless-scripting",
    "title": "Data Processing",
    "section": "",
    "text": "Use a runtime field to calculate a discount on product prices.\nApply a Painless script to dynamically compute the discounted price.\nEnsure the runtime field is available for queries and aggregations.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nDefine the index with appropriate mappings:\n\nPUT /product_catalog\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    },\n    \"runtime\": {\n      \"discounted_price\": {\n        \"type\": \"double\",\n        \"script\": {\n          \"source\": \"\"\"\n            if (doc['price'].size() != 0) {\n              emit(doc['price'].value * 0.9);\n            } else {\n              emit(Double.NaN);\n            }\n          \"\"\"\n        }\n      }\n    }\n  }\n}\n\nIndex sample documents using the _bulk endpoint:\n\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 20.0 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 30.0 }\n\n\n\n\nSearch the indexed documents and retrieve the runtime field\n\nGET /product_catalog/_search\n{\n  \"_source\": [\"name\", \"price\"],\n  \"fields\": [\"discounted_price\"],\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\nVerify the discounted price in the search results\n\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"script_fields\": {\n    \"discounted_price\": {\n      \"script\": {\n        \"source\": \"doc['price'].value * 0.9\"\n      }\n    }\n  }\n}\n\n\n\n\nThe Painless script calculates a 10% discount on the price.\nRuntime fields are defined in the index mappings and can be used for querying and aggregations without being stored in the index.\n\n\n\n\n\nDelete the index\n\nDELETE product_catalog\n\n\n\n\nBulk API\nPainless Scripting Language\nRuntime Fields\n\n\n\n\n\n\n\n\nExtract the domain from a URL field.\nUse Painless scripting to define the runtime field.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate an index with a URL field:\n\nPUT /myindex\n{\n  \"mappings\": {\n    \"properties\": {\n      \"url\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\nDefine a runtime field to extract the domain:\n\nPUT /myindex/_mapping\n{\n  \"runtime\": {\n    \"domain\": {\n      \"type\": \"keyword\",\n      \"script\": {\n        \"source\": \"\"\"\n          if (doc['url'].size() != 0) {\n            String url = doc['url'].value;\n            int startIndex = url.indexOf(\"://\") + 3;\n            int endIndex = url.indexOf(\"/\", startIndex);\n            if (endIndex == -1) {\n              endIndex = url.length();\n            }\n            emit(url.substring(startIndex, endIndex));\n          }\n        \"\"\"\n      }\n    }\n  }\n}\n\nAdd documents to the index:\n\nPOST /myindex/_bulk\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"url\": \"https://www.example.com/path/to/page\" }\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"url\": \"http://sub.example.com/other/page\" }\n\n\n\n\nVerify that the runtime field is working correctly:\n\nGET /myindex/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\"domain\"]\n}\n\n\n\n\nThe runtime field uses Painless scripting to extract the domain from the URL field.\nThe script splits the URL into components and returns the domain.\n\n\n\n\n\nDelete the index\n\nDELETE myindex\n\n\n\n\nPainless Scripting\nRuntime Fields\n\n\n\n\n\n\n\n\nDefine a search query that utilizes a runtime field to calculate the age difference in years between two date fields (date_of_birth and current_date) within the search results.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate the Index\n\nPUT /people\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"date_of_birth\": {\n        \"type\": \"date\"\n      },\n      \"current_date\": {\n        \"type\": \"date\"\n      }\n    },\n    \"runtime\": {\n      \"age_difference\": {\n        \"type\": \"long\",\n        \"script\": {\n          \"source\": \"\"\"\n            if (doc['date_of_birth'].size() != 0 && doc['current_date'].size() != 0) {\n              ZonedDateTime dob = ZonedDateTime.parse(doc['date_of_birth'].value.toString());\n              ZonedDateTime current = ZonedDateTime.parse(doc['current_date'].value.toString());\n              long yearsBetween = ChronoUnit.YEARS.between(dob, current);\n              emit(yearsBetween);\n            }\n          \"\"\"\n        }\n      }\n    }\n  }\n}\n\nIndex sample documents\n\nPOST /people/_bulk\n{ \"index\": { \"_index\": \"people\", \"_id\": \"1\" } }\n{ \"name\": \"Alice\", \"date_of_birth\": \"1990-01-01\", \"current_date\": \"2024-07-08\" }\n{ \"index\": { \"_index\": \"people\", \"_id\": \"2\" } }\n{ \"name\": \"Bob\", \"date_of_birth\": \"1985-05-15\", \"current_date\": \"2024-07-08\" }\n{ \"index\": { \"_index\": \"people\", \"_id\": \"3\" } }\n{ \"name\": \"Charlie\", \"date_of_birth\": \"2000-12-25\", \"current_date\": \"2024-07-08\" }\n\nConstruct a search query with a runtime field:\n\nGET /people/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"script_fields\": {\n    \"age_difference\": {\n      \"script\": {\n        \"source\": \"\"\"\n          if (doc['date_of_birth'].size() != 0 && doc['current_date'].size() != 0) {\n            ZonedDateTime dob = ZonedDateTime.parse(doc['date_of_birth'].value.toString());\n            ZonedDateTime current = ZonedDateTime.parse(doc['current_date'].value.toString());\n            long yearsBetween = ChronoUnit.YEARS.between(dob, current);\n            return yearsBetween;\n          } else {\n            return null;\n          }\n        \"\"\"\n      }\n    }\n  }\n}\n\n\n\n\nEnsure the documents in your index have date_of_birth and current_date fields in a compatible date format (e.g., milliseconds since epoch).\nRun the search query and examine the response. The results should include an additional field named age representing the calculated age difference in years for each document.\n\n\n\n\n\nThe runtime field definition utilizes Painless scripting to perform the age calculation.\nThe script calculates the difference in milliseconds between current_date and date_of_birth, then divides by the conversion factor for milliseconds in a year (considering leap years).\nThe Math.floor function ensures the age is a whole number of years.\n\n\n\n\n\nDelete the index\n\nDELETE people\n\n\n\n\nPainless Scripting\nRuntime Fields"
  },
  {
    "objectID": "6-appendix.html",
    "href": "6-appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "If you’re going to learn from the various examples in this study guide it would help to have a running Elastic cluster. You can get one in one of three ways:\n\nElastic Cloud\n\nAccess to a paid cluster\n14-day free trial\n\nLocal instance\n\nIf you have access to a paid Elastic Cloud instance then you can skip the rest of these instructions as you have already been using Elastic to some extent and you can just jump into the exercises.\nIf you don’t already have an account with Elastic, then the most convenient method is to have a local cluster running on your PC/Mac using Docker or, if you insist, creating a 14-day free trail of Elastic that will run out long before you are ready to give it up. In other words, create a local instance and spare yourself some pain.\nLet’s briefly go over the set-up of either:\n\nthe 14-day free trail\nlocal instance\n\n\n\nOnce you have signed up for your 14-day trial you will be directed to create your first deployment. Don’t rush it. In the process of creating the new deployment you will be shown:\n\nthe username (elastic)\nthe password (the usual collection of numbers/letters/punctuation)\n\n\n\nFigure N: The Username/Password for use from the Command Line\n\nOnce the deployment is created and you are taken to one of the many landing pages you will need the URL to the Elastic cluster endpoint that will allow you to work with Elasticsearch. The Elastic cluster endpoint can be found by:\n\n1. Pressing the hamburger menu (the three parallel lines) on the top left hand side of the Elastic web page to open the main menu:\n\nFigure N:\n1. Press Manage This Deployment. That will take you the Deployments page which should list your deployment. There will be a section titled Applications. Click on the Elasticsearch link labeled Copy Endpoint.\n\nFigure N:\n1. Paste that link somewhere for future use. You’re going to be using it quite a bit. If you lose it just go back to the Deployments page and click on the link again.\n\n\n\n\nDocker. We are going to use Docker. It is the easiest to clean up when you are done vs. copying github repos, etc.\nThe instructions on the Elastic site explain how to install the Elastic Stack locally using Docker. This book assumes you are using Ubuntu Linux and understand the Linux command line and shell. If you are using another version of Linux then it may be worth looking at the Elastic documentation for the variations (there aren’t many).\nOn The Off-chance You Have already Done This and Need to Reset Everything\nWe know. It happens. You follow all the steps exactly and still need to redo all the steps.\nTo reset you Elasticsearch install do the following (assuming you have docker installed):\nsudo docker stop es01 es02 kibana sudo docker rm es01 es02 kibana sudo docker network rm elastic\nYou can safelyl start again.\nInstall Docker (if you don’t already have it installed) Open a terminal window and execute: sudo snap install docker\nCreate a Docker Network Named Elastic sudo docker network create elastic Install the Elastic image\nThe version available at the time this was written was 8.14.1. It will probably have changed by the time you get this study guide. The exam only cares about Elastic version 8.1. sudo docker pull docker.elastic.co/elasticsearch/elasticsearch:8.14.1 Run the Elastic image\nCreate a single-node cluster named es01. It will use the Docker network we created above as well as the default HTTP port, transport port, and the Elastic image we just downloaded.\nAt the very end of the installation there will be output that you need to retain.\nsudo docker run --name es01 --net elastic -p 9200:9200 -p 9300:9300 -e\n\"discovery.type=single-node\" -e \"cluster.name=cluster-1\" -m 1GB -t docker.elastic.co/elasticsearch/elasticsearch:8.14.1\nThere will be a lot of output, but at the end of the installation there will be some output that includes:\n\nthe password for user elastic\nthe HTTP CA certificate SHA-256 fingerprint\nan enrollment token (you will use it when you set up Kibana)\nanother enrollment token used to enroll other nodes (you won’t be using that in this study guide) Just copy the entire block to a text editor and remember where you saved it.\n\nNote that the name of the container instance is es01. You’ll need to remember that when you start the container again later.\nPull/Run the Kibana image\nOpen another terminal tab or window as es01 should still be running and waiting to hook up with the Kibana instance.\nsudo docker pull docker.elastic.co/kibana/kibana:8.14.1 sudo docker run --name kibana --net elastic -p 5601:5601\ndocker.elastic.co/kibana/kibana:8.14.1\nNotice that the name of the container instance is kibana. You’ll need that when you start the container again later.\nWhen you run the Kibana instance (the last command above) the output to the command line will print out a URL to the Kibana instance. The URL will be something like:\nhttp://0.0.0.0:5601/?code=971626\nThe code will probably be different.\nCopy and paste the URL into your browser which should open on the following landing page:\n\n\nFigure A-1: Configure Kibana page\n\nYou need to enter the enrollment token that appeared during the execution of es01 (the Elasticsearch container instance). Enter the token in the Kibana screen and press Configure Elastic. If all goes well the Welcome to Elastic page should appear.\n\nFigure A-2: The Kibana Login Page to the Elastic Instance Log in with the user name elastic and the password from the es01 output.\nPress Log In.\nA Welcome Home landing page should appear. The first time it will display a dialog asking if you want it to do things for you or if you want to Explore On Your Own.\nPress Explore on My Own.\nPress [hamburger menu] → Management → DevTools\nThat should open on the Console where you will be working through the various examples listed in this book. Enter the following and press the triangle to the right on the line where you entered this:\nGET /_cluster/settings\nYou should get something that looks like this:\n{\n\"persistent\": {},\n\"transient\": {}\n}\nGo celebrate! You have a working Elasticsearch cluster.\nTo shut it down the first time just press Ctrl-C in each tab/terminal where you have the instances running.\nTo restart your cluster open another tab or terminal and enter:\nsudo docker start es01 kibana\nWhen you decide to stop your cluster enter: sudo docker stop es01 kibana\n\n\n\nSetting up es01\n\nShell into the es01 container\n\nsudo docker exec -it es01 /bin/bash\n\ncreate the p12 certificate\n\n# https://www.elastic.co/guide/en/elasticsearch/reference/8.14/security-basic-setup.html\n\nelasticsearch-certutil ca\n\nelasticsearch-certutil cert --ca elastic-stack-ca.p12\n\nmove the p12 file into the config/certs directory and modify the file permissions\n\ncd config/certs \nmv ../../elastic-certificates.p12 . \nchmod ug+rw elastic-certificates.p12\n\nreset the passwords (even if there is none)\n\nelasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password \nelasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password\n\nexit the container\n\nexit\n\ndocker copy the elasticsearch.yml file from es01\n\nsudo docker cp es01:/usr/share/elasticsearch/config/elasticsearch.yml elasticsearch-es01.yml sudo chmod ugo+rw elasticsearch.yml\n\nEdit elasticsearch-es01.yml\n\ncluster.name: \"cluster-1\" \n\nnetwork.host: 0.0.0.0 \n\nnode.name: c1-node-1\n\n# Enable security features xpack.security.enabled: true\nxpack.security.enrollment.enabled: false\nxpack.security.transport.ssl.enabled: true\n\nxpack.security.transport.ssl.verification_mode: certificate\nxpack.security.transport.ssl.client_authentication: required\n\nxpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12\nxpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12 \nxpack.security.authc.token.enabled: false\n\n# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents \nxpack.security.http.ssl: enabled: false\n\nkeystore.path: certs/http.p12\n\nCopy elasticsearch-es01.yml back to es01\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nrestart es01 and confirm commication with kibana\n\nsudo docker stop es01 kibana sudo docker start es01 kibana\n\nsudo docker logs -f es01 # may need separate terminal \nsudo docker logs -f kibana # may need separate terminal\nes02\n\nopen another tab or terminal and docker run es02\n\nsudo docker run --name es02 --net elastic -p 9201:9200 -p 9301:9300 -e \"discovery.type=single-node\" -e \"cluster.name=cluster-2\" -m 1GB -t docker.elastic.co/elasticsearch/elasticsearch:8.14.1\n\ndocker copy elastic-certificates.p12 from es01 (both clusters have to have the same transport certificate)\n\nsudo docker cp es01:/usr/share/elasticsearch/config/certs/elasticcertificates.p12 .\n\ndocker copy elastic-certificates.p12 to es02\n\nsudo docker cp elastic-certificates.p12 es02:/usr/share/elasticsearch/config/certs\n\nCopy elasticsearch-es01.yml to elasticsearch-es02.yml\n\ncp elasticsearch-es01.yml elasticsearch-es02.yml\n\nEdit elasticsearch-es02.yml. The only thing to change is the cluster name and the node name.\n\ncluster.name: \"cluster-2\"\n\nnetwork.host: 0.0.0.0\n\nnode.name: c2-node-1\n\n# Enable security features xpack.security.enabled: true\nxpack.security.enrollment.enabled: false\nxpack.security.transport.ssl.enabled: true\n\nxpack.security.transport.ssl.verification_mode: certificate\nxpack.security.transport.ssl.client_authentication: required\n\nxpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12\nxpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12 \nxpack.security.authc.token.enabled: false\n\n# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents \nxpack.security.http.ssl: enabled: false\n\nkeystore.path: certs/http.p12\n\nConfirm rw permissions to elasticsearch.yml\n\nls -l elasticsearch-es02.yml\n\ndocker copy the modified elasticsearch-es02.yml file to es02\n\nsudo docker cp elasticsearch-es02.yml es02:/usr/share/elasticsearch/config/elasticsearch-es02.yml\n\ndocker exec ssh es02\n\nsudo docker exec -it es02 /bin/bash\n\nreset the passwords to access the p12 file (even if there is none)\n\n# https://www.elastic.co/guide/en/elasticsearch/reference/8.14/security-basic-setup.html\n\nelasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password\nelasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password\n\nrestart es02\n\nsudo docker stop es02 \nsudo docker start es02\n\n\n# Do this from Kibana -&gt; Management -&gt; DevTools\n\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n          \"seeds\": [\n            \"es01:9300\"\n          ],\n          \"skip_unavailable\": true\n        },\n        \"es02\": {\n          \"seeds\": [\n            \"es02:9300\"\n          ],\n          \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}\nGo to the Remote Cluster dashboard page. You should see es01 and es02 listed and communication as active.\n\nFigure N: es01 and es02 are communicating and ready for cross-cluster queries.\nIf you ever feel the need to delete the connection between the two cluster just execute this:\nI HAVE NOT FOUND THIS TO BE TRUE:\n\nAFTER EVERY SESSION YOU MUST DELETE THE CLUSTER SETTINGS AND RESTORE THEM BEFORE STARTING AGAIN:\n\n# TO REMOVE THE CLUSTER SETTINGS\n\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster.remote.local.seeds\": null,\n    \"cluster.remote.local.skip_unavailable\": null,\n    \"cluster.remote.remote.seeds\": null,\n    \"cluster.remote.remote.skip_unavailable\": null\n  }\n}\n\n\n\n\n\nQ: What is the URL for the running instance of Elasticsearch?\nA: On your local machine the URL is usually https://172.18.0.2:9200 or https://localhost:9200.\nIn the Elastic Cloud, after you have created a deployment, you can click on:\n [hamburger menu] → Manage this deployment → [takes you to the landing page of your deployment] → Applications → Elasticsearch → Copy endpoint Paste the copied endpoint into a browser and get to work.\nQ: How do I run the two docker images of elastic and kibana when I need to restart them?\nA: In a terminal window run:\nsudo docker start es01 sudo docker start kibana\nGo to the Kibana URL to run all the exercises:\nhttp://0.0.0.0:5601\nQ: What is the importance of the @timestamp field?\nA: The @timestamp field in Elasticsearch serves a crucial purpose related to time-based data.\n\n1. Timestamp for Time-Series Data:\n\n\nElasticsearch often deals with time-series data, where each document represents an event or record associated with a specific timestamp (e.g., log entries, sensor readings, transactions).\nThe @timestamp field explicitly indicates when the document was created or when the event occurred.\n\n\n7. Data Streams and Rollover Policies:\n\n\nWhen using data streams, which handle append-only time series, the @timestamp field is mandatory.\nData streams manage collections of documents with a time dimension. Having an explicit timestamp field allows Elasticsearch to apply time-based rollover policies.\nRollover policies create new write indices (current write index) and backing indices (for historical data) based on time intervals or document count thresholds.\nThe @timestamp field ensures that data streams can efficiently manage these indices.\n\n\n8. Write and Backing Indices:\n\n\nWrites are directed to the current write index, while reads can access all backing indices.\nThe @timestamp field plays a key role in determining which indices contain relevant data for a given time range.\nWithout this field, Elasticsearch wouldn’t know how to organize data into time-based segments.\n\n\n9. Mapping and Default Options:\n\n\nEvery document indexed to a data stream must contain an @timestamp field.\nBy default, Elasticsearch maps @timestamp as a date field with default options.\nYou can customize the mapping if needed, but having the field is essential for proper data stream functioning.\n\nIn summary, the @timestamp field ensures that Elasticsearch can effectively manage time-series data, apply rollover policies, and organize indices based on time intervals. It’s a fundamental part of handling time-based data in Elasticsearch3 4 . If you’re working with logs, events, or any time-stamped data, the @timestamp field becomes critical for accurate indexing and querying.\nQ: This stuff looks hard. Why?\nA: Okay, so the use of JSON for everything in Elasticsearch gives the impression that you must be a JSON rockstar to make things happen. Not the truth, but it helps. The fact is that you can accomplish a lot with a little in Elasticsearch, but once you leave the comfort of the easy stuff it can be quite complicated to make things work. For example, let’s say that you want to create an index. That’s easy as all you have to do it a PUT and the name of the index:\nPUT my_index\nWoo hoo! You will see the above sometimes done as:\nPUT /my_index\nor even\nPUT /my_index/\nThey are all equivalent. Compare that to a request that executes a query with sorting, filtering, and aggregations all in one call:\nPUT /_scripts/product_search\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"multi_match\": {\n                \"query\": \"{{query_string}}\",\n                \"fields\": [\"name\", \"description\"]\n              }\n            }\n          ],\n          \"filter\": [\n            {\n              \"term\": {\n                \"category\": \"{{category}}\"\n              }\n            },\n            {\n              \"terms\": {\n                \"tags\": \"{{tags}}\"\n              }\n            }\n          ]\n        }\n      },\n      \"sort\": [\n        {\n          \"{{sort_field}}\": \"{{sort_order}}\"\n        }\n      ],\n      \"from\": \"{{from}}\",\n      \"size\": \"{{size}}\",\n      \"aggs\": {\n        \"categories\": {\n          \"terms\": {\n            \"field\": \"category\"\n          }\n        },\n        \"tags\": {\n          \"terms\": {\n            \"field\": \"tags\"\n          }\n        },\n        \"specifications\": {\n          \"nested\": {\n            \"path\": \"specifications\"\n          },\n          \"aggs\": {\n            \"ram\": {\n              \"terms\": {\n                \"field\": \"specifications.ram\"\n              }\n            },\n            \"storage\": {\n              \"terms\": {\n                \"field\": \"specifications.storage\"\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\nOf course, if we turn the above into a search template, as we do in Developing Search Applications &gt; Task 3.5 &gt; Example 3, then we can execute the above with a much simpler request:\nGET /products/_search/template\n{\n  \"id\": \"product_search\",\n  \"params\": {\n    \"query_string\": \"product\",\n    \"category\": \"Electronics\",\n    \"tags\": [\"electronics\", \"gadget\"],\n    \"sort_field\": \"price\",\n    \"sort_order\": \"desc\",\n    \"from\": 0,\n    \"size\": 2\n  }\n}\nBut notice how all the manual work. First you have to work out the query, which is non-trivial, and then convert it into a search template and then define the final query based on the needs to the template. The template makes things much more efficient (can you say function call?), but the actual JSON is far from non-trivial. You must know the query syntax available in Elasticsearch to make it happen.\nThat can lead to a certain amount of confusion (is there an advantage, from an engineering perspective, to have so many different ways to do the same thing? Well, it is the Unix way…), but makes certain developers happy. Does it adhere to REST conventions? I’ll leave that to others to decide…\n\n\n\nThis is all about auto-complete. The more you know of the various nodes needed by the REST API call you are trying to make the quicker you can get back to completing your exercise.\nThe Console where you enter your REST API calls to Elasticsearch has a number of keyboard shortcuts to help you fill out the JSON that needs to be sent with most calls. Here is a list of the keyboard shortcuts in no particular order.\nOn a new line the first thing the auto-complete will do is fill in the HTTP command as soon as you enter one letter (press Enter after selecting one of the choices):\n\nFigure N:\nAs soon as you press another key another dropdown will open giving you more choices. For example, pressing slash (/):\n\n\n\nFigure N:\nPressing dot(.):\n\n\n\nFigure N:\nPressing any alphabetic key (a-z):\n\n\n\nFigure N:\nNotice how pressing practically any letter will who results with the letter that matches being highlighted:\n\n\n\nFigure N:\nIt will also list full word matches as well:\n\n\n\nFigure N:\nWhen you enter a single curly brace to open up a JSON node you will see a white X in a red box:\n\n\n\nFigure N:\nFear not! Press Enter and the Console editor will add the closing curly brace and indent the position of the cursor so you can type your next line:\n\n\n\nFigure N:\nTo determine what the allowable names are press open quote (“):\n\nFigure N:\nIf you selected index_patterns it would complete the line for you:\n\nFigure N:\nUnfortunately, not all of the node types will appear. Notice, for this example, dynamic_template does not appear after mappings even though it is meant to be used after mappings in the creation of a dynamic template:\n\nFigure N:\nOnce you start entering node information for an unrecognized node the auto-complete will stop working. I’m sure there is a way to fix that, but I have not found it. Be aware.\nOther valid attributes that don’t appear in auto-complete:\n\n• terms_set\n\nYou can select the Settings tab to change your Console editor settings:\n\nFigure N:\n\nFigure N:\nOr you can select the Help tab to look at the full list of supported keyboard commands:\n\nFigure N:\n\nFigure N:\nRandom cut and paste stuff\nTest\n\nVerify the index creation\n\n\nGET [index name] or GET /_cat/indices\n\n\nVerify the field mappings GET /[index name]/_mapping\nIndex and search sample data:\n\n\n◦ Index\nPOST /[index name]/_doc\n{\n[field name] : [value]\n[field name] : [value]\n}\n◦ Search\nGET /[index name]/_search?q=[field name]:[value]*\n\n\nAny other appropriate tests\n\n\n[CODE HERE]\n\n========\nTo check out the cluster from either a browser or using curl\nFor example If you are running a local cluster then you know your username/password. You should also know your endpoint as it will look something like:\nlocalhost:9200/[paths based on what you are trying to do]\nThe port might be different based on any customizations you might have made when you installed the cluster. Check that you can get to it by opening a command line window and run:\ncurl -u [your username]:[your password] -X GET \"localhost:9200/nonexistent/_search? pretty\"\nYou should get an error message from your cluster (unless you have an index called nonexistent) that looks something like this:\n{\n\"error\" : {\n\"root_cause\" : [\n{\n\"type\" : \"index_not_found_exception\",\n\"reason\" : \"no such index [nonexistent]\",\n\"resource.type\" : \"index_or_alias\",\n\"resource.id\" : \"nonexistent\",\n\"index_uuid\" : \"_na_\",\n\"index\" : \"nonexistent\"\n}\n],\n\"type\" : \"index_not_found_exception\",\n\"reason\" : \"no such index [nonexistent]\",\n\"resource.type\" : \"index_or_alias\",\n\"resource.id\" : \"nonexistent\",\n\"index_uuid\" : \"_na_\",\n\"index\" : \"nonexistent\"\n},\n\"status\" : 404\n}\nWith all that said, at some point you need to stop the containers and restart them at a later time (like when you want to get some sleep):\n\nCtrl+C to stop the containers (doesn’t matter which you stop first)\nIn one terminal window start Elasticsearch\n\n\ndocker start es01 #or whatever name you gave the container\n\n\nIn another terminal window start Kibana\n\n\ndocker start kibana # or whatever name you gave the containers\n\nGive it a minute or so and you should be able to see the login page again."
  },
  {
    "objectID": "6-appendix.html#set-up-your-elastic-cluster",
    "href": "6-appendix.html#set-up-your-elastic-cluster",
    "title": "Appendix",
    "section": "",
    "text": "If you’re going to learn from the various examples in this study guide it would help to have a running Elastic cluster. You can get one in one of three ways:\n\nElastic Cloud\n\nAccess to a paid cluster\n14-day free trial\n\nLocal instance\n\nIf you have access to a paid Elastic Cloud instance then you can skip the rest of these instructions as you have already been using Elastic to some extent and you can just jump into the exercises.\nIf you don’t already have an account with Elastic, then the most convenient method is to have a local cluster running on your PC/Mac using Docker or, if you insist, creating a 14-day free trail of Elastic that will run out long before you are ready to give it up. In other words, create a local instance and spare yourself some pain.\nLet’s briefly go over the set-up of either:\n\nthe 14-day free trail\nlocal instance\n\n\n\nOnce you have signed up for your 14-day trial you will be directed to create your first deployment. Don’t rush it. In the process of creating the new deployment you will be shown:\n\nthe username (elastic)\nthe password (the usual collection of numbers/letters/punctuation)\n\n\n\nFigure N: The Username/Password for use from the Command Line\n\nOnce the deployment is created and you are taken to one of the many landing pages you will need the URL to the Elastic cluster endpoint that will allow you to work with Elasticsearch. The Elastic cluster endpoint can be found by:\n\n1. Pressing the hamburger menu (the three parallel lines) on the top left hand side of the Elastic web page to open the main menu:\n\nFigure N:\n1. Press Manage This Deployment. That will take you the Deployments page which should list your deployment. There will be a section titled Applications. Click on the Elasticsearch link labeled Copy Endpoint.\n\nFigure N:\n1. Paste that link somewhere for future use. You’re going to be using it quite a bit. If you lose it just go back to the Deployments page and click on the link again.\n\n\n\n\nDocker. We are going to use Docker. It is the easiest to clean up when you are done vs. copying github repos, etc.\nThe instructions on the Elastic site explain how to install the Elastic Stack locally using Docker. This book assumes you are using Ubuntu Linux and understand the Linux command line and shell. If you are using another version of Linux then it may be worth looking at the Elastic documentation for the variations (there aren’t many).\nOn The Off-chance You Have already Done This and Need to Reset Everything\nWe know. It happens. You follow all the steps exactly and still need to redo all the steps.\nTo reset you Elasticsearch install do the following (assuming you have docker installed):\nsudo docker stop es01 es02 kibana sudo docker rm es01 es02 kibana sudo docker network rm elastic\nYou can safelyl start again.\nInstall Docker (if you don’t already have it installed) Open a terminal window and execute: sudo snap install docker\nCreate a Docker Network Named Elastic sudo docker network create elastic Install the Elastic image\nThe version available at the time this was written was 8.14.1. It will probably have changed by the time you get this study guide. The exam only cares about Elastic version 8.1. sudo docker pull docker.elastic.co/elasticsearch/elasticsearch:8.14.1 Run the Elastic image\nCreate a single-node cluster named es01. It will use the Docker network we created above as well as the default HTTP port, transport port, and the Elastic image we just downloaded.\nAt the very end of the installation there will be output that you need to retain.\nsudo docker run --name es01 --net elastic -p 9200:9200 -p 9300:9300 -e\n\"discovery.type=single-node\" -e \"cluster.name=cluster-1\" -m 1GB -t docker.elastic.co/elasticsearch/elasticsearch:8.14.1\nThere will be a lot of output, but at the end of the installation there will be some output that includes:\n\nthe password for user elastic\nthe HTTP CA certificate SHA-256 fingerprint\nan enrollment token (you will use it when you set up Kibana)\nanother enrollment token used to enroll other nodes (you won’t be using that in this study guide) Just copy the entire block to a text editor and remember where you saved it.\n\nNote that the name of the container instance is es01. You’ll need to remember that when you start the container again later.\nPull/Run the Kibana image\nOpen another terminal tab or window as es01 should still be running and waiting to hook up with the Kibana instance.\nsudo docker pull docker.elastic.co/kibana/kibana:8.14.1 sudo docker run --name kibana --net elastic -p 5601:5601\ndocker.elastic.co/kibana/kibana:8.14.1\nNotice that the name of the container instance is kibana. You’ll need that when you start the container again later.\nWhen you run the Kibana instance (the last command above) the output to the command line will print out a URL to the Kibana instance. The URL will be something like:\nhttp://0.0.0.0:5601/?code=971626\nThe code will probably be different.\nCopy and paste the URL into your browser which should open on the following landing page:\n\n\nFigure A-1: Configure Kibana page\n\nYou need to enter the enrollment token that appeared during the execution of es01 (the Elasticsearch container instance). Enter the token in the Kibana screen and press Configure Elastic. If all goes well the Welcome to Elastic page should appear.\n\nFigure A-2: The Kibana Login Page to the Elastic Instance Log in with the user name elastic and the password from the es01 output.\nPress Log In.\nA Welcome Home landing page should appear. The first time it will display a dialog asking if you want it to do things for you or if you want to Explore On Your Own.\nPress Explore on My Own.\nPress [hamburger menu] → Management → DevTools\nThat should open on the Console where you will be working through the various examples listed in this book. Enter the following and press the triangle to the right on the line where you entered this:\nGET /_cluster/settings\nYou should get something that looks like this:\n{\n\"persistent\": {},\n\"transient\": {}\n}\nGo celebrate! You have a working Elasticsearch cluster.\nTo shut it down the first time just press Ctrl-C in each tab/terminal where you have the instances running.\nTo restart your cluster open another tab or terminal and enter:\nsudo docker start es01 kibana\nWhen you decide to stop your cluster enter: sudo docker stop es01 kibana\n\n\n\nSetting up es01\n\nShell into the es01 container\n\nsudo docker exec -it es01 /bin/bash\n\ncreate the p12 certificate\n\n# https://www.elastic.co/guide/en/elasticsearch/reference/8.14/security-basic-setup.html\n\nelasticsearch-certutil ca\n\nelasticsearch-certutil cert --ca elastic-stack-ca.p12\n\nmove the p12 file into the config/certs directory and modify the file permissions\n\ncd config/certs \nmv ../../elastic-certificates.p12 . \nchmod ug+rw elastic-certificates.p12\n\nreset the passwords (even if there is none)\n\nelasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password \nelasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password\n\nexit the container\n\nexit\n\ndocker copy the elasticsearch.yml file from es01\n\nsudo docker cp es01:/usr/share/elasticsearch/config/elasticsearch.yml elasticsearch-es01.yml sudo chmod ugo+rw elasticsearch.yml\n\nEdit elasticsearch-es01.yml\n\ncluster.name: \"cluster-1\" \n\nnetwork.host: 0.0.0.0 \n\nnode.name: c1-node-1\n\n# Enable security features xpack.security.enabled: true\nxpack.security.enrollment.enabled: false\nxpack.security.transport.ssl.enabled: true\n\nxpack.security.transport.ssl.verification_mode: certificate\nxpack.security.transport.ssl.client_authentication: required\n\nxpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12\nxpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12 \nxpack.security.authc.token.enabled: false\n\n# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents \nxpack.security.http.ssl: enabled: false\n\nkeystore.path: certs/http.p12\n\nCopy elasticsearch-es01.yml back to es01\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nrestart es01 and confirm commication with kibana\n\nsudo docker stop es01 kibana sudo docker start es01 kibana\n\nsudo docker logs -f es01 # may need separate terminal \nsudo docker logs -f kibana # may need separate terminal\nes02\n\nopen another tab or terminal and docker run es02\n\nsudo docker run --name es02 --net elastic -p 9201:9200 -p 9301:9300 -e \"discovery.type=single-node\" -e \"cluster.name=cluster-2\" -m 1GB -t docker.elastic.co/elasticsearch/elasticsearch:8.14.1\n\ndocker copy elastic-certificates.p12 from es01 (both clusters have to have the same transport certificate)\n\nsudo docker cp es01:/usr/share/elasticsearch/config/certs/elasticcertificates.p12 .\n\ndocker copy elastic-certificates.p12 to es02\n\nsudo docker cp elastic-certificates.p12 es02:/usr/share/elasticsearch/config/certs\n\nCopy elasticsearch-es01.yml to elasticsearch-es02.yml\n\ncp elasticsearch-es01.yml elasticsearch-es02.yml\n\nEdit elasticsearch-es02.yml. The only thing to change is the cluster name and the node name.\n\ncluster.name: \"cluster-2\"\n\nnetwork.host: 0.0.0.0\n\nnode.name: c2-node-1\n\n# Enable security features xpack.security.enabled: true\nxpack.security.enrollment.enabled: false\nxpack.security.transport.ssl.enabled: true\n\nxpack.security.transport.ssl.verification_mode: certificate\nxpack.security.transport.ssl.client_authentication: required\n\nxpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12\nxpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12 \nxpack.security.authc.token.enabled: false\n\n# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents \nxpack.security.http.ssl: enabled: false\n\nkeystore.path: certs/http.p12\n\nConfirm rw permissions to elasticsearch.yml\n\nls -l elasticsearch-es02.yml\n\ndocker copy the modified elasticsearch-es02.yml file to es02\n\nsudo docker cp elasticsearch-es02.yml es02:/usr/share/elasticsearch/config/elasticsearch-es02.yml\n\ndocker exec ssh es02\n\nsudo docker exec -it es02 /bin/bash\n\nreset the passwords to access the p12 file (even if there is none)\n\n# https://www.elastic.co/guide/en/elasticsearch/reference/8.14/security-basic-setup.html\n\nelasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password\nelasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password\n\nrestart es02\n\nsudo docker stop es02 \nsudo docker start es02\n\n\n# Do this from Kibana -&gt; Management -&gt; DevTools\n\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n          \"seeds\": [\n            \"es01:9300\"\n          ],\n          \"skip_unavailable\": true\n        },\n        \"es02\": {\n          \"seeds\": [\n            \"es02:9300\"\n          ],\n          \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}\nGo to the Remote Cluster dashboard page. You should see es01 and es02 listed and communication as active.\n\nFigure N: es01 and es02 are communicating and ready for cross-cluster queries.\nIf you ever feel the need to delete the connection between the two cluster just execute this:\nI HAVE NOT FOUND THIS TO BE TRUE:\n\nAFTER EVERY SESSION YOU MUST DELETE THE CLUSTER SETTINGS AND RESTORE THEM BEFORE STARTING AGAIN:\n\n# TO REMOVE THE CLUSTER SETTINGS\n\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster.remote.local.seeds\": null,\n    \"cluster.remote.local.skip_unavailable\": null,\n    \"cluster.remote.remote.seeds\": null,\n    \"cluster.remote.remote.skip_unavailable\": null\n  }\n}"
  },
  {
    "objectID": "6-appendix.html#faq",
    "href": "6-appendix.html#faq",
    "title": "Appendix",
    "section": "",
    "text": "Q: What is the URL for the running instance of Elasticsearch?\nA: On your local machine the URL is usually https://172.18.0.2:9200 or https://localhost:9200.\nIn the Elastic Cloud, after you have created a deployment, you can click on:\n [hamburger menu] → Manage this deployment → [takes you to the landing page of your deployment] → Applications → Elasticsearch → Copy endpoint Paste the copied endpoint into a browser and get to work.\nQ: How do I run the two docker images of elastic and kibana when I need to restart them?\nA: In a terminal window run:\nsudo docker start es01 sudo docker start kibana\nGo to the Kibana URL to run all the exercises:\nhttp://0.0.0.0:5601\nQ: What is the importance of the @timestamp field?\nA: The @timestamp field in Elasticsearch serves a crucial purpose related to time-based data.\n\n1. Timestamp for Time-Series Data:\n\n\nElasticsearch often deals with time-series data, where each document represents an event or record associated with a specific timestamp (e.g., log entries, sensor readings, transactions).\nThe @timestamp field explicitly indicates when the document was created or when the event occurred.\n\n\n7. Data Streams and Rollover Policies:\n\n\nWhen using data streams, which handle append-only time series, the @timestamp field is mandatory.\nData streams manage collections of documents with a time dimension. Having an explicit timestamp field allows Elasticsearch to apply time-based rollover policies.\nRollover policies create new write indices (current write index) and backing indices (for historical data) based on time intervals or document count thresholds.\nThe @timestamp field ensures that data streams can efficiently manage these indices.\n\n\n8. Write and Backing Indices:\n\n\nWrites are directed to the current write index, while reads can access all backing indices.\nThe @timestamp field plays a key role in determining which indices contain relevant data for a given time range.\nWithout this field, Elasticsearch wouldn’t know how to organize data into time-based segments.\n\n\n9. Mapping and Default Options:\n\n\nEvery document indexed to a data stream must contain an @timestamp field.\nBy default, Elasticsearch maps @timestamp as a date field with default options.\nYou can customize the mapping if needed, but having the field is essential for proper data stream functioning.\n\nIn summary, the @timestamp field ensures that Elasticsearch can effectively manage time-series data, apply rollover policies, and organize indices based on time intervals. It’s a fundamental part of handling time-based data in Elasticsearch3 4 . If you’re working with logs, events, or any time-stamped data, the @timestamp field becomes critical for accurate indexing and querying.\nQ: This stuff looks hard. Why?\nA: Okay, so the use of JSON for everything in Elasticsearch gives the impression that you must be a JSON rockstar to make things happen. Not the truth, but it helps. The fact is that you can accomplish a lot with a little in Elasticsearch, but once you leave the comfort of the easy stuff it can be quite complicated to make things work. For example, let’s say that you want to create an index. That’s easy as all you have to do it a PUT and the name of the index:\nPUT my_index\nWoo hoo! You will see the above sometimes done as:\nPUT /my_index\nor even\nPUT /my_index/\nThey are all equivalent. Compare that to a request that executes a query with sorting, filtering, and aggregations all in one call:\nPUT /_scripts/product_search\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"multi_match\": {\n                \"query\": \"{{query_string}}\",\n                \"fields\": [\"name\", \"description\"]\n              }\n            }\n          ],\n          \"filter\": [\n            {\n              \"term\": {\n                \"category\": \"{{category}}\"\n              }\n            },\n            {\n              \"terms\": {\n                \"tags\": \"{{tags}}\"\n              }\n            }\n          ]\n        }\n      },\n      \"sort\": [\n        {\n          \"{{sort_field}}\": \"{{sort_order}}\"\n        }\n      ],\n      \"from\": \"{{from}}\",\n      \"size\": \"{{size}}\",\n      \"aggs\": {\n        \"categories\": {\n          \"terms\": {\n            \"field\": \"category\"\n          }\n        },\n        \"tags\": {\n          \"terms\": {\n            \"field\": \"tags\"\n          }\n        },\n        \"specifications\": {\n          \"nested\": {\n            \"path\": \"specifications\"\n          },\n          \"aggs\": {\n            \"ram\": {\n              \"terms\": {\n                \"field\": \"specifications.ram\"\n              }\n            },\n            \"storage\": {\n              \"terms\": {\n                \"field\": \"specifications.storage\"\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\nOf course, if we turn the above into a search template, as we do in Developing Search Applications &gt; Task 3.5 &gt; Example 3, then we can execute the above with a much simpler request:\nGET /products/_search/template\n{\n  \"id\": \"product_search\",\n  \"params\": {\n    \"query_string\": \"product\",\n    \"category\": \"Electronics\",\n    \"tags\": [\"electronics\", \"gadget\"],\n    \"sort_field\": \"price\",\n    \"sort_order\": \"desc\",\n    \"from\": 0,\n    \"size\": 2\n  }\n}\nBut notice how all the manual work. First you have to work out the query, which is non-trivial, and then convert it into a search template and then define the final query based on the needs to the template. The template makes things much more efficient (can you say function call?), but the actual JSON is far from non-trivial. You must know the query syntax available in Elasticsearch to make it happen.\nThat can lead to a certain amount of confusion (is there an advantage, from an engineering perspective, to have so many different ways to do the same thing? Well, it is the Unix way…), but makes certain developers happy. Does it adhere to REST conventions? I’ll leave that to others to decide…"
  },
  {
    "objectID": "6-appendix.html#devtools-console-auto-complete",
    "href": "6-appendix.html#devtools-console-auto-complete",
    "title": "Appendix",
    "section": "",
    "text": "This is all about auto-complete. The more you know of the various nodes needed by the REST API call you are trying to make the quicker you can get back to completing your exercise.\nThe Console where you enter your REST API calls to Elasticsearch has a number of keyboard shortcuts to help you fill out the JSON that needs to be sent with most calls. Here is a list of the keyboard shortcuts in no particular order.\nOn a new line the first thing the auto-complete will do is fill in the HTTP command as soon as you enter one letter (press Enter after selecting one of the choices):\n\nFigure N:\nAs soon as you press another key another dropdown will open giving you more choices. For example, pressing slash (/):\n\n\n\nFigure N:\nPressing dot(.):\n\n\n\nFigure N:\nPressing any alphabetic key (a-z):\n\n\n\nFigure N:\nNotice how pressing practically any letter will who results with the letter that matches being highlighted:\n\n\n\nFigure N:\nIt will also list full word matches as well:\n\n\n\nFigure N:\nWhen you enter a single curly brace to open up a JSON node you will see a white X in a red box:\n\n\n\nFigure N:\nFear not! Press Enter and the Console editor will add the closing curly brace and indent the position of the cursor so you can type your next line:\n\n\n\nFigure N:\nTo determine what the allowable names are press open quote (“):\n\nFigure N:\nIf you selected index_patterns it would complete the line for you:\n\nFigure N:\nUnfortunately, not all of the node types will appear. Notice, for this example, dynamic_template does not appear after mappings even though it is meant to be used after mappings in the creation of a dynamic template:\n\nFigure N:\nOnce you start entering node information for an unrecognized node the auto-complete will stop working. I’m sure there is a way to fix that, but I have not found it. Be aware.\nOther valid attributes that don’t appear in auto-complete:\n\n• terms_set\n\nYou can select the Settings tab to change your Console editor settings:\n\nFigure N:\n\nFigure N:\nOr you can select the Help tab to look at the full list of supported keyboard commands:\n\nFigure N:\n\nFigure N:\nRandom cut and paste stuff\nTest\n\nVerify the index creation\n\n\nGET [index name] or GET /_cat/indices\n\n\nVerify the field mappings GET /[index name]/_mapping\nIndex and search sample data:\n\n\n◦ Index\nPOST /[index name]/_doc\n{\n[field name] : [value]\n[field name] : [value]\n}\n◦ Search\nGET /[index name]/_search?q=[field name]:[value]*\n\n\nAny other appropriate tests\n\n\n[CODE HERE]\n\n========\nTo check out the cluster from either a browser or using curl\nFor example If you are running a local cluster then you know your username/password. You should also know your endpoint as it will look something like:\nlocalhost:9200/[paths based on what you are trying to do]\nThe port might be different based on any customizations you might have made when you installed the cluster. Check that you can get to it by opening a command line window and run:\ncurl -u [your username]:[your password] -X GET \"localhost:9200/nonexistent/_search? pretty\"\nYou should get an error message from your cluster (unless you have an index called nonexistent) that looks something like this:\n{\n\"error\" : {\n\"root_cause\" : [\n{\n\"type\" : \"index_not_found_exception\",\n\"reason\" : \"no such index [nonexistent]\",\n\"resource.type\" : \"index_or_alias\",\n\"resource.id\" : \"nonexistent\",\n\"index_uuid\" : \"_na_\",\n\"index\" : \"nonexistent\"\n}\n],\n\"type\" : \"index_not_found_exception\",\n\"reason\" : \"no such index [nonexistent]\",\n\"resource.type\" : \"index_or_alias\",\n\"resource.id\" : \"nonexistent\",\n\"index_uuid\" : \"_na_\",\n\"index\" : \"nonexistent\"\n},\n\"status\" : 404\n}\nWith all that said, at some point you need to stop the containers and restart them at a later time (like when you want to get some sleep):\n\nCtrl+C to stop the containers (doesn’t matter which you stop first)\nIn one terminal window start Elasticsearch\n\n\ndocker start es01 #or whatever name you gave the container\n\n\nIn another terminal window start Kibana\n\n\ndocker start kibana # or whatever name you gave the containers\n\nGive it a minute or so and you should be able to see the login page again."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nThis is the website for The Definitive Guide to the Elastic Certified Engineer Exam v8.1. This book will attempt to go over all of the various topics that may appear in the Elastic Certified Engineer Exam, but it is by no means comprehensive. Using the list of modules and topics from the landing page on the Elastic site for the exam I have created as many examples as I thought useful in the pursuit of passing the test.\nWhile you are learning enough to pass the test you will also start to learn some of the additional areas about Elastic that you may not have considered in your journey to learn what you can. Very little knowledge goes to waste so revel in the thought that studying for the exam will also make you a better Elasticsearch developer.\nThis website, hosted on Github, will always be free, but bear in mind that the Elastic exam will move on from Elasticsearch v8.1 to the whatever version they consider the most important for their certification.\nIf you run into any problems (and you will), please report them here. I will update the content with the corrections as soon as possible. I appreciate your patience!\nIn general, the content from this website may not be copied or reproduced. The code examples are all so short that they do not exist in a Github repo anywhere. Copy and paste the various REST API calls from the HTML pages into the Elastic Kibana Console to your heart’s content.\nIf you find the online edition of the book useful, please consider ordering a paper copy or the eBook from your favorite vendor."
  },
  {
    "objectID": "2-searching-data.html",
    "href": "2-searching-data.html",
    "title": "Searching Data",
    "section": "",
    "text": "The following section will have only one full example, but will show variations of term and phrase queries. Also, bear in mind that when they say term they may not mean the Elasticsearch use of the word, but rather the generic search use of the word. There are a lot of ways to execute a search in Elasticsearch. Don’t get bogged down; focus on term and phrase searches for this section of the example.\n\n\n\n\n\nCreate an index\nIndex some documents\nExecute a term query\nExecute a phrase query\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\n\nPOST /example_index/_bulk\n{ \"index\": {} }\n{ \"title\": \"The quick brown fox\", \"text\": \"The quick brown fox jumps over the lazy dog.\" }\n{ \"index\": {} }\n{ \"title\": \"Fast and curious\", \"text\": \"A fast and curious fox was seen leaping over a lazy dog.\" }\n{ \"index\": {} }\n{ \"title\": \"A fox in action\", \"text\": \"In a remarkable display of agility, a quick fox effortlessly jumped over a dog.\" }\n{ \"index\": {} }\n{ \"title\": \"Wildlife wonders\", \"text\": \"Observers were amazed as the quick brown fox jumped over the lazy dog.\" }\n{ \"index\": {} }\n{ \"title\": \"Fox tales\", \"text\": \"The tale of the quick fox that jumped over the lazy dog has become a legend.\" }\n\nExecute a term query\n\n\nUse the GET method to search for documents containing the term “Elasticsearch” in the title field using 3 different ways to search at the term level (there are 10 different ways currently. Refer to the Term-level Queries documentation for the full list).\nGET example_index/_search\n{\n  \"query\": {\n    \"term\": {\n      \"title\": {\n        \"value\": \"quick\"\n      }\n    }\n  }\n}\nGET example_index/_search\n{\n  \"query\": {\n    \"terms\": {\n      \"text\": [\"display\", \"amazed\"]\n    }\n  }\n}\nGET example_index/_search\n{\n  \"query\": {\n    \"terms_set\": {\n      \"text\": {\n        \"terms\": [\"quick\", \"over\", \"display\"],\n        \"minimum_should_match\": 3\n      }\n    }\n  }\n}\n\n\nExecute a phrase query\n\n\nreturns 2 docs\nGET /example_index/_search\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"text\": \"quick brown fox\"\n    }\n  }\n}\nreturns 1 doc\nGET /example_index/_search\n{\n  \"query\": {\n    \"match_phrase_prefix\": {\n      \"text\": \"fast and curi\"\n    }\n  }\n}\nreturns 1 doc\nGET /example_index/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"default_field\": \"text\",\n      \"query\": \"\\\"fox jumps\\\"\"\n    }\n  }\n}\n\n\n\n\n\nThe default standard analyzer (lowercasing, whitespace tokenization, basic normalization) is used.\nThe term query is used for exact matches and is not analyzed, meaning it matches the exact term in the inverted index.\nThe match_phrase query analyzes the input text and matches it as a phrase, making it useful for finding exact sequences of terms.\n\n\n\n\n\nVerify the various queries return the proper results.\n\n\n\n\n\nDelete the example index\nDELETE example_index\n\n\n\n\n\nFull Text Queries\nMatch Phrase Query\nMatch Phrase Prefix Query\nQuery DSL\nTerm-level Queries\n\n\n\n\n\n\n\n\n\n\n\nSearch for documents with a term in the “title”, “description”, and “category” field\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\n\nPOST /books/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"title\": \"To Kill a Mockingbird\", \"description\": \"A novel about the serious issues of rape and racial inequality.\", \"category\": \"Fiction\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"title\": \"1984\", \"description\": \"A novel that delves into the dangers of totalitarianism.\", \"category\": \"Dystopian\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"title\": \"The Great Gatsby\", \"description\": \"A critique of the American Dream.\", \"category\": \"Fiction\" }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"title\": \"Moby Dick\", \"description\": \"The quest of Ahab to exact revenge on the whale Moby Dick.\", \"category\": \"Adventure\" }\n{ \"index\": { \"_id\": \"5\" } }\n{ \"title\": \"Pride and Prejudice\", \"description\": \"A romantic novel that also critiques the British landed gentry at the end of the 18th century.\", \"category\": \"Romance\" }\n\nCreate a boolean search query. The order in which the various clauses are added don’t matter to the final result.\n\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {}\n  }\n}\n\nAdd a must query for the description field. This will return 4 documents.\n\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a filter query for the category field. This will return 2 documents.\n\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"fiction\"\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a must_not filter for the title field. This will return 1 document.\n\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"fiction\"\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"term\": {\n            \"title\": {\n              \"value\": \"gatsby\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\nExecute the final search query\n\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"fiction\"\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"term\": {\n            \"title\": {\n              \"value\": \"gatsby\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe bool query allows for combining multiple queries and filters with Boolean logic.\nThe must, must_not, and filter clauses ensure that all searches and filters must match for a document to be returned.\n\n\n\n\n\nVerify that the search query returns documents with the term “novel”, “dream”, and “critique” in the “description” field. Why are there no documents with the term “critique”?\n\n\n\n\n\nDelete the index\nDELETE books\n\n\n\n\n\nElasticsearch Boolean Query\nElasticsearch Match Query\nElasticsearch Range Query\nElasticsearch Term Query\n\n\n\n\n\n\n\n\nFind all documents where the name field exists (name: *) and the price field falls within a specified range.\nAdditionally, filter out any documents where the discontinued field is set to true.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\n\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Coffee Maker\",\"price\":49.99,\"discontinued\":false}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Gaming Laptop\",\"price\":1299.99,\"discontinued\":false}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Wireless Headphones\",\"price\":79.99,\"discontinued\":true}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Smartwatch\",\"price\":249.99,\"discontinued\":false}\n\nConstruct the search query\n\nGET products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 10,\n              \"lte\": 300\n            }\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"match\": {\n            \"discontinued\": true\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nSimilar to the previous example, the bool query combines multiple conditions.\nThe must clause specifies documents that must match all conditions within it.\nThe range query ensures the price field is between $10 (inclusive) and $300 (inclusive).\nThe must_not clause excludes documents that match the specified criteria.\nThe match query filters out documents where the discontinued field is set to true.\n\n\n\n\n\nRun the search query and verify the results only include documents for products with:\n\nA price between $10 and $300 (inclusive).\ndiscontinued set to true (not discontinued).\n\n\nThis should return documents with IDs 1 and 4 (Coffee Maker and Smartwatch) based on the sample data.\n\n\n\n\nThe chosen price range (gte: 10, lte: 300) can be adjusted based on your specific needs.\nYou can modify the match query for the name field to use more specific criteria if needed.\n\n\n\n\n\nDelete the index\n\nDELETE products\n\n\n\n\nElasticsearch Boolean Query\nElasticsearch Match Query\nElasticsearch Range Query\nElasticsearch Term Query\n\n\n\n\n\n\n\n\nSearch for products that belong to the “Electronics” category.\nThe product name should contain the term “phone”.\nExclude products with a price greater than 500.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\n\nPOST /products/_bulk\n{\"index\": { \"_id\": 1 } }\n{ \"name\": \"Smartphone X\", \"category\": \"Electronics\", \"price\": 399.99 }\n{\"index\": { \"_id\": 2 } }\n{ \"name\": \"Laptop Y\", \"category\": \"Electronics\", \"price\": 799.99 }\n{\"index\": { \"_id\": 3 } }\n{ \"name\": \"Headphones Z\", \"category\": \"Electronics\", \"price\": 99.99 }\n{\"index\": { \"_id\": 4 } }\n{ \"name\": \"Gaming Console\", \"category\": \"Electronics\", \"price\": 299.99 }\n\nStart with a boolean query that only matches the category “electronics”. This returns 4 documents.\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a should clause for the word “phone”. This still returns 4 documents (why?).\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ],\n      \"should\": [\n        {\n          \"match\": {\n            \"name\": \"phone\"\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a must_not clause for the any price greater than $300. This still returns 2 documents (why?).\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ],\n      \"should\": [\n        {\n          \"match\": {\n            \"name\": \"phone\"\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"range\": {\n            \"price\": {\n              \"gt\": 300\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe search results should include the following documents:\n\nHeadphones Z\nGaming Console\n\n\n\n\n\n\nThe term query is used for exact matches on the category field.\nThe match query is used for matches on the name field.\nThe range query is used to filter out documents based on the price field.\nThe bool query combines these conditions using the specified occurrence types.\n\n\n\n\n\nDelete the index\n\nDELETE products\n\n\n\n\nElasticsearch Boolean Query\nElasticsearch Match Query\nElasticsearch Range Query\nElasticsearch Term Query\n\n\n\n\n\n\n\n\nCreate an index named “products”.\nCreate at least 4 documents with varying categories, prices, ratings, and brands.\nUse the “must”, “should”, “must_not”, and “filter” clauses in the Boolean query.\nThe query should return products that match the specified criteria and are from a specific brand.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the “products” index and add some sample documents using the /_bulk endpoint.\n\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Laptop\",\"category\":\"Electronics\",\"price\":1200,\"rating\":4.5,\"brand\":\"Apple\"}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Smartphone\",\"category\":\"Electronics\",\"price\":800,\"rating\":4.2,\"brand\":\"Samsung\"}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Sofa\",\"category\":\"Furniture\",\"price\":1000,\"rating\":3.8,\"brand\":\"IKEA\"}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Headphones\",\"category\":\"Electronics\",\"price\":150,\"rating\":2.5,\"brand\":\"Sony\"}\n{\"index\":{\"_id\":5}}\n{\"name\":\"Dining Table\",\"category\":\"Furniture\",\"price\":600,\"rating\":4.1,\"brand\":\"Ashley\"}\n\nStart with a boolean query that only matches the category “electronics”. This returns 3 documents.\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a must clause to only return products whose price is greater than $500. This should return 4 documents (why?).\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ],\n      \"must\": [\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 500\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a must_not clause to exclude items with a rating less than 4. This will return 3 documents.\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ],\n      \"must\": [\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 500\n            }\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"range\": {\n            \"rating\": {\n              \"lt\": 4\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd the final filter to only return Apple products. This should return 1 document.\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ],\n      \"must\": [\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 500\n            }\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"range\": {\n            \"rating\": {\n              \"lt\": 4\n            }\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"brand\": \"apple\"\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nCheck the response from the search query to ensure that it returns the expected documents (products in the “Electronics” category or with a price greater than $500, but excluding products with a rating less than 4, and from the brand “Apple”).\n\n\n\n\n\nThe filter clause is used to include only documents with the brand “Apple”.\n\n\n\n\n\nDelete the index\n\nDELETE products\n\n\n\n\nElasticsearch Boolean Query\nElasticsearch Term Query\nElasticsearch Range Query\n\n\n\n\n\n\nAsynchronous search uses the same parameters as regular search with a few extra features listed in their entirety in the document listed below. For example, in the solution below the size option is from the Search API. There is only one example here as you can look up the other options as needed during the exam.\n\n\n\n\n\nAn Elasticsearch index named “logs” with a large number of documents (e.g., millions of log entries).\nPerform a search on the “logs” index that may take a long time to complete due to the size of the index.\nRetrieve the search results asynchronously without blocking the client.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nSubmit an asynchronous search request (if logs doesn’t exist as an index then switch to an existing index).\n\nPOST /logs/_async_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"size\": 10000,\n  \"wait_for_completion_timeout\": \"30s\"\n}\nThis request will return an id and a response object containing partial results if available within the specified timeout.\n\nCheck the status of the asynchronous search using the id.\n\nGET /_async_search/status/{id}\n\nRetrieve the search results using the id.\n\nGET /_async_search/results/{id}\n\n\n\n\nIndex a large number of sample log documents in the “logs” index or use an index with a large number of documents.\nExecute the asynchronous search request and store the returned id.\nPeriodically check the status of the search using the id and the /_async_search/status/{id} endpoint.\n\nGET /_async_search/status/{id}\n\nOnce the search is complete, retrieve the final results using the id and the /_async_search/results/{id} endpoint.\n\nGET /_async_search/results/{id}\n\n\n\n\nThe _async_search endpoint is used to submit an asynchronous search request.\nThe wait_for_completion_timeout parameter specifies the maximum time to wait for partial results before returning.\nThe id returned by the initial request is used to check the status and retrieve the final results.\nAsynchronous search is useful for long-running searches on large datasets, as it doesn’t block the client while the search is being processed.\n\n\n\n\n\nIf you created an index for this example you might want to delete it.\n\nDELETE logs\n\n\n\n\nElasticsearch Async Search API\nSubmitting Async Search\nRetrieving Async Search Results\n\n\n\n\n\n\n\n\n\n\n\nCreate an index.\nIndex at least four documents using the _bulk endpoint.\nExecute metric and bucket aggregations.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\n\nEnsure you have access to Kibana or any REST client to execute the following requests.\n\nCreate an index with the following schema (needed for the aggregations to work properly).\n\nPUT product_prices\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\n\nIndex documents.\n\nPOST /product_prices/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\n\nExecute a simple aggregation (should return 2 buckets).\n\nGET product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      }\n    }\n  }\n}\n\nAdd and execute a single aggregation.\n\nGET product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n\nAdd two more aggregations and execute the query.\n\nGET /product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        },\n        \"max_price\": {\n          \"max\": {\n            \"field\": \"price\"\n          }\n        },\n        \"min_price\": {\n          \"min\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index creation.\n\nGET /product_prices\n\nVerify the documents have been indexed.\n\nGET /product_prices/_search\n\nExecute the aggregation query and verify the results.\n\nGET /product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        },\n        \"max_price\": {\n          \"max\": {\n            \"field\": \"price\"\n          }\n        },\n        \"min_price\": {\n          \"min\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n\nUsing the documents above you should see a response like this:\n\n{\n  \"took\": 2,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 4,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": null,\n    \"hits\": []\n  },\n  \"aggregations\": {\n    \"categories\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Apparel\",\n          \"doc_count\": 2,\n          \"average_price\": {\n            \"value\": 16.49\n          },\n          \"max_price\": {\n            \"value\": 19.99\n          },\n          \"min_price\": {\n            \"value\": 12.99\n          }\n        },\n        {\n          \"key\": \"Books\",\n          \"doc_count\": 2,\n          \"average_price\": {\n            \"value\": 34.99\n          },\n          \"max_price\": {\n            \"value\": 39.99\n          },\n          \"min_price\": {\n            \"value\": 29.99\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe category field must be of type keyword.\nThe terms aggregation creates buckets for each unique category.\nThe avg, max, and min sub-aggregations calculate the average, maximum, and minimum prices within each category bucket.\nSetting size to 0 ensures that only aggregation results are returned, not individual documents.\n\n\n\n\n\nDelete the index.\n\nDELETE product_prices\n\n\n\n\nElasticsearch: Aggregations\nElasticsearch: Terms Aggregation\nElasticsearch: Avg Aggregation\nElasticsearch: Max Aggregation\nElasticsearch: Min Aggregation\n\n\n\n\n\n\n\n\nCreate a new index with four documents representing website traffic data.\nUse the “sum” metric aggregation to calculate the total page views.\nUse the “terms” bucket aggregation to group traffic by country.\nUse the “avg” metric aggregation to calculate the average page views per country.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate a new index (in order to create a bucket using country it has to be of type keyword).\n\nPUT traffic\n{\n  \"mappings\": {\n    \"properties\": {\n      \"country\": {\n        \"type\": \"keyword\"\n      },\n      \"page_views\": {\n        \"type\": \"long\"\n      }\n    }\n  }\n}\n\nAdd four documents representing website traffic data.\n\nPOST /traffic/_bulk\n{\"index\":{}}\n{\"country\":\"USA\",\"page_views\":100}\n{\"index\":{}}\n{\"country\":\"USA\",\"page_views\":200}\n{\"index\":{}}\n{\"country\":\"Canada\",\"page_views\":50}\n{\"index\":{}}\n{\"country\":\"Canada\",\"page_views\":75}\n\nExecute the metric and bucket aggregations.\n\nGET /traffic/_search\n{\n  \"aggs\": {\n    \"total_page_views\": {\n      \"sum\": {\n        \"field\": \"page_views\"\n      }\n    },\n    \"traffic_by_country\": {\n      \"terms\": {\n        \"field\": \"country\"\n      },\n      \"aggs\": {\n        \"avg_page_views\": {\n          \"avg\": {\n            \"field\": \"page_views\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify that the total page views are calculated correctly (should be 425).\n\nGET /traffic/_search\n{\n  \"aggs\": {\n    \"total_page_views\": {\n      \"sum\": {\n        \"field\": \"page_views\"\n      }\n    }\n  }\n}\n\nVerify that the traffic is grouped correctly by country and average page views are calculated.\n\nGET /traffic/_search\n{\n  \"aggs\": {\n    \"traffic_by_country\": {\n      \"terms\": {\n        \"field\": \"country\"\n      },\n      \"aggs\": {\n        \"avg_page_views\": {\n          \"avg\": {\n            \"field\": \"page_views\"\n          }\n        }\n      }\n    }\n  }\n}\nResponse:\n{\n  \"took\": 1,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 4,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": null,\n    \"hits\": []\n  },\n  \"aggregations\": {\n    \"total_page_views\": {\n      \"value\": 425\n    },\n    \"traffic_by_country\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Canada\",\n          \"doc_count\": 2,\n          \"avg_page_views\": {\n            \"value\": 62.5\n          }\n        },\n        {\n          \"key\": \"USA\",\n          \"doc_count\": 2,\n          \"avg_page_views\": {\n            \"value\": 150\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe country field must be of type keyword.\nThe “sum” metric aggregation is used to calculate the total page views.\nThe “terms” bucket aggregation is used to group traffic by country.\nThe “avg” metric aggregation is used to calculate the average page views per country.\n\n\n\n\n\nDelete the index.\n\nDELETE traffic\n\n\n\n\nElasticsearch Aggregations\nMetric Aggregations\nBucket Aggregations\nTerms Aggregation\n\n\n\n\n\n\n\n\nAn Elasticsearch index named “employees” with documents containing fields like “name”, “department”, “position”, “salary”, “hire_date”, etc.\nCalculate the average salary across all employees.\nGroup the employees by department and calculate the maximum salary for each department.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate an index with the proper mapping for the department as we want to bucket by it.\n\nPUT employees\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"department\": {\n        \"type\": \"keyword\"\n      },\n      \"position\": {\n        \"type\": \"text\"\n      },\n      \"salary\": {\n        \"type\": \"integer\"\n      },\n      \"hire_date\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n\nIndex sample employee documents using the /_bulk endpoint.\n\nPOST /employees/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"John Doe\", \"department\":\"Engineering\", \"position\":\"Software Engineer\", \"salary\":80000, \"hire_date\":\"2018-01-15\"}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Jane Smith\", \"department\":\"Engineering\", \"position\":\"DevOps Engineer\", \"salary\":75000, \"hire_date\":\"2020-03-01\"}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Bob Johnson\", \"department\":\"Sales\", \"position\":\"Sales Manager\", \"salary\":90000, \"hire_date\":\"2016-06-01\"}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Alice Williams\", \"department\":\"Sales\", \"position\":\"Sales Representative\", \"salary\":65000, \"hire_date\":\"2019-09-15\"}\n\nDefine the aggregation query.\n\nGET employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"avg_salary_all_emps\": {\n      \"avg\": {\n        \"field\": \"salary\"\n      }\n    },\n    \"max_salary_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"max_salary\": {\n          \"max\": {\n            \"field\": \"salary\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nExecute the aggregation query, and it should return the following:\n\n{\n  \"took\": 1,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 4,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": null,\n    \"hits\": []\n  },\n  \"aggregations\": {\n    \"avg_salary_all_emps\": {\n      \"value\": 77500\n    },\n    \"max_salary_by_department\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Engineering\",\n          \"doc_count\": 2,\n          \"max_salary\": {\n            \"value\": 80000\n          }\n        },\n        {\n          \"key\": \"Sales\",\n          \"doc_count\": 2,\n          \"max_salary\": {\n            \"value\": 90000\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe department field must be of type keyword.\nThe size parameter is set to 0 to exclude hit documents from the response.\nThe avg_salary metric aggregation calculates the average of the “salary” field across all documents.\nThe max_salary_by_departments bucket aggregation groups the documents by the department field.\nInside the max_salary_by_departments aggregation, the max_salary metric sub-aggregation calculates the maximum value of the salary field for each department.\n\n\n\n\n\nDelete the index.\n\nDELETE employees\n\n\n\n\nElasticsearch Aggregations\nMetric Aggregations\nBucket Aggregations\nTerms Aggregation\n\n\n\n\n\n\n\n\n\n\n\nCreate an index.\nIndex at least four documents using the _bulk endpoint.\nExecute aggregations by category and sub-aggregations by price.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate an index.\n\nPUT /product_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\n\nIndex some sample documents.\n\nPOST /product_index/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\n\nExecute an aggregation by category.\n\nGET /product_index/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      }\n    }\n  }\n}\n\nAdd sub-aggregations by price.\n\nGET /product_index/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        },\n        \"price_ranges\": {\n          \"range\": {\n            \"field\": \"price\",\n            \"ranges\": [\n              { \"to\": 20 },\n              { \"from\": 20, \"to\": 40 },\n              { \"from\": 40 }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index creation and mappings.\n\nGET /product_index\n\nVerify the test documents are in the index.\n\nGET /product_index/_search\n\nExecute the aggregation query and confirm the results.\n\nGET /product_index/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        },\n        \"price_ranges\": {\n          \"range\": {\n            \"field\": \"price\",\n            \"ranges\": [\n              { \"to\": 20 },\n              { \"from\": 20, \"to\": 40 },\n              { \"from\": 40 }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\nResults\n{\n  \"aggregations\": {\n    \"categories\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Apparel\",\n          \"doc_count\": 2,\n          \"average_price\": {\n            \"value\": 16.49\n          },\n          \"price_ranges\": {\n            \"buckets\": [\n              {\n                \"key\": \"*-20.0\",\n                \"to\": 20,\n                \"doc_count\": 2\n              },\n              {\n                \"key\": \"20.0-40.0\",\n                \"from\": 20,\n                \"to\": 40,\n                \"doc_count\": 0\n              },\n              {\n                \"key\": \"40.0-*\",\n                \"from\": 40,\n                \"doc_count\": 0\n              }\n            ]\n          }\n        },\n        {\n          \"key\": \"Books\",\n          \"doc_count\": 2,\n          \"average_price\": {\n            \"value\": 34.99\n          },\n          \"price_ranges\": {\n            \"buckets\": [\n              {\n                \"key\": \"*-20.0\",\n                \"to\": 20,\n                \"doc_count\": 0\n              },\n              {\n                \"key\": \"20.0-40.0\",\n                \"from\": 20,\n                \"to\": 40,\n                \"doc_count\": 2\n              },\n              {\n                \"key\": \"40.0-*\",\n                \"from\": 40,\n                \"doc_count\": 0\n              }\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe category field must be of type keyword.\nSetting “size”: 0 ensures the search doesn’t return any documents, focusing solely on the aggregations.\nThe terms aggregation creates buckets for each unique category.\nThe avg sub-aggregation calculates the average price within each category bucket.\nThe range sub-aggregation divides the prices into specified ranges within each category bucket.\n\n\n\n\n\nDelete the index.\n\nDELETE product_index\n\n\n\n\nElasticsearch: Aggregations\nElasticsearch: Terms Aggregation\nElasticsearch: Avg Aggregation\nElasticsearch: Range Aggregation\n\n\n\n\n\n\n\n\nCreate a new index with four documents representing employee data.\nUse the terms aggregation to group employees by department.\nUse the avg sub-aggregation to calculate the average salary per department.\nUse the filters sub-aggregation to group employees by job_title.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate a new index called employees.\n\nPUT employees\n{\n  \"mappings\": {\n    \"properties\": {\n      \"department\": {\n        \"type\": \"keyword\"\n      },\n      \"salary\": {\n        \"type\": \"integer\"\n      },\n      \"job_title\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\nInsert four documents representing employee data.\n\nPOST /employees/_bulk\n{\"index\":{}}\n{\"department\":\"Sales\",\"salary\":100000,\"job_title\":\"Manager\"}\n{\"index\":{}}\n{\"department\":\"Sales\",\"salary\":80000,\"job_title\":\"Representative\"}\n{\"index\":{}}\n{\"department\":\"Marketing\",\"salary\":120000,\"job_title\":\"Manager\"}\n{\"index\":{}}\n{\"department\":\"Marketing\",\"salary\":90000,\"job_title\":\"Coordinator\"}\n\nExecute an aggregation by department.\n\nGET /employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      }\n    }\n  }\n}\n\nAdd the sub-aggregations for average salary.\n\nGET /employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"avg_salary\": {\n          \"avg\": {\n            \"field\": \"salary\"\n          }\n        }\n      }\n    }\n  }\n}\n\nAdd a filters sub-aggregation for job_title.\n\nGET /employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"avg_salary\": {\n          \"avg\": {\n            \"field\": \"salary\"\n          }\n        },\n        \"employees_by_job_title\": {\n          \"filters\": {\n            \"filters\": {\n              \"Manager\": {\n                \"term\": {\n                  \"job_title\": \"Manager\"\n                }\n              },\n              \"Representative\": {\n                \"term\": {\n                  \"job_title\": \"Representative\"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify that the employees are grouped correctly by department and job title.\n\nGET /employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"employees_by_job_title\": {\n          \"filters\": {\n            \"filters\": {\n              \"Manager\": {\n                \"term\": {\n                  \"job_title\": \"Manager\"\n                }\n              },\n              \"Representative\": {\n                \"term\": {\n                  \"job_title\": \"Representative\"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\nVerify that the average salary is calculated correctly for each department.\n\nGET /employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"avg_salary\": {\n          \"avg\": {\n            \"field\": \"salary\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nThe department field must be of type keyword.\nSetting “size”: 0 ensures the search doesn’t return any documents, focusing solely on the aggregations.\nThe terms aggregation is used to group employees by department.\nThe avg sub-aggregation is used to calculate the average salary per department.\nThe filters sub-aggregation is used to group employees by job_title.\n\n\n\n\n\nDelete the index.\n\nDELETE employees\n\n\n\n\nElasticsearch: Aggregations\nElasticsearch: Avg Aggregation\nElasticsearch: Filters Aggregation\nElasticsearch: Range Aggregation\nElasticsearch: Terms Aggregation\n\n\n\n\n\n\n\n\nAnalyze application logs stored in an Elasticsearch index named app-logs.\nUse a date histogram aggregation to group logs by the hour.\nWithin each hour bucket, create a sub-aggregation to group logs by their severity level (log_level).\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate a new index called app-logs.\n\nPUT app-logs\n{\n  \"mappings\": {\n    \"properties\": {\n      \"@timestamp\": {\n        \"type\": \"date\"\n      },\n      \"log_level\": {\n        \"type\": \"keyword\"\n      },\n      \"message\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n\nInsert sample data.\n\nPOST /app-logs/_bulk\n{\"index\":{},\"_id\":\"1\"}\n{\"@timestamp\":\"2024-05-24T10:30:00\",\"log_level\":\"INFO\",\"message\":\"Application started successfully.\"}\n{\"index\":{},\"_id\":\"2\"}\n{\"@timestamp\":\"2024-05-24T11:15:00\",\"log_level\":\"WARNING\",\"message\":\"Potential memory leak detected.\"}\n{\"index\":{},\"_id\":\"3\"}\n{\"@timestamp\":\"2024-05-24T12:00:00\",\"log_level\":\"ERROR\",\"message\":\"Database connection failed.\"}\n{\"index\":{},\"_id\":\"4\"}\n{\"@timestamp\":\"2024-05-24T10:45:00\",\"log_level\":\"DEBUG\",\"message\":\"Processing user request.\"}\n\nUse a date histogram aggregation to group logs by the hour.\n\nGET /app-logs/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"logs_by_hour\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"fixed_interval\": \"1h\"\n      }\n    }\n  }\n}\n\nWithin each hour bucket, create a sub-aggregation to group logs by their severity level (log_level).\n\nGET /app-logs/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"logs_by_hour\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"fixed_interval\": \"1h\"\n      },\n      \"aggs\": {\n        \"log_levels\": {\n          \"terms\": {\n            \"field\": \"log_level\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nRun the search query and examine the response.\n\n{\n  \"aggregations\": {\n    \"logs_by_hour\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2024-05-24T10:00:00.000Z\",\n          \"key\": 1716544800000,\n          \"doc_count\": 2,\n          \"log_levels\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n              {\n                \"key\": \"DEBUG\",\n                \"doc_count\": 1\n              },\n              {\n                \"key\": \"INFO\",\n                \"doc_count\": 1\n              }\n            ]\n          }\n        },\n        {\n          \"key_as_string\": \"2024-05-24T11:00:00.000Z\",\n          \"key\": 1716548400000,\n          \"doc_count\": 1,\n          \"log_levels\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n              {\n                \"key\": \"WARNING\",\n                \"doc_count\": 1\n              }\n            ]\n          }\n        },\n        {\n          \"key_as_string\": \"2024-05-24T12:00:00.000Z\",\n          \"key\": 1716552000000,\n          \"doc_count\": 1,\n          \"log_levels\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n              {\n                \"key\": \"ERROR\",\n                \"doc_count\": 1\n              }\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nSetting “size”: 0 ensures the search doesn’t return any documents, focusing solely on the aggregations.\nThe date_histogram aggregation groups documents based on the @timestamp field with an interval of one hour.\nThe nested terms aggregation within the logs_by_hour aggregation counts the occurrences of each unique log_level within each hour bucket.\n\n\n\n\n\nDelete the index.\n\nDELETE app-logs\n\n\n\n\nBucket Aggregations\nDate Histogram Aggregation\nTerms Aggregation\n\n\n\n\n\n\nIf you are running your instance of Elasticsearch locally, and need to create an additional cluster so that you can run these examples, go to the Appendix: Adding a Cluster to your Elasticsearch Instance for information on how to set up an additional single-node cluster.\n\n\n\n\n\nSet up two single-node clusters on localhost or Elastic Cloud.\nCreate an index in each cluster.\nIndex at least four documents in each cluster using the _bulk endpoint.\nConfigure cross-cluster search.\nExecute a cross-cluster search query.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nSet up multiple clusters on localhost.\n\n\nAssume you have two clusters, es01 and es02 and they have been set up as directed in the Appendix.\nIn the local cluster, configure communication between the clusters by updating the local cluster settings.\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n          \"seeds\": [\n            \"es01:9300\"\n          ],\n          \"skip_unavailable\": true\n        },\n        \"es02\": {\n          \"seeds\": [\n            \"es02:9300\"\n          ],\n          \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}\n\n\nCreate a product index in each cluster.\n\n\nFrom the Kibana Console (es01)\nPUT /products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\nFrom the command line (es02).\ncurl -u elastic:[your password here] -X PUT \"http://localhost:9201/products?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}'\n\n\nIndex product documents into each cluster.\n\n\nFor es01:\nPOST /products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\nFor es02 through the command line:\ncurl -u elastic:[your password here] -X POST \"http://localhost:9201/products/_bulk?pretty\" -H 'Content-Type: application/json' -d'\n{ \"index\": { \"_id\": \"5\" } }\n{ \"product\": \"Elasticsearch Stickers\", \"category\": \"Accessories\", \"price\": 4.99 }\n{ \"index\": { \"_id\": \"6\" } }\n{ \"product\": \"Elasticsearch Notebook\", \"category\": \"Stationery\", \"price\": 7.99 }\n{ \"index\": { \"_id\": \"7\" } }\n{ \"product\": \"Elasticsearch Pen\", \"category\": \"Stationery\", \"price\": 3.49 }\n{ \"index\": { \"_id\": \"8\" } }\n{ \"product\": \"Elasticsearch Hoodie\", \"category\": \"Apparel\", \"price\": 45.99 }    '\n\n\nConfigure Cross-Cluster Search (CCS).\n\n\nIn the local cluster, ensure the remote cluster is configured by checking the settings:\nGET /_cluster/settings?include_defaults=true&filter_path=defaults.cluster.remote\n\n\nExecute a Cross-Cluster Search query.\n\nGET /products,es02:products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"product\": \"Elasticsearch\"\n    }\n  }\n}\n\n\n\n\nVerify the index creation.\n\nGET /products\nFrom the command line execute:\ncurl -u elastic:[your password here] -X GET \"http://localhost:9201/products?pretty\"\n\nVerify that the documents have been indexed.\n\nGET /products/_search\nGET /es02:products/_search\n\nEnsure the remote cluster is correctly configured and visible from the local cluster.\n\nGET /_remote/info\n\nExecute a Cross-Cluster Search query.\n\nGET /products,es02:products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"product\": \"Elasticsearch\"\n    }\n  }\n}\n\n\n\n\nCross-cluster search is useful for querying data across multiple Elasticsearch clusters, providing a unified search experience.\nEnsure the remote cluster settings are correctly configured in the cluster settings.\nProperly handle the index names to avoid conflicts and ensure clear distinction between clusters.\n\n\n\n\n\nDelete the es01 index.\nDELETE products\nDelete the es02 index from the command line.\ncurl -u elastic:[your password here] -X DELETE \"http://localhost:9201/products?pretty\"\n\n\n\n\n\nElasticsearch: Cross-Cluster Search\nElasticsearch: Create Index API\nElasticsearch: Bulk API\nElasticsearch: Index Document API\n\n\n\n\n\n\n\n\n\n\n\nCreate an index.\nIndex four documents.\nDefine a runtime field.\nExecute a search query that utilizes the query-time runtime field.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate an index.\n\nPUT /product_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\nIndex some documents.\n\nPOST /product_index/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"price\": 29.99, \"category\": \"Books\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"price\": 39.99, \"category\": \"Books\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"price\": 19.99, \"category\": \"Apparel\" }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"price\": 12.99, \"category\": \"Apparel\" }\n\nDefine a query-time runtime field.\n\nGET /product_index/_search\n{\n  \"runtime_mappings\": {\n    \"discounted_price\": {\n      \"type\": \"double\",\n      \"script\": {\n        \"source\": \"emit(doc['price'].value * 0.9)\"\n      }\n    }\n  },\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\"price\", \"discounted_price\"]\n}\n\n\n\n\nVerify the creation of the index and its mappings.\n\nGET /product_index\n\nVerify the indexed documents.\n\nGET /product_index/_search\n\nDefine the runtime field, and execute the query.\n\nGET /product_index/_search\n{\n  \"runtime_mappings\": {\n    \"discounted_price\": {\n      \"type\": \"double\",\n      \"script\": {\n        \"source\": \"emit(doc['price'].value * 0.9)\"\n      }\n    }\n  },\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\"price\", \"discounted_price\"]\n}\n\n\n\n\nRuntime fields allow for dynamic calculation of field values at search time, useful for complex calculations or when the field values are not stored.\nThe script in the runtime field calculates the discounted price by applying a 10% discount to the price field.\n\n\n\n\n\nDelete the index.\n\nDELETE product_index\n\n\n\n\nElasticsearch: Runtime Fields\nElasticsearch: Create Index API\nElasticsearch: Bulk API\nElasticsearch: Index Document API\n\n\n\n\n\nIn this example, the runtime field is defined as part of the index that executes code when documents are indexed. The salary field is read at index time to create a new value for the runtime field total_salary.\n\n\n\nAn Elasticsearch index (employees) with documents containing employee information (name, department, salary).\nA runtime field (total_salary) to calculate the total salary of each employee.\nA search query to retrieve employees with a total salary above a certain threshold.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the employees index with a mapping for the runtime field.\n\nPUT employees\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"department\": {\n        \"type\": \"text\"\n      },\n      \"salary\": {\n        \"type\": \"float\"\n      },\n      \"total_salary\": {\n        \"type\": \"double\",\n        \"on_script_error\": \"fail\",\n        \"script\": {\n          \"source\": \"emit(doc['salary'].value * 1.1)\"\n        }\n      }\n    }\n  }\n}\n\nIndex some documents.\n\nPOST /employees/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"John Doe\", \"department\": \"Sales\", \"salary\": 50000 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Jane Smith\", \"department\": \"Marketing\", \"salary\": 60000 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"name\": \"Bob Johnson\", \"department\": \"IT\", \"salary\": 70000 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"name\": \"Alice Brown\", \"department\": \"HR\", \"salary\": 55000 }\n\nExecute a search query with a runtime field.\n\nGET /employees/_search\n{\n  \"query\": {\n    \"range\": {\n      \"total_salary\": {\n        \"gt\": 65000\n      }\n    }\n  }\n}\n\n\n\n\nVerify the creation of the index and its mappings.\n\nGET /employees\n\nVerify the indexed documents.\n\nGET /employees/_search\n\nExecute the query and verify the search results contain only employees with a total salary above 65000.\n\n\n\n\n\nRuntime fields are calculated on the fly and can be used in search queries, aggregations, and sorting.\nThe script used in the runtime field calculates the total salary by increasing the base salary by 10%.\n\n\n\n\n\nDelete the index.\n\nDELETE employees\n\n\n\n\nElasticsearch: Runtime Fields\nElasticsearch Script Fields\n\n\n\n\n\n\n\n\nSearch for restaurants based on their location (city and state).\nInclude the restaurant’s name, cuisine type, and a calculated rating score in the search results.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex some sample restaurant documents.\n\nPOST /restaurants/_bulk\n{ \"index\": { \"_id\": 1 } }\n{ \"name\": \"Tasty Bites\", \"city\": \"New York\", \"state\": \"NY\", \"cuisine\": \"Italian\", \"review_score\": 4.5, \"number_of_reviews\": 200 }\n{ \"index\": { \"_id\": 2 } }\n{ \"name\": \"Spicy Palace\", \"city\": \"Los Angeles\", \"state\": \"CA\", \"cuisine\": \"Indian\", \"review_score\": 4.2, \"number_of_reviews\": 150 }\n{ \"index\": { \"_id\": 3 } }\n{ \"name\": \"Sushi Spot\", \"city\": \"San Francisco\", \"state\": \"CA\", \"cuisine\": \"Japanese\", \"review_score\": 4.7, \"number_of_reviews\": 300 }\n{ \"index\": { \"_id\": 4 } }\n{ \"name\": \"Burger Joint\", \"city\": \"Chicago\", \"state\": \"IL\", \"cuisine\": \"American\", \"review_score\": 3.8, \"number_of_reviews\": 100 }\n\nDefine a runtime field named weighted_rating to calculate a weighted rating score for New York restaurants.\n\nGET /restaurants/_search\n{\n  \"runtime_mappings\": {\n    \"weighted_rating\": {\n      \"type\": \"double\",\n      \"script\": {\n        \"source\": \"emit(Math.sqrt(doc['review_score'].value * doc['number_of_reviews'].value))\"\n      }\n    }\n  },\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"city\": \"New York\"\n          }\n        },\n        {\n          \"match\": {\n            \"state\": \"NY\"\n          }\n        }\n      ]\n    }\n  },\n  \"fields\": [\n    \"name\",\n    \"cuisine\",\n    \"weighted_rating\"\n  ]\n}\n\n\n\n\nVerify the creation of the index and its mappings.\n\nGET /restaurants\n\nVerify the indexed documents.\n\nGET /restaurants/_search\n\nExecute the query and verify the restaurant name, cuisine type, and the calculated weighted rating score for restaurants located in New York, NY.\n\n\n\n\n\nThe runtime_mappings section defines a new field weighted_rating that calculates a weighted rating score based on the review_score and number_of_reviews fields.\nThe query section uses the match query to search for restaurants in New York, NY.\nThe fields section specifies the fields to include in the search results, including the runtime field weighted_rating.\n\n\n\n\n\nDelete the index.\n\nDELETE restaurants\n\n\n\n\nRuntime Fields\nMatch Query"
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-a-search-query-for-terms-andor-phrases-in-one-or-more-fields-of-an-index",
    "href": "2-searching-data.html#task-write-and-execute-a-search-query-for-terms-andor-phrases-in-one-or-more-fields-of-an-index",
    "title": "Searching Data",
    "section": "",
    "text": "The following section will have only one full example, but will show variations of term and phrase queries. Also, bear in mind that when they say term they may not mean the Elasticsearch use of the word, but rather the generic search use of the word. There are a lot of ways to execute a search in Elasticsearch. Don’t get bogged down; focus on term and phrase searches for this section of the example.\n\n\n\n\n\nCreate an index\nIndex some documents\nExecute a term query\nExecute a phrase query\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\n\nPOST /example_index/_bulk\n{ \"index\": {} }\n{ \"title\": \"The quick brown fox\", \"text\": \"The quick brown fox jumps over the lazy dog.\" }\n{ \"index\": {} }\n{ \"title\": \"Fast and curious\", \"text\": \"A fast and curious fox was seen leaping over a lazy dog.\" }\n{ \"index\": {} }\n{ \"title\": \"A fox in action\", \"text\": \"In a remarkable display of agility, a quick fox effortlessly jumped over a dog.\" }\n{ \"index\": {} }\n{ \"title\": \"Wildlife wonders\", \"text\": \"Observers were amazed as the quick brown fox jumped over the lazy dog.\" }\n{ \"index\": {} }\n{ \"title\": \"Fox tales\", \"text\": \"The tale of the quick fox that jumped over the lazy dog has become a legend.\" }\n\nExecute a term query\n\n\nUse the GET method to search for documents containing the term “Elasticsearch” in the title field using 3 different ways to search at the term level (there are 10 different ways currently. Refer to the Term-level Queries documentation for the full list).\nGET example_index/_search\n{\n  \"query\": {\n    \"term\": {\n      \"title\": {\n        \"value\": \"quick\"\n      }\n    }\n  }\n}\nGET example_index/_search\n{\n  \"query\": {\n    \"terms\": {\n      \"text\": [\"display\", \"amazed\"]\n    }\n  }\n}\nGET example_index/_search\n{\n  \"query\": {\n    \"terms_set\": {\n      \"text\": {\n        \"terms\": [\"quick\", \"over\", \"display\"],\n        \"minimum_should_match\": 3\n      }\n    }\n  }\n}\n\n\nExecute a phrase query\n\n\nreturns 2 docs\nGET /example_index/_search\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"text\": \"quick brown fox\"\n    }\n  }\n}\nreturns 1 doc\nGET /example_index/_search\n{\n  \"query\": {\n    \"match_phrase_prefix\": {\n      \"text\": \"fast and curi\"\n    }\n  }\n}\nreturns 1 doc\nGET /example_index/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"default_field\": \"text\",\n      \"query\": \"\\\"fox jumps\\\"\"\n    }\n  }\n}\n\n\n\n\n\nThe default standard analyzer (lowercasing, whitespace tokenization, basic normalization) is used.\nThe term query is used for exact matches and is not analyzed, meaning it matches the exact term in the inverted index.\nThe match_phrase query analyzes the input text and matches it as a phrase, making it useful for finding exact sequences of terms.\n\n\n\n\n\nVerify the various queries return the proper results.\n\n\n\n\n\nDelete the example index\nDELETE example_index\n\n\n\n\n\nFull Text Queries\nMatch Phrase Query\nMatch Phrase Prefix Query\nQuery DSL\nTerm-level Queries"
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-a-search-query-that-is-a-boolean-combination-of-multiple-queries-and-filters",
    "href": "2-searching-data.html#task-write-and-execute-a-search-query-that-is-a-boolean-combination-of-multiple-queries-and-filters",
    "title": "Searching Data",
    "section": "",
    "text": "Search for documents with a term in the “title”, “description”, and “category” field\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\n\nPOST /books/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"title\": \"To Kill a Mockingbird\", \"description\": \"A novel about the serious issues of rape and racial inequality.\", \"category\": \"Fiction\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"title\": \"1984\", \"description\": \"A novel that delves into the dangers of totalitarianism.\", \"category\": \"Dystopian\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"title\": \"The Great Gatsby\", \"description\": \"A critique of the American Dream.\", \"category\": \"Fiction\" }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"title\": \"Moby Dick\", \"description\": \"The quest of Ahab to exact revenge on the whale Moby Dick.\", \"category\": \"Adventure\" }\n{ \"index\": { \"_id\": \"5\" } }\n{ \"title\": \"Pride and Prejudice\", \"description\": \"A romantic novel that also critiques the British landed gentry at the end of the 18th century.\", \"category\": \"Romance\" }\n\nCreate a boolean search query. The order in which the various clauses are added don’t matter to the final result.\n\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {}\n  }\n}\n\nAdd a must query for the description field. This will return 4 documents.\n\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a filter query for the category field. This will return 2 documents.\n\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"fiction\"\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a must_not filter for the title field. This will return 1 document.\n\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"fiction\"\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"term\": {\n            \"title\": {\n              \"value\": \"gatsby\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\nExecute the final search query\n\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"fiction\"\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"term\": {\n            \"title\": {\n              \"value\": \"gatsby\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe bool query allows for combining multiple queries and filters with Boolean logic.\nThe must, must_not, and filter clauses ensure that all searches and filters must match for a document to be returned.\n\n\n\n\n\nVerify that the search query returns documents with the term “novel”, “dream”, and “critique” in the “description” field. Why are there no documents with the term “critique”?\n\n\n\n\n\nDelete the index\nDELETE books\n\n\n\n\n\nElasticsearch Boolean Query\nElasticsearch Match Query\nElasticsearch Range Query\nElasticsearch Term Query\n\n\n\n\n\n\n\n\nFind all documents where the name field exists (name: *) and the price field falls within a specified range.\nAdditionally, filter out any documents where the discontinued field is set to true.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\n\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Coffee Maker\",\"price\":49.99,\"discontinued\":false}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Gaming Laptop\",\"price\":1299.99,\"discontinued\":false}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Wireless Headphones\",\"price\":79.99,\"discontinued\":true}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Smartwatch\",\"price\":249.99,\"discontinued\":false}\n\nConstruct the search query\n\nGET products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 10,\n              \"lte\": 300\n            }\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"match\": {\n            \"discontinued\": true\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nSimilar to the previous example, the bool query combines multiple conditions.\nThe must clause specifies documents that must match all conditions within it.\nThe range query ensures the price field is between $10 (inclusive) and $300 (inclusive).\nThe must_not clause excludes documents that match the specified criteria.\nThe match query filters out documents where the discontinued field is set to true.\n\n\n\n\n\nRun the search query and verify the results only include documents for products with:\n\nA price between $10 and $300 (inclusive).\ndiscontinued set to true (not discontinued).\n\n\nThis should return documents with IDs 1 and 4 (Coffee Maker and Smartwatch) based on the sample data.\n\n\n\n\nThe chosen price range (gte: 10, lte: 300) can be adjusted based on your specific needs.\nYou can modify the match query for the name field to use more specific criteria if needed.\n\n\n\n\n\nDelete the index\n\nDELETE products\n\n\n\n\nElasticsearch Boolean Query\nElasticsearch Match Query\nElasticsearch Range Query\nElasticsearch Term Query\n\n\n\n\n\n\n\n\nSearch for products that belong to the “Electronics” category.\nThe product name should contain the term “phone”.\nExclude products with a price greater than 500.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\n\nPOST /products/_bulk\n{\"index\": { \"_id\": 1 } }\n{ \"name\": \"Smartphone X\", \"category\": \"Electronics\", \"price\": 399.99 }\n{\"index\": { \"_id\": 2 } }\n{ \"name\": \"Laptop Y\", \"category\": \"Electronics\", \"price\": 799.99 }\n{\"index\": { \"_id\": 3 } }\n{ \"name\": \"Headphones Z\", \"category\": \"Electronics\", \"price\": 99.99 }\n{\"index\": { \"_id\": 4 } }\n{ \"name\": \"Gaming Console\", \"category\": \"Electronics\", \"price\": 299.99 }\n\nStart with a boolean query that only matches the category “electronics”. This returns 4 documents.\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a should clause for the word “phone”. This still returns 4 documents (why?).\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ],\n      \"should\": [\n        {\n          \"match\": {\n            \"name\": \"phone\"\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a must_not clause for the any price greater than $300. This still returns 2 documents (why?).\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ],\n      \"should\": [\n        {\n          \"match\": {\n            \"name\": \"phone\"\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"range\": {\n            \"price\": {\n              \"gt\": 300\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe search results should include the following documents:\n\nHeadphones Z\nGaming Console\n\n\n\n\n\n\nThe term query is used for exact matches on the category field.\nThe match query is used for matches on the name field.\nThe range query is used to filter out documents based on the price field.\nThe bool query combines these conditions using the specified occurrence types.\n\n\n\n\n\nDelete the index\n\nDELETE products\n\n\n\n\nElasticsearch Boolean Query\nElasticsearch Match Query\nElasticsearch Range Query\nElasticsearch Term Query\n\n\n\n\n\n\n\n\nCreate an index named “products”.\nCreate at least 4 documents with varying categories, prices, ratings, and brands.\nUse the “must”, “should”, “must_not”, and “filter” clauses in the Boolean query.\nThe query should return products that match the specified criteria and are from a specific brand.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the “products” index and add some sample documents using the /_bulk endpoint.\n\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Laptop\",\"category\":\"Electronics\",\"price\":1200,\"rating\":4.5,\"brand\":\"Apple\"}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Smartphone\",\"category\":\"Electronics\",\"price\":800,\"rating\":4.2,\"brand\":\"Samsung\"}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Sofa\",\"category\":\"Furniture\",\"price\":1000,\"rating\":3.8,\"brand\":\"IKEA\"}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Headphones\",\"category\":\"Electronics\",\"price\":150,\"rating\":2.5,\"brand\":\"Sony\"}\n{\"index\":{\"_id\":5}}\n{\"name\":\"Dining Table\",\"category\":\"Furniture\",\"price\":600,\"rating\":4.1,\"brand\":\"Ashley\"}\n\nStart with a boolean query that only matches the category “electronics”. This returns 3 documents.\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a must clause to only return products whose price is greater than $500. This should return 4 documents (why?).\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ],\n      \"must\": [\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 500\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd a must_not clause to exclude items with a rating less than 4. This will return 3 documents.\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ],\n      \"must\": [\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 500\n            }\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"range\": {\n            \"rating\": {\n              \"lt\": 4\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\nAdd the final filter to only return Apple products. This should return 1 document.\n\nGET /products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ],\n      \"must\": [\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 500\n            }\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"range\": {\n            \"rating\": {\n              \"lt\": 4\n            }\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"brand\": \"apple\"\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nCheck the response from the search query to ensure that it returns the expected documents (products in the “Electronics” category or with a price greater than $500, but excluding products with a rating less than 4, and from the brand “Apple”).\n\n\n\n\n\nThe filter clause is used to include only documents with the brand “Apple”.\n\n\n\n\n\nDelete the index\n\nDELETE products\n\n\n\n\nElasticsearch Boolean Query\nElasticsearch Term Query\nElasticsearch Range Query"
  },
  {
    "objectID": "2-searching-data.html#task-create-an-asynchronous-search",
    "href": "2-searching-data.html#task-create-an-asynchronous-search",
    "title": "Searching Data",
    "section": "",
    "text": "Asynchronous search uses the same parameters as regular search with a few extra features listed in their entirety in the document listed below. For example, in the solution below the size option is from the Search API. There is only one example here as you can look up the other options as needed during the exam.\n\n\n\n\n\nAn Elasticsearch index named “logs” with a large number of documents (e.g., millions of log entries).\nPerform a search on the “logs” index that may take a long time to complete due to the size of the index.\nRetrieve the search results asynchronously without blocking the client.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nSubmit an asynchronous search request (if logs doesn’t exist as an index then switch to an existing index).\n\nPOST /logs/_async_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"size\": 10000,\n  \"wait_for_completion_timeout\": \"30s\"\n}\nThis request will return an id and a response object containing partial results if available within the specified timeout.\n\nCheck the status of the asynchronous search using the id.\n\nGET /_async_search/status/{id}\n\nRetrieve the search results using the id.\n\nGET /_async_search/results/{id}\n\n\n\n\nIndex a large number of sample log documents in the “logs” index or use an index with a large number of documents.\nExecute the asynchronous search request and store the returned id.\nPeriodically check the status of the search using the id and the /_async_search/status/{id} endpoint.\n\nGET /_async_search/status/{id}\n\nOnce the search is complete, retrieve the final results using the id and the /_async_search/results/{id} endpoint.\n\nGET /_async_search/results/{id}\n\n\n\n\nThe _async_search endpoint is used to submit an asynchronous search request.\nThe wait_for_completion_timeout parameter specifies the maximum time to wait for partial results before returning.\nThe id returned by the initial request is used to check the status and retrieve the final results.\nAsynchronous search is useful for long-running searches on large datasets, as it doesn’t block the client while the search is being processed.\n\n\n\n\n\nIf you created an index for this example you might want to delete it.\n\nDELETE logs\n\n\n\n\nElasticsearch Async Search API\nSubmitting Async Search\nRetrieving Async Search Results"
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-metric-and-bucket-aggregations",
    "href": "2-searching-data.html#task-write-and-execute-metric-and-bucket-aggregations",
    "title": "Searching Data",
    "section": "",
    "text": "Create an index.\nIndex at least four documents using the _bulk endpoint.\nExecute metric and bucket aggregations.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\n\nEnsure you have access to Kibana or any REST client to execute the following requests.\n\nCreate an index with the following schema (needed for the aggregations to work properly).\n\nPUT product_prices\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\n\nIndex documents.\n\nPOST /product_prices/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\n\nExecute a simple aggregation (should return 2 buckets).\n\nGET product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      }\n    }\n  }\n}\n\nAdd and execute a single aggregation.\n\nGET product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n\nAdd two more aggregations and execute the query.\n\nGET /product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        },\n        \"max_price\": {\n          \"max\": {\n            \"field\": \"price\"\n          }\n        },\n        \"min_price\": {\n          \"min\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index creation.\n\nGET /product_prices\n\nVerify the documents have been indexed.\n\nGET /product_prices/_search\n\nExecute the aggregation query and verify the results.\n\nGET /product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        },\n        \"max_price\": {\n          \"max\": {\n            \"field\": \"price\"\n          }\n        },\n        \"min_price\": {\n          \"min\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n\nUsing the documents above you should see a response like this:\n\n{\n  \"took\": 2,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 4,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": null,\n    \"hits\": []\n  },\n  \"aggregations\": {\n    \"categories\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Apparel\",\n          \"doc_count\": 2,\n          \"average_price\": {\n            \"value\": 16.49\n          },\n          \"max_price\": {\n            \"value\": 19.99\n          },\n          \"min_price\": {\n            \"value\": 12.99\n          }\n        },\n        {\n          \"key\": \"Books\",\n          \"doc_count\": 2,\n          \"average_price\": {\n            \"value\": 34.99\n          },\n          \"max_price\": {\n            \"value\": 39.99\n          },\n          \"min_price\": {\n            \"value\": 29.99\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe category field must be of type keyword.\nThe terms aggregation creates buckets for each unique category.\nThe avg, max, and min sub-aggregations calculate the average, maximum, and minimum prices within each category bucket.\nSetting size to 0 ensures that only aggregation results are returned, not individual documents.\n\n\n\n\n\nDelete the index.\n\nDELETE product_prices\n\n\n\n\nElasticsearch: Aggregations\nElasticsearch: Terms Aggregation\nElasticsearch: Avg Aggregation\nElasticsearch: Max Aggregation\nElasticsearch: Min Aggregation\n\n\n\n\n\n\n\n\nCreate a new index with four documents representing website traffic data.\nUse the “sum” metric aggregation to calculate the total page views.\nUse the “terms” bucket aggregation to group traffic by country.\nUse the “avg” metric aggregation to calculate the average page views per country.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate a new index (in order to create a bucket using country it has to be of type keyword).\n\nPUT traffic\n{\n  \"mappings\": {\n    \"properties\": {\n      \"country\": {\n        \"type\": \"keyword\"\n      },\n      \"page_views\": {\n        \"type\": \"long\"\n      }\n    }\n  }\n}\n\nAdd four documents representing website traffic data.\n\nPOST /traffic/_bulk\n{\"index\":{}}\n{\"country\":\"USA\",\"page_views\":100}\n{\"index\":{}}\n{\"country\":\"USA\",\"page_views\":200}\n{\"index\":{}}\n{\"country\":\"Canada\",\"page_views\":50}\n{\"index\":{}}\n{\"country\":\"Canada\",\"page_views\":75}\n\nExecute the metric and bucket aggregations.\n\nGET /traffic/_search\n{\n  \"aggs\": {\n    \"total_page_views\": {\n      \"sum\": {\n        \"field\": \"page_views\"\n      }\n    },\n    \"traffic_by_country\": {\n      \"terms\": {\n        \"field\": \"country\"\n      },\n      \"aggs\": {\n        \"avg_page_views\": {\n          \"avg\": {\n            \"field\": \"page_views\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify that the total page views are calculated correctly (should be 425).\n\nGET /traffic/_search\n{\n  \"aggs\": {\n    \"total_page_views\": {\n      \"sum\": {\n        \"field\": \"page_views\"\n      }\n    }\n  }\n}\n\nVerify that the traffic is grouped correctly by country and average page views are calculated.\n\nGET /traffic/_search\n{\n  \"aggs\": {\n    \"traffic_by_country\": {\n      \"terms\": {\n        \"field\": \"country\"\n      },\n      \"aggs\": {\n        \"avg_page_views\": {\n          \"avg\": {\n            \"field\": \"page_views\"\n          }\n        }\n      }\n    }\n  }\n}\nResponse:\n{\n  \"took\": 1,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 4,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": null,\n    \"hits\": []\n  },\n  \"aggregations\": {\n    \"total_page_views\": {\n      \"value\": 425\n    },\n    \"traffic_by_country\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Canada\",\n          \"doc_count\": 2,\n          \"avg_page_views\": {\n            \"value\": 62.5\n          }\n        },\n        {\n          \"key\": \"USA\",\n          \"doc_count\": 2,\n          \"avg_page_views\": {\n            \"value\": 150\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe country field must be of type keyword.\nThe “sum” metric aggregation is used to calculate the total page views.\nThe “terms” bucket aggregation is used to group traffic by country.\nThe “avg” metric aggregation is used to calculate the average page views per country.\n\n\n\n\n\nDelete the index.\n\nDELETE traffic\n\n\n\n\nElasticsearch Aggregations\nMetric Aggregations\nBucket Aggregations\nTerms Aggregation\n\n\n\n\n\n\n\n\nAn Elasticsearch index named “employees” with documents containing fields like “name”, “department”, “position”, “salary”, “hire_date”, etc.\nCalculate the average salary across all employees.\nGroup the employees by department and calculate the maximum salary for each department.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate an index with the proper mapping for the department as we want to bucket by it.\n\nPUT employees\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"department\": {\n        \"type\": \"keyword\"\n      },\n      \"position\": {\n        \"type\": \"text\"\n      },\n      \"salary\": {\n        \"type\": \"integer\"\n      },\n      \"hire_date\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n\nIndex sample employee documents using the /_bulk endpoint.\n\nPOST /employees/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"John Doe\", \"department\":\"Engineering\", \"position\":\"Software Engineer\", \"salary\":80000, \"hire_date\":\"2018-01-15\"}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Jane Smith\", \"department\":\"Engineering\", \"position\":\"DevOps Engineer\", \"salary\":75000, \"hire_date\":\"2020-03-01\"}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Bob Johnson\", \"department\":\"Sales\", \"position\":\"Sales Manager\", \"salary\":90000, \"hire_date\":\"2016-06-01\"}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Alice Williams\", \"department\":\"Sales\", \"position\":\"Sales Representative\", \"salary\":65000, \"hire_date\":\"2019-09-15\"}\n\nDefine the aggregation query.\n\nGET employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"avg_salary_all_emps\": {\n      \"avg\": {\n        \"field\": \"salary\"\n      }\n    },\n    \"max_salary_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"max_salary\": {\n          \"max\": {\n            \"field\": \"salary\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nExecute the aggregation query, and it should return the following:\n\n{\n  \"took\": 1,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 4,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": null,\n    \"hits\": []\n  },\n  \"aggregations\": {\n    \"avg_salary_all_emps\": {\n      \"value\": 77500\n    },\n    \"max_salary_by_department\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Engineering\",\n          \"doc_count\": 2,\n          \"max_salary\": {\n            \"value\": 80000\n          }\n        },\n        {\n          \"key\": \"Sales\",\n          \"doc_count\": 2,\n          \"max_salary\": {\n            \"value\": 90000\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe department field must be of type keyword.\nThe size parameter is set to 0 to exclude hit documents from the response.\nThe avg_salary metric aggregation calculates the average of the “salary” field across all documents.\nThe max_salary_by_departments bucket aggregation groups the documents by the department field.\nInside the max_salary_by_departments aggregation, the max_salary metric sub-aggregation calculates the maximum value of the salary field for each department.\n\n\n\n\n\nDelete the index.\n\nDELETE employees\n\n\n\n\nElasticsearch Aggregations\nMetric Aggregations\nBucket Aggregations\nTerms Aggregation"
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-aggregations-that-contain-subaggregations",
    "href": "2-searching-data.html#task-write-and-execute-aggregations-that-contain-subaggregations",
    "title": "Searching Data",
    "section": "",
    "text": "Create an index.\nIndex at least four documents using the _bulk endpoint.\nExecute aggregations by category and sub-aggregations by price.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate an index.\n\nPUT /product_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\n\nIndex some sample documents.\n\nPOST /product_index/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\n\nExecute an aggregation by category.\n\nGET /product_index/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      }\n    }\n  }\n}\n\nAdd sub-aggregations by price.\n\nGET /product_index/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        },\n        \"price_ranges\": {\n          \"range\": {\n            \"field\": \"price\",\n            \"ranges\": [\n              { \"to\": 20 },\n              { \"from\": 20, \"to\": 40 },\n              { \"from\": 40 }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify the index creation and mappings.\n\nGET /product_index\n\nVerify the test documents are in the index.\n\nGET /product_index/_search\n\nExecute the aggregation query and confirm the results.\n\nGET /product_index/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"categories\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        },\n        \"price_ranges\": {\n          \"range\": {\n            \"field\": \"price\",\n            \"ranges\": [\n              { \"to\": 20 },\n              { \"from\": 20, \"to\": 40 },\n              { \"from\": 40 }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\nResults\n{\n  \"aggregations\": {\n    \"categories\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Apparel\",\n          \"doc_count\": 2,\n          \"average_price\": {\n            \"value\": 16.49\n          },\n          \"price_ranges\": {\n            \"buckets\": [\n              {\n                \"key\": \"*-20.0\",\n                \"to\": 20,\n                \"doc_count\": 2\n              },\n              {\n                \"key\": \"20.0-40.0\",\n                \"from\": 20,\n                \"to\": 40,\n                \"doc_count\": 0\n              },\n              {\n                \"key\": \"40.0-*\",\n                \"from\": 40,\n                \"doc_count\": 0\n              }\n            ]\n          }\n        },\n        {\n          \"key\": \"Books\",\n          \"doc_count\": 2,\n          \"average_price\": {\n            \"value\": 34.99\n          },\n          \"price_ranges\": {\n            \"buckets\": [\n              {\n                \"key\": \"*-20.0\",\n                \"to\": 20,\n                \"doc_count\": 0\n              },\n              {\n                \"key\": \"20.0-40.0\",\n                \"from\": 20,\n                \"to\": 40,\n                \"doc_count\": 2\n              },\n              {\n                \"key\": \"40.0-*\",\n                \"from\": 40,\n                \"doc_count\": 0\n              }\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nThe category field must be of type keyword.\nSetting “size”: 0 ensures the search doesn’t return any documents, focusing solely on the aggregations.\nThe terms aggregation creates buckets for each unique category.\nThe avg sub-aggregation calculates the average price within each category bucket.\nThe range sub-aggregation divides the prices into specified ranges within each category bucket.\n\n\n\n\n\nDelete the index.\n\nDELETE product_index\n\n\n\n\nElasticsearch: Aggregations\nElasticsearch: Terms Aggregation\nElasticsearch: Avg Aggregation\nElasticsearch: Range Aggregation\n\n\n\n\n\n\n\n\nCreate a new index with four documents representing employee data.\nUse the terms aggregation to group employees by department.\nUse the avg sub-aggregation to calculate the average salary per department.\nUse the filters sub-aggregation to group employees by job_title.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate a new index called employees.\n\nPUT employees\n{\n  \"mappings\": {\n    \"properties\": {\n      \"department\": {\n        \"type\": \"keyword\"\n      },\n      \"salary\": {\n        \"type\": \"integer\"\n      },\n      \"job_title\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\nInsert four documents representing employee data.\n\nPOST /employees/_bulk\n{\"index\":{}}\n{\"department\":\"Sales\",\"salary\":100000,\"job_title\":\"Manager\"}\n{\"index\":{}}\n{\"department\":\"Sales\",\"salary\":80000,\"job_title\":\"Representative\"}\n{\"index\":{}}\n{\"department\":\"Marketing\",\"salary\":120000,\"job_title\":\"Manager\"}\n{\"index\":{}}\n{\"department\":\"Marketing\",\"salary\":90000,\"job_title\":\"Coordinator\"}\n\nExecute an aggregation by department.\n\nGET /employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      }\n    }\n  }\n}\n\nAdd the sub-aggregations for average salary.\n\nGET /employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"avg_salary\": {\n          \"avg\": {\n            \"field\": \"salary\"\n          }\n        }\n      }\n    }\n  }\n}\n\nAdd a filters sub-aggregation for job_title.\n\nGET /employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"avg_salary\": {\n          \"avg\": {\n            \"field\": \"salary\"\n          }\n        },\n        \"employees_by_job_title\": {\n          \"filters\": {\n            \"filters\": {\n              \"Manager\": {\n                \"term\": {\n                  \"job_title\": \"Manager\"\n                }\n              },\n              \"Representative\": {\n                \"term\": {\n                  \"job_title\": \"Representative\"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nVerify that the employees are grouped correctly by department and job title.\n\nGET /employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"employees_by_job_title\": {\n          \"filters\": {\n            \"filters\": {\n              \"Manager\": {\n                \"term\": {\n                  \"job_title\": \"Manager\"\n                }\n              },\n              \"Representative\": {\n                \"term\": {\n                  \"job_title\": \"Representative\"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\nVerify that the average salary is calculated correctly for each department.\n\nGET /employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"avg_salary\": {\n          \"avg\": {\n            \"field\": \"salary\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nThe department field must be of type keyword.\nSetting “size”: 0 ensures the search doesn’t return any documents, focusing solely on the aggregations.\nThe terms aggregation is used to group employees by department.\nThe avg sub-aggregation is used to calculate the average salary per department.\nThe filters sub-aggregation is used to group employees by job_title.\n\n\n\n\n\nDelete the index.\n\nDELETE employees\n\n\n\n\nElasticsearch: Aggregations\nElasticsearch: Avg Aggregation\nElasticsearch: Filters Aggregation\nElasticsearch: Range Aggregation\nElasticsearch: Terms Aggregation\n\n\n\n\n\n\n\n\nAnalyze application logs stored in an Elasticsearch index named app-logs.\nUse a date histogram aggregation to group logs by the hour.\nWithin each hour bucket, create a sub-aggregation to group logs by their severity level (log_level).\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate a new index called app-logs.\n\nPUT app-logs\n{\n  \"mappings\": {\n    \"properties\": {\n      \"@timestamp\": {\n        \"type\": \"date\"\n      },\n      \"log_level\": {\n        \"type\": \"keyword\"\n      },\n      \"message\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n\nInsert sample data.\n\nPOST /app-logs/_bulk\n{\"index\":{},\"_id\":\"1\"}\n{\"@timestamp\":\"2024-05-24T10:30:00\",\"log_level\":\"INFO\",\"message\":\"Application started successfully.\"}\n{\"index\":{},\"_id\":\"2\"}\n{\"@timestamp\":\"2024-05-24T11:15:00\",\"log_level\":\"WARNING\",\"message\":\"Potential memory leak detected.\"}\n{\"index\":{},\"_id\":\"3\"}\n{\"@timestamp\":\"2024-05-24T12:00:00\",\"log_level\":\"ERROR\",\"message\":\"Database connection failed.\"}\n{\"index\":{},\"_id\":\"4\"}\n{\"@timestamp\":\"2024-05-24T10:45:00\",\"log_level\":\"DEBUG\",\"message\":\"Processing user request.\"}\n\nUse a date histogram aggregation to group logs by the hour.\n\nGET /app-logs/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"logs_by_hour\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"fixed_interval\": \"1h\"\n      }\n    }\n  }\n}\n\nWithin each hour bucket, create a sub-aggregation to group logs by their severity level (log_level).\n\nGET /app-logs/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"logs_by_hour\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"fixed_interval\": \"1h\"\n      },\n      \"aggs\": {\n        \"log_levels\": {\n          \"terms\": {\n            \"field\": \"log_level\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nRun the search query and examine the response.\n\n{\n  \"aggregations\": {\n    \"logs_by_hour\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2024-05-24T10:00:00.000Z\",\n          \"key\": 1716544800000,\n          \"doc_count\": 2,\n          \"log_levels\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n              {\n                \"key\": \"DEBUG\",\n                \"doc_count\": 1\n              },\n              {\n                \"key\": \"INFO\",\n                \"doc_count\": 1\n              }\n            ]\n          }\n        },\n        {\n          \"key_as_string\": \"2024-05-24T11:00:00.000Z\",\n          \"key\": 1716548400000,\n          \"doc_count\": 1,\n          \"log_levels\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n              {\n                \"key\": \"WARNING\",\n                \"doc_count\": 1\n              }\n            ]\n          }\n        },\n        {\n          \"key_as_string\": \"2024-05-24T12:00:00.000Z\",\n          \"key\": 1716552000000,\n          \"doc_count\": 1,\n          \"log_levels\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n              {\n                \"key\": \"ERROR\",\n                \"doc_count\": 1\n              }\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nSetting “size”: 0 ensures the search doesn’t return any documents, focusing solely on the aggregations.\nThe date_histogram aggregation groups documents based on the @timestamp field with an interval of one hour.\nThe nested terms aggregation within the logs_by_hour aggregation counts the occurrences of each unique log_level within each hour bucket.\n\n\n\n\n\nDelete the index.\n\nDELETE app-logs\n\n\n\n\nBucket Aggregations\nDate Histogram Aggregation\nTerms Aggregation"
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-a-query-that-searches-across-multiple-clusters",
    "href": "2-searching-data.html#task-write-and-execute-a-query-that-searches-across-multiple-clusters",
    "title": "Searching Data",
    "section": "",
    "text": "If you are running your instance of Elasticsearch locally, and need to create an additional cluster so that you can run these examples, go to the Appendix: Adding a Cluster to your Elasticsearch Instance for information on how to set up an additional single-node cluster.\n\n\n\n\n\nSet up two single-node clusters on localhost or Elastic Cloud.\nCreate an index in each cluster.\nIndex at least four documents in each cluster using the _bulk endpoint.\nConfigure cross-cluster search.\nExecute a cross-cluster search query.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nSet up multiple clusters on localhost.\n\n\nAssume you have two clusters, es01 and es02 and they have been set up as directed in the Appendix.\nIn the local cluster, configure communication between the clusters by updating the local cluster settings.\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n          \"seeds\": [\n            \"es01:9300\"\n          ],\n          \"skip_unavailable\": true\n        },\n        \"es02\": {\n          \"seeds\": [\n            \"es02:9300\"\n          ],\n          \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}\n\n\nCreate a product index in each cluster.\n\n\nFrom the Kibana Console (es01)\nPUT /products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\nFrom the command line (es02).\ncurl -u elastic:[your password here] -X PUT \"http://localhost:9201/products?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}'\n\n\nIndex product documents into each cluster.\n\n\nFor es01:\nPOST /products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\nFor es02 through the command line:\ncurl -u elastic:[your password here] -X POST \"http://localhost:9201/products/_bulk?pretty\" -H 'Content-Type: application/json' -d'\n{ \"index\": { \"_id\": \"5\" } }\n{ \"product\": \"Elasticsearch Stickers\", \"category\": \"Accessories\", \"price\": 4.99 }\n{ \"index\": { \"_id\": \"6\" } }\n{ \"product\": \"Elasticsearch Notebook\", \"category\": \"Stationery\", \"price\": 7.99 }\n{ \"index\": { \"_id\": \"7\" } }\n{ \"product\": \"Elasticsearch Pen\", \"category\": \"Stationery\", \"price\": 3.49 }\n{ \"index\": { \"_id\": \"8\" } }\n{ \"product\": \"Elasticsearch Hoodie\", \"category\": \"Apparel\", \"price\": 45.99 }    '\n\n\nConfigure Cross-Cluster Search (CCS).\n\n\nIn the local cluster, ensure the remote cluster is configured by checking the settings:\nGET /_cluster/settings?include_defaults=true&filter_path=defaults.cluster.remote\n\n\nExecute a Cross-Cluster Search query.\n\nGET /products,es02:products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"product\": \"Elasticsearch\"\n    }\n  }\n}\n\n\n\n\nVerify the index creation.\n\nGET /products\nFrom the command line execute:\ncurl -u elastic:[your password here] -X GET \"http://localhost:9201/products?pretty\"\n\nVerify that the documents have been indexed.\n\nGET /products/_search\nGET /es02:products/_search\n\nEnsure the remote cluster is correctly configured and visible from the local cluster.\n\nGET /_remote/info\n\nExecute a Cross-Cluster Search query.\n\nGET /products,es02:products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"product\": \"Elasticsearch\"\n    }\n  }\n}\n\n\n\n\nCross-cluster search is useful for querying data across multiple Elasticsearch clusters, providing a unified search experience.\nEnsure the remote cluster settings are correctly configured in the cluster settings.\nProperly handle the index names to avoid conflicts and ensure clear distinction between clusters.\n\n\n\n\n\nDelete the es01 index.\nDELETE products\nDelete the es02 index from the command line.\ncurl -u elastic:[your password here] -X DELETE \"http://localhost:9201/products?pretty\"\n\n\n\n\n\nElasticsearch: Cross-Cluster Search\nElasticsearch: Create Index API\nElasticsearch: Bulk API\nElasticsearch: Index Document API"
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-a-search-that-utilizes-a-runtime-field",
    "href": "2-searching-data.html#task-write-and-execute-a-search-that-utilizes-a-runtime-field",
    "title": "Searching Data",
    "section": "",
    "text": "Create an index.\nIndex four documents.\nDefine a runtime field.\nExecute a search query that utilizes the query-time runtime field.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate an index.\n\nPUT /product_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\nIndex some documents.\n\nPOST /product_index/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"price\": 29.99, \"category\": \"Books\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"price\": 39.99, \"category\": \"Books\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"price\": 19.99, \"category\": \"Apparel\" }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"price\": 12.99, \"category\": \"Apparel\" }\n\nDefine a query-time runtime field.\n\nGET /product_index/_search\n{\n  \"runtime_mappings\": {\n    \"discounted_price\": {\n      \"type\": \"double\",\n      \"script\": {\n        \"source\": \"emit(doc['price'].value * 0.9)\"\n      }\n    }\n  },\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\"price\", \"discounted_price\"]\n}\n\n\n\n\nVerify the creation of the index and its mappings.\n\nGET /product_index\n\nVerify the indexed documents.\n\nGET /product_index/_search\n\nDefine the runtime field, and execute the query.\n\nGET /product_index/_search\n{\n  \"runtime_mappings\": {\n    \"discounted_price\": {\n      \"type\": \"double\",\n      \"script\": {\n        \"source\": \"emit(doc['price'].value * 0.9)\"\n      }\n    }\n  },\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\"price\", \"discounted_price\"]\n}\n\n\n\n\nRuntime fields allow for dynamic calculation of field values at search time, useful for complex calculations or when the field values are not stored.\nThe script in the runtime field calculates the discounted price by applying a 10% discount to the price field.\n\n\n\n\n\nDelete the index.\n\nDELETE product_index\n\n\n\n\nElasticsearch: Runtime Fields\nElasticsearch: Create Index API\nElasticsearch: Bulk API\nElasticsearch: Index Document API\n\n\n\n\n\nIn this example, the runtime field is defined as part of the index that executes code when documents are indexed. The salary field is read at index time to create a new value for the runtime field total_salary.\n\n\n\nAn Elasticsearch index (employees) with documents containing employee information (name, department, salary).\nA runtime field (total_salary) to calculate the total salary of each employee.\nA search query to retrieve employees with a total salary above a certain threshold.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nCreate the employees index with a mapping for the runtime field.\n\nPUT employees\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"department\": {\n        \"type\": \"text\"\n      },\n      \"salary\": {\n        \"type\": \"float\"\n      },\n      \"total_salary\": {\n        \"type\": \"double\",\n        \"on_script_error\": \"fail\",\n        \"script\": {\n          \"source\": \"emit(doc['salary'].value * 1.1)\"\n        }\n      }\n    }\n  }\n}\n\nIndex some documents.\n\nPOST /employees/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"John Doe\", \"department\": \"Sales\", \"salary\": 50000 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Jane Smith\", \"department\": \"Marketing\", \"salary\": 60000 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"name\": \"Bob Johnson\", \"department\": \"IT\", \"salary\": 70000 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"name\": \"Alice Brown\", \"department\": \"HR\", \"salary\": 55000 }\n\nExecute a search query with a runtime field.\n\nGET /employees/_search\n{\n  \"query\": {\n    \"range\": {\n      \"total_salary\": {\n        \"gt\": 65000\n      }\n    }\n  }\n}\n\n\n\n\nVerify the creation of the index and its mappings.\n\nGET /employees\n\nVerify the indexed documents.\n\nGET /employees/_search\n\nExecute the query and verify the search results contain only employees with a total salary above 65000.\n\n\n\n\n\nRuntime fields are calculated on the fly and can be used in search queries, aggregations, and sorting.\nThe script used in the runtime field calculates the total salary by increasing the base salary by 10%.\n\n\n\n\n\nDelete the index.\n\nDELETE employees\n\n\n\n\nElasticsearch: Runtime Fields\nElasticsearch Script Fields\n\n\n\n\n\n\n\n\nSearch for restaurants based on their location (city and state).\nInclude the restaurant’s name, cuisine type, and a calculated rating score in the search results.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nIndex some sample restaurant documents.\n\nPOST /restaurants/_bulk\n{ \"index\": { \"_id\": 1 } }\n{ \"name\": \"Tasty Bites\", \"city\": \"New York\", \"state\": \"NY\", \"cuisine\": \"Italian\", \"review_score\": 4.5, \"number_of_reviews\": 200 }\n{ \"index\": { \"_id\": 2 } }\n{ \"name\": \"Spicy Palace\", \"city\": \"Los Angeles\", \"state\": \"CA\", \"cuisine\": \"Indian\", \"review_score\": 4.2, \"number_of_reviews\": 150 }\n{ \"index\": { \"_id\": 3 } }\n{ \"name\": \"Sushi Spot\", \"city\": \"San Francisco\", \"state\": \"CA\", \"cuisine\": \"Japanese\", \"review_score\": 4.7, \"number_of_reviews\": 300 }\n{ \"index\": { \"_id\": 4 } }\n{ \"name\": \"Burger Joint\", \"city\": \"Chicago\", \"state\": \"IL\", \"cuisine\": \"American\", \"review_score\": 3.8, \"number_of_reviews\": 100 }\n\nDefine a runtime field named weighted_rating to calculate a weighted rating score for New York restaurants.\n\nGET /restaurants/_search\n{\n  \"runtime_mappings\": {\n    \"weighted_rating\": {\n      \"type\": \"double\",\n      \"script\": {\n        \"source\": \"emit(Math.sqrt(doc['review_score'].value * doc['number_of_reviews'].value))\"\n      }\n    }\n  },\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"city\": \"New York\"\n          }\n        },\n        {\n          \"match\": {\n            \"state\": \"NY\"\n          }\n        }\n      ]\n    }\n  },\n  \"fields\": [\n    \"name\",\n    \"cuisine\",\n    \"weighted_rating\"\n  ]\n}\n\n\n\n\nVerify the creation of the index and its mappings.\n\nGET /restaurants\n\nVerify the indexed documents.\n\nGET /restaurants/_search\n\nExecute the query and verify the restaurant name, cuisine type, and the calculated weighted rating score for restaurants located in New York, NY.\n\n\n\n\n\nThe runtime_mappings section defines a new field weighted_rating that calculates a weighted rating score based on the review_score and number_of_reviews fields.\nThe query section uses the match query to search for restaurants in New York, NY.\nThe fields section specifies the fields to include in the search results, including the runtime field weighted_rating.\n\n\n\n\n\nDelete the index.\n\nDELETE restaurants\n\n\n\n\nRuntime Fields\nMatch Query"
  },
  {
    "objectID": "5-cluster-management.html",
    "href": "5-cluster-management.html",
    "title": "Cluster Management",
    "section": "",
    "text": "While the odds are rather high that you will have some unassigned shards if you have done enough of the examples and not cleaned up after yourself we will artifically create some so the below will make some degree of sense.\n\n\n\n\n\nIdentify the cause of unassigned shards.\nReassign shards to nodes to improve cluster health.\nEnsure all indices are properly allocated and the cluster health status is green.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate an index that needs more replicas than available nodes\nPUT /a-bad-index\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 2\n  }\n}\nCheck the cluster health and identify unassigned shards (you should see at least 2 unassigned shards)\n\nGET /_cluster/health\n\n\n\nNumber of unassigned shards\n\n\n\nList the unassigned shards (you should see the index name created above with 2 messages of UNASSIGNED)\nGET _cat/shards?v=true&h=index,shard,prirep,state,node,unassigned.reason&s=state\n\n\n\nNames of the unassigned shards\n\n\nIdentify the reason for the unassigned shards\nGET _cluster/allocation/explain\n{\n  \"index\": \"a-bad-index\", \n  \"shard\": 0, \n  \"primary\": true \n}\nIn the scenario where you are running an Elasticsearch cluster locally and only have one node then you simply have to lower the number of replicas\nPUT /a-bad-index/_settings\n{\n  \"index\": {\n    \"number_of_replicas\": 0\n  }\n}\nRunning the shard check again will show that the unassigned shards are now gone.\nGET _cat/shards?v=true&h=index,shard,prirep,state,node,unassigned.reason&s=state\n\n\n\nThe unassigned shards are gone\n\n\nVerify the cluster health again\nGET /_cluster/health\n\n\n\n\n\nCheck the cluster health status\nGET /_cluster/health\nEnsure there are no unassigned shards\nGET /_cat/shards?v&h=index,shard,prirep,state,unassigned.reason,node\n\n\n\n\n\nThe cluster reroute command should be used carefully, especially when accepting data loss.\nForce merging should be done during low traffic periods as it is resource-intensive.\nRegularly monitoring cluster health can prevent shard allocation issues.\n\n\n\n\nStart here:\n\nTroubleshooting\n\nFor more detail:\n\nCat Shards API\nCluster Health API\nCluster Reroute API\nDiagnose Unassigned Shards\nForce Merge API\nNodes Stats API\n\n\n\n\n\n\n\n\nIdentify the cause of a shard failure\nResolve the issue and restore the cluster’s health\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCheck the cluster’s health\nGET /_cluster/health\nIdentify the index and shard with issues\nGET /_cat/shards\nCheck the shard’s status\nGET /_cat/shards/{index_name}-{shard_number}\nResolve the issue (e.g., restart a node, reassign the shard)\nPOST /_cluster/reroute\n{\n  \"commands\": [\n    {\n      \"move\": {\n        \"index\": \"{index_name}\",\n        \"shard\": {shard_number},\n        \"from_node\": \"{node_name}\",\n        \"to_node\": \"{new_node_name}\"\n      }\n    }\n  ]\n}\nVerify the cluster’s health\nGET /_cluster/health\n\n\n\n\n\nVerify that the shard is no longer in a failed state\nGET /_cat/shards/{index_name}-{shard_number}\n\n\n\n\n\nRegularly monitoring the cluster’s health can help identify issues before they become critical.\nUnderstanding the cause of the shard failure is crucial to resolving the issue effectively.\n\n\n\n\n\nElasticsearch Cluster Health\nElasticsearch Shard\n\n\n\n\n\n\n\n\n\n\n\nBack up the entire Elasticsearch cluster (all the indices on the cluster)\nRestore specific indices from the backup\n\n\n\n\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client.\nCreate two sample indexes with some data\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"1\" } }\n{ \"name\": \"Document 1.1\" }\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"2\" } }\n{ \"name\": \"Document 1.2\" }\n\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"1\" } }\n{ \"name\": \"Document 2.1\" }\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"2\" } }\n{ \"name\": \"Document 2.2\" }\nConfirm the documents were indexed\nGET example_index*/_search\nCreate a snapshot repository\nPUT /_snapshot/example_index_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nCreate a snapshot of the two example indices\nPUT /_snapshot/example_index_backup/snapshot_1\n{\n    \"indices\": \"example_index1,example_index2\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\nVerify the snapshot status\nGET /_snapshot/example_index_backup/snapshot_1\nDelete the two known indices\nDELETE /example_index1\nDELETE /example_index2\nCheck that the two indexes are gone.\nGET /example_index*/_search\nRestore both indices from the snapshot\nPOST /_snapshot/example_index_backup/snapshot_1/_restore\nConfirm both indices were restored\nGET /example_index*/_search\nRestore one index from the snapshot\nDELETE /example_index1\nDELETE /example_index2\nPOST /_snapshot/example_index_backup/snapshot_1/_restore\n{\n  \"indices\": \"example_index2\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false\n}\n\n\n\n\n\nVerify the index has been restored\nGET /example_index2/_search\nVerify the integrity of the snapshot\nPOST /_snapshot/example_index_backup/_verify\nCheck the cluster health to ensure the index is properly allocated\nGET /_cluster/health/example_index2\n\n\n\n\n\nThe snapshot repository is configured with the fs (file system) type, which stores the backup data in the container’s local file system. For production use, you may want to use a more suitable repository type, such as s3 or gcs.\nThe snapshot name snapshot_1 is used to create a backup of the two indices.\n\n\n\n\n\nDelete the indices\nDELETE /example_index1\nDELETE /example_index2\nDelete the Backup JSON     DELETE /_snapshot/example_index_backup/snapshot_1\n\n\n\n\n\nSnapshot and Restore\nSnapshot Repository APIs\nSnapshot Restore API\n\n\n\n\n\n\n\n\nBack up the entire Elasticsearch cluster\nRestore specific indices from the backup\n\n\n\n\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client.\nCreate two sample indexes with some data\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"1\" } }\n{ \"name\": \"Document 1.1\" }\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"2\" } }\n{ \"name\": \"Document 1.2\" }\n\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"1\" } }\n{ \"name\": \"Document 2.1\" }\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"2\" } }\n{ \"name\": \"Document 2.2\" }\nConfirm the documents were indexed\nGET example_index*/_search\nCreate a snapshot repository\nPUT /_snapshot/example_cluster_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nCreate a snapshot of the entire cluster\nPUT /_snapshot/example_cluster_backup/full_cluster_backup\nVerify the snapshot status\nGET /_snapshot/example_cluster_backup/full_cluster_backup\nDelete one of the existing indices\nDELETE example_index2\nRestore that specific index from the snapshot with a different name\nPOST /_snapshot/example_cluster_backup/full_cluster_backup/_restore\n{\n    \"indices\": \"example_index2\",\n    \"rename_pattern\": \"example_index2\",\n    \"rename_replacement\": \"restored_example_index2\"\n}\n\n\n\n\n\nVerify the index has been restored\nGET /restored_example_index2/_search\nThe response should include the documents from the original example_index2.\nOptionally, you can delete the original index and verify that the restored index remains\nDELETE /example_index2\nGET /restored_example_index2/_search\nVerify the integrity of the snapshot\nPOST /_snapshot/example_cluster_backup/_verify\nCheck the cluster health to ensure the index is properly allocated\nGET /_cluster/health/restored_example_index2\n\n\n\n\n\nThe snapshot repository is configured with the fs (file system) type, which stores the backup data in the container’s local file system. For production use, you may want to use a more suitable repository type, such as s3 or gcs.\nThe snapshot name full_cluster_backup is used to create a backup of the entire cluster.\nDuring the restore process, the rename_pattern and rename_replacement options are used to rename the restored index to restored_example_index2.\n\n\n\n\n\nDelete the indices\nDELETE /example_index1\nDELETE /example_index2\nDELETE /restored_example_index2\nDelete the Backup JSON     DELETE /_snapshot/example_cluster_backup/full_cluster_backup\n\n\n\n\n\nSnapshot and Restore\nSnapshot Repository APIs\nSnapshot Restore API\n\n\n\n\n\n\n\n\nCreate a repository for storing snapshots.\nTake a snapshot of the specified index.\nRestore the snapshot to the cluster.\nVerify the integrity and availability of the restored data.\n\n\n\n\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client\nCreate two sample indexes with some data\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"1\" } }\n{ \"name\": \"Document 1.1\" }\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"2\" } }\n{ \"name\": \"Document 1.2\" }\n\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"1\" } }\n{ \"name\": \"Document 2.1\" }\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"2\" } }\n{ \"name\": \"Document 2.2\" }\nConfirm the documents were indexed\nGET example_index*/_search\nCreate a snapshot repository\nPUT /_snapshot/single_index_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nTake a snapshot of the specific index\nPUT /_snapshot/single_index_backup/snapshot_1\n{\n    \"indices\": \"example_index1\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\nVerify the snapshot status\nGET /_snapshot/single_index_backup/snapshot_1\nDelete the index to simulate data loss (optional for testing restore)\nDELETE /example_index1\nRestore the snapshot\nPOST /_snapshot/single_index_backup/snapshot_1/_restore\n{\n    \"indices\": \"example_index1\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\n\n\n\n\n\nVerify the index has been restored\nGET /example_index1/_search\nVerify the integrity of the snapshot\nPOST /_snapshot/single_index_backup/_verify\nCheck the cluster health to ensure the index is properly allocated\nGET /_cluster/health/example_index1\n\n\n\n\n\nThe repository type fs is used for simplicity; other types like s3 can be used depending on the environment.\nignore_unavailable ensures the snapshot process continues even if some indices are missing.\ninclude_global_state is set to false to avoid restoring cluster-wide settings unintentionally.\n\n\n\n\n\nDelete the indices\nDELETE /example_index1\nDELETE /example_index2\nDELETE /restored_example_index2\nDelete the Backup\nDELETE /_snapshot/single_index_backup/snapshot_1\n\n\n\n\n\nSnapshot and Restore\nCreate Snapshot API\nRestore Snapshot API\n\n\n\n\n\n\n\n\nSigh. This will only work if you have an Enterprise license.\n\n\n\nCreate a repository for storing snapshots.\nTake a snapshot of the specified index.\nMount the snapshot as a searchable index.\nVerify the index is searchable without restoring it to the cluster.\n\n\n\n\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client.\nCreate a sample index with some data\nPOST _bulk\n{ \"index\": { \"_index\": \"products\", \"_id\": \"1\" } }\n{ \"name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 999.99, \"stock\": 50, \"description\": \"A high-performance laptop with 16GB RAM and 512GB SSD.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"2\" } }\n{ \"name\": \"Smartphone\", \"category\": \"Electronics\", \"price\": 699.99, \"stock\": 100, \"description\": \"A latest model smartphone with a stunning display and powerful processor.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"3\" } }\n{ \"name\": \"Headphones\", \"category\": \"Accessories\", \"price\": 199.99, \"stock\": 200, \"description\": \"Noise-cancelling over-ear headphones with superior sound quality.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"4\" } }\n{ \"name\": \"Coffee Maker\", \"category\": \"Home Appliances\", \"price\": 89.99, \"stock\": 75, \"description\": \"A programmable coffee maker with a 12-cup capacity.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"5\" } }\n{ \"name\": \"Running Shoes\", \"category\": \"Footwear\", \"price\": 129.99, \"stock\": 150, \"description\": \"Lightweight running shoes with excellent cushioning and support.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"6\" } }\n{ \"name\": \"Backpack\", \"category\": \"Accessories\", \"price\": 49.99, \"stock\": 300, \"description\": \"Durable backpack with multiple compartments and ergonomic design.\" }\nConfirm the documents were indexed\nGET products/_search\nCreate a snapshot repository\nPUT /_snapshot/products_index_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nTake a snapshot of the specific index\nPUT /_snapshot/products_index_backup/snapshot_1\n{\n  \"indices\": \"products\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false\n}\nVerify the snapshot status\nGET /_snapshot/products_index_backup/snapshot_1\nDelete the index to simulate data loss (optional for testing restore)\nDELETE /products\nMount the snapshot as a searchable index\nPUT /_snapshot/products_index_backup/snapshot_1/_mount\n{\n    \"index\": \"products\",\n    \"renamed_index\": \"products_backup_searchable\"\n}\nIf you don’t have an Enterprise license the above will fail.\n\n\n\n\n\nVerify the mounted index is searchable\nGET /products_backup_searchable/_search\n{\n    \"query\": {\n        \"match_all\": {}\n    }\n}\nCheck the cluster health to ensure the searchable snapshot is properly allocated\nGET /_cluster/health/products_backup_searchable\n\n\n\n\n\nThe repository type fs is used for simplicity; other types like s3 can be used depending on the environment.\nignore_unavailable ensures the snapshot process continues even if some indices are missing.\ninclude_global_state is set to false to avoid restoring cluster-wide settings unintentionally.\nMounting the snapshot as a searchable index allows for searching the data without the need to fully restore it, saving resources and time.\n\n\n\n\n\nDelete the index\nDELETE /products\nDelete the Backup\nDELETE /_snapshot/products_index_backup/snapshot_1\n\n\n\n\n\nCreate Snapshot API\nMount Searchable Snapshot API\nSearch API\nSnapshot and Restore\n\n\n\n\n\n\nFYI: This is similar to the example at Searching Data &gt; Write and execute a query that searches across multiple clusters\n\n\nThe following instructions are for two single-node clusters running locally on your computer.\n\n\n\nConfigure the remote cluster to be searchable from the local cluster.\nEnsure secure communication between clusters.\nVerify the cross-cluster search functionality.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nConfigure the remote cluster on the local cluster\nPUT /_cluster/settings\n{\n\"persistent\": {\n    \"cluster\": {\n    \"remote\": {\n        \"es01\": {\n        \"seeds\": [\n            \"es01:9300\"\n        ],\n        \"skip_unavailable\": true\n        },\n        \"es02\": {\n        \"seeds\": [\n            \"es02:9300\"\n        ],\n        \"skip_unavailable\": false\n        }\n    }\n    }\n}\n}\n(optional if you are doing this locally) Set up security settings where you have keystores properly setup. On the remote cluster:\nPUT /_cluster/settings\n{\n    \"persistent\": {\n        \"xpack.security.enabled\": true,\n        \"xpack.security.transport.ssl.enabled\": true,\n        \"xpack.security.transport.ssl.verification_mode\": \"certificate\",\n        \"xpack.security.transport.ssl.keystore.path\": \"/path/to/keystore.jks\",\n        \"xpack.security.transport.ssl.truststore.path\": \"/path/to/truststore.jks\"\n    }\n}\nOn the local cluster:\nPUT /_cluster/settings\n{\n    \"persistent\": {\n        \"xpack.security.enabled\": true,\n        \"xpack.security.transport.ssl.enabled\": true,\n        \"xpack.security.transport.ssl.verification_mode\": \"certificate\",\n        \"xpack.security.transport.ssl.keystore.path\": \"/path/to/keystore.jks\",\n        \"xpack.security.transport.ssl.truststore.path\": \"/path/to/truststore.jks\"\n    }\n}\nVerify the remote cluster configuration\nGET /_remote/info\nIndex product documents into each cluster.\n\n\nFor es01 (potentially the local cluster):\nPOST /products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\nFor es02 (potentially the “remote” cluster) through the command line:\ncurl -u elastic:[your password here] -X POST \"http://localhost:9201/products/_bulk?pretty\" -H 'Content-Type: application/json' -d'\n{ \"index\": { \"_id\": \"5\" } }\n{ \"product\": \"Elasticsearch Stickers\", \"category\": \"Accessories\", \"price\": 4.99 }\n{ \"index\": { \"_id\": \"6\" } }\n{ \"product\": \"Elasticsearch Notebook\", \"category\": \"Stationery\", \"price\": 7.99 }\n{ \"index\": { \"_id\": \"7\" } }\n{ \"product\": \"Elasticsearch Pen\", \"category\": \"Stationery\", \"price\": 3.49 }\n{ \"index\": { \"_id\": \"8\" } }\n{ \"product\": \"Elasticsearch Hoodie\", \"category\": \"Apparel\", \"price\": 45.99 }    '\n\n\nPerform a cross-cluster search query\nGET /remote_cluster:products/_search\n{\n    \"query\": {\n        \"match_all\": {}\n    }\n}\n\n\n\n\n\nVerify the remote cluster info\nGET /_remote/info\nSearch the remote cluster from the local cluster\nGET /remote_cluster:product_catalog/_search\n{\n    \"query\": {\n        \"match_all\": {}\n    }\n}\n\n\n\n\n\nEnsure that the nodes listed in the seeds setting are accessible from the local cluster.\nSecurity settings such as SSL/TLS should be configured to ensure secure communication between clusters.\nRegularly monitor the connection status between the clusters to ensure reliability.\n\n\n\n\n\nDelete the es01 index.\nDELETE products\nDelete the es02 index from the command line.\ncurl -u elastic:[your password here] -X DELETE \"http://localhost:9201/products?pretty\"\n\n\n\n\n\nCross-Cluster Search\nCluster Remote Info API\nSearch API\nSecurity Settings\n\n\n\n\n\n\nThere are a number of ways to set up cross-cluster replication and they can all be found here.\nCross-cluster replication needs an Enterprise license\n\n\n\nIn this example, we will run 2 single-node clusters locally using containers (as we have for all the other examples).\n\nThe es01 container instance will be considered\n\nleader\nremote\n\nThe es02 container instance will be considered\n\nfollower\nlocal\n\n\nYou may also need to get a free 30-day trial license of certain features including cross-cluster replication. Since the second cluster is not hooked up to Kibana execute this from the command line (assuming you called the docker instance es02 as we have been using in this guide):\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X POST \"http://localhost:9201/_license/start_trial?pretty&acknowledge=true\"\n\n\n\nConfigure remote cluster settings on both leader and follower clusters.\nSet up the leader index on the leader cluster.\nConfigure the follower index on the follower cluster to replicate from the leader index.\nEnsure secure communication between clusters.\nVerify replication and data consistency.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nConfigure the remote cluster settings on the leader cluster (es01)\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n        \"seeds\": [\n            \"es01:9300\"\n        ],\n        \"skip_unavailable\": true\n        },\n        \"es02\": {\n        \"seeds\": [\n            \"es02:9300\"\n        ],\n        \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}\nConfigure the local cluster settings on the follower cluster (es02)\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X PUT \"http://localhost:9201/_cluster/settings?pretty\" -H \"Content-Type: application/json\" -d'\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n        \"seeds\": [\n            \"es01:9300\"\n        ],\n        \"skip_unavailable\": true\n        },\n        \"es02\": {\n        \"seeds\": [\n            \"es02:9300\"\n        ],\n        \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}'\nCreate the leader index on the leader cluster (es01)\nPUT /product_catalog\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n     \"properties\": {\n     \"product_id\": {\n       \"type\": \"keyword\"\n     },\n     \"name\": {\n       \"type\": \"text\"\n     },\n     \"description\": {\n       \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\nIndex sample documents in the leader index\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 20.0 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 30.0 }\nConfigure the follower index on the follower cluster through the command line\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X PUT \"http://localhost:9201/product_catalog_follower/_ccr/follow?pretty\" -H \"Content-Type: application/json\" -d'\n{\n  \"remote_cluster\": \"es01\",\n  \"leader_index\": \"product_catalog\"\n}'\n\n\n\n\n\nVerify the follower index (es02) is following the leader index\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] \"http://localhost:9201/product_catalog_follower/_stats?pretty\"\nCheck the data in the follower index (es02) to ensure it matches the leader (es01) index\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] \"http://localhost:9201/product_catalog_follower/_search?pretty\"\n\n\n\n\n\nEnsure the nodes listed in the seeds setting are accessible from the follower cluster.\nSecurity settings such as SSL/TLS should be configured to ensure secure communication between clusters (but not for this example given the YAML changes suggested in the Appendix).\nRegularly monitor the replication status and performance to ensure data consistency and reliability.\n\n\n\n\n\nDelete the follower configuration\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X DELETE \"http://localhost:9201/product_catalog_follower?pretty\"\nDelete the index\nDELETE product_catalog\n\n\n\n\n\nCross-Cluster Replication\nCreate Follower Index API\nCluster Remote Info API\nSearch API\nSecurity Settings"
  },
  {
    "objectID": "5-cluster-management.html#task-diagnose-shard-issues-and-repair-a-clusters-health",
    "href": "5-cluster-management.html#task-diagnose-shard-issues-and-repair-a-clusters-health",
    "title": "Cluster Management",
    "section": "",
    "text": "While the odds are rather high that you will have some unassigned shards if you have done enough of the examples and not cleaned up after yourself we will artifically create some so the below will make some degree of sense.\n\n\n\n\n\nIdentify the cause of unassigned shards.\nReassign shards to nodes to improve cluster health.\nEnsure all indices are properly allocated and the cluster health status is green.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCreate an index that needs more replicas than available nodes\nPUT /a-bad-index\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 2\n  }\n}\nCheck the cluster health and identify unassigned shards (you should see at least 2 unassigned shards)\n\nGET /_cluster/health\n\n\n\nNumber of unassigned shards\n\n\n\nList the unassigned shards (you should see the index name created above with 2 messages of UNASSIGNED)\nGET _cat/shards?v=true&h=index,shard,prirep,state,node,unassigned.reason&s=state\n\n\n\nNames of the unassigned shards\n\n\nIdentify the reason for the unassigned shards\nGET _cluster/allocation/explain\n{\n  \"index\": \"a-bad-index\", \n  \"shard\": 0, \n  \"primary\": true \n}\nIn the scenario where you are running an Elasticsearch cluster locally and only have one node then you simply have to lower the number of replicas\nPUT /a-bad-index/_settings\n{\n  \"index\": {\n    \"number_of_replicas\": 0\n  }\n}\nRunning the shard check again will show that the unassigned shards are now gone.\nGET _cat/shards?v=true&h=index,shard,prirep,state,node,unassigned.reason&s=state\n\n\n\nThe unassigned shards are gone\n\n\nVerify the cluster health again\nGET /_cluster/health\n\n\n\n\n\nCheck the cluster health status\nGET /_cluster/health\nEnsure there are no unassigned shards\nGET /_cat/shards?v&h=index,shard,prirep,state,unassigned.reason,node\n\n\n\n\n\nThe cluster reroute command should be used carefully, especially when accepting data loss.\nForce merging should be done during low traffic periods as it is resource-intensive.\nRegularly monitoring cluster health can prevent shard allocation issues.\n\n\n\n\nStart here:\n\nTroubleshooting\n\nFor more detail:\n\nCat Shards API\nCluster Health API\nCluster Reroute API\nDiagnose Unassigned Shards\nForce Merge API\nNodes Stats API\n\n\n\n\n\n\n\n\nIdentify the cause of a shard failure\nResolve the issue and restore the cluster’s health\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCheck the cluster’s health\nGET /_cluster/health\nIdentify the index and shard with issues\nGET /_cat/shards\nCheck the shard’s status\nGET /_cat/shards/{index_name}-{shard_number}\nResolve the issue (e.g., restart a node, reassign the shard)\nPOST /_cluster/reroute\n{\n  \"commands\": [\n    {\n      \"move\": {\n        \"index\": \"{index_name}\",\n        \"shard\": {shard_number},\n        \"from_node\": \"{node_name}\",\n        \"to_node\": \"{new_node_name}\"\n      }\n    }\n  ]\n}\nVerify the cluster’s health\nGET /_cluster/health\n\n\n\n\n\nVerify that the shard is no longer in a failed state\nGET /_cat/shards/{index_name}-{shard_number}\n\n\n\n\n\nRegularly monitoring the cluster’s health can help identify issues before they become critical.\nUnderstanding the cause of the shard failure is crucial to resolving the issue effectively.\n\n\n\n\n\nElasticsearch Cluster Health\nElasticsearch Shard"
  },
  {
    "objectID": "5-cluster-management.html#join-with",
    "href": "5-cluster-management.html#join-with",
    "title": "Cluster Management",
    "section": "",
    "text": "Diagnose the cause of unassigned shards in an Elasticsearch cluster.\nTake appropriate actions to resolve the issue and restore cluster health.\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCheck Cluster Health:\n\nGet an overview of the cluster’s health status:\n\nGET /_cluster/health\n\nLook for the status (red, yellow, or green) and analyze the shard allocation details, particularly unassigned shards.\n\nIdentify the Cause:\n\nBased on the cluster health report and logs, investigate the cause of unassigned shards.\n\nCommon reasons include:\n\nNode failures: Check for failing nodes or unavailable disks.\nResource limitations: Insufficient disk space or memory on nodes.\nIndex configuration issues: Inconsistent shard allocation settings across nodes.\n\nResolve the Issue:\n\nDepending on the cause, take corrective actions:\n\nRestart failed nodes or repair unavailable disks.\nAllocate additional resources to nodes or optimize shard allocation.\nReview and adjust index settings for shard allocation.\n\n\nVerify Cluster Health:\n\nMonitor the cluster health again after taking corrective actions:\n\nGET /_cluster/health\n\nThe cluster health should ideally be green, indicating all shards are allocated and functioning correctly.\n\n\n\n\n\n\nThis task involves real-time cluster monitoring and troubleshooting. There is no specific API call to test as the solution depends on the identified root cause.\n\n\n\n\n\nUnderstanding the cluster health report and shard allocation details is crucial for diagnosing issues.\nThe chosen resolution method depends on the specific cause of unassigned shards.\n\n\n\n\n\n\n\n\nCluster Health API\nShard Allocation\n\n\n\n\n\n\n\n\nIdentify the cause of a shard failure\nResolve the issue and restore the cluster’s health\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCheck the cluster’s health\nGET /_cluster/health\nIdentify the index and shard with issues\nGET /_cat/shards\nCheck the shard’s status\nGET /_cat/shards/{index_name}-{shard_number}\nResolve the issue (e.g., restart a node, reassign the shard)\nPOST /_cluster/reroute\n{\n    \"commands\": [\n        {\n            \"move\": {\n                \"index\": \"{index_name}\",\n                \"shard\": {shard_number},\n                \"from_node\": \"{node_name}\",\n                \"to_node\": \"{new_node_name}\"\n            }\n        }\n    ]\n}\nVerify the cluster’s health\nGET /_cluster/health\n\n\n\n\n\nVerify that the shard is no longer in a failed state\nGET /_cat/shards/{index_name}-{shard_number}\n\n\n\n\n\nRegularly monitoring the cluster’s health can help identify issues before they become critical.\nUnderstanding the cause of the shard failure is crucial to resolving the issue effectively.\n\n\n\n\n\n\n\n\nElasticsearch Cluster Health\nElasticsearch Shard"
  },
  {
    "objectID": "5-cluster-management.html#join-with-1",
    "href": "5-cluster-management.html#join-with-1",
    "title": "Cluster Management",
    "section": "",
    "text": "Identify indices with shard allocation issues\nDiagnose the root cause of the shard allocation issues\nResolve the shard allocation issues to restore the cluster’s health\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCheck the cluster health and identify indices with shard allocation issues\nGET /_cluster/health?level=indices\nThe response will show the health status of each index, including the number of unassigned shards, if any.\nFor indices with unassigned shards, retrieve detailed shard allocation information\nGET /_cluster/allocation/explain\nThe response will provide information about the unassigned shards and the reasons why they cannot be allocated.\nBased on the shard allocation explanation, take appropriate actions to resolve the issues. Common examples include:\n\nAdding new nodes to the cluster (if there are not enough nodes/resources)\nIncreasing the cluster’s disk space (if there is not enough disk space)\nAdjusting shard allocation settings (e.g., cluster.routing.allocation.enable)\nRemoving corrupted or problematic indices\n\nFor example, if the issue is related to disk space, you can increase the disk space on the existing nodes or add new nodes with more disk space.\nAfter resolving the issue, reattempt the shard allocation\nPOST /_cluster/reroute?retry_failed=true\nThis will trigger a new shard allocation attempt for the previously unassigned shards.\n\n\n\n\n\nMonitor the cluster health after the corrective actions\nGET /_cluster/health?level=indices\nThe response should show all shards as assigned and the cluster health status as “green”.\nAdditionally, you can check the shard allocation explanation again\nGET /_cluster/allocation/explain\nThe response should not show any unassigned shards or shard allocation issues.\n\n\n\n\n\nThe specific actions taken to resolve shard allocation issues depend on the root cause identified in the shard allocation explanation.\nCommon causes of shard allocation issues include lack of resources (nodes, disk space), cluster settings, and data corruption.\nResolving shard allocation issues is crucial for maintaining a healthy and stable Elasticsearch cluster.\n\n\n\n\n\n\n\n\nCluster Health API\nCluster Reroute API\nCluster Allocation Explanation API\nShard Allocation Filtering"
  },
  {
    "objectID": "5-cluster-management.html#join-with-2",
    "href": "5-cluster-management.html#join-with-2",
    "title": "Cluster Management",
    "section": "",
    "text": "An Elasticsearch cluster with multiple nodes\nOne or more indices with shards in an unhealthy state (e.g., relocating, initializing, unassigned)\n\n\n\n\n\nOpen the Kibana Console or use a REST client\nCheck the cluster health\nGET /_cluster/health\nThis will provide an overview of the cluster’s health status, including the number of shards in different states.\nIf the cluster health is not “green” (all shards are active and assigned), check for shard issues\nGET /_cat/shards?v\nThis will list all shards in the cluster, their state, and the node they are assigned to.\nIdentify the shards that are in an unhealthy state (e.g., relocating, initializing, unassigned) and the corresponding indices.\nFor unassigned shards, try to allocate them manually\nPOST /_cluster/reroute?retry_failed=true\nThis will attempt to assign the unassigned shards to available nodes.\nIf shards are stuck in the “relocating” or “initializing” state, you may need to cancel the shard allocation and retry\nPOST /_cluster/reroute?cancel_data_node=&lt;node_id&gt;\nReplace &lt;node_id&gt; with the ID of the node where the stuck shard is currently located.\nIf the issue persists, you may need to restart the affected nodes or perform a full cluster restart.\n\n\n\n\n\nAfter each step, check the cluster health and shard status again to verify if the issue has been resolved.\nIf all shards are active and assigned, the cluster health should be “green”.\n\n\n\n\n\nUnassigned shards can occur due to various reasons, such as node failures, disk space issues, or cluster configuration problems.\nShards stuck in the “relocating” or “initializing” state may indicate a node or network issue.\nCanceling shard allocation and retrying can help resolve stuck shards in some cases.\nRestarting nodes or the entire cluster should be a last resort, as it can cause temporary data unavailability.\n\n\n\n\n\n\n\n\nElasticsearch Cluster Health\nElasticsearch Cat Shards API\nElasticsearch Cluster Reroute API\nElasticsearch Node Restart"
  },
  {
    "objectID": "5-cluster-management.html#task-backup-and-restore-a-cluster-andor-specific-indices",
    "href": "5-cluster-management.html#task-backup-and-restore-a-cluster-andor-specific-indices",
    "title": "Cluster Management",
    "section": "",
    "text": "Back up the entire Elasticsearch cluster (all the indices on the cluster)\nRestore specific indices from the backup\n\n\n\n\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client.\nCreate two sample indexes with some data\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"1\" } }\n{ \"name\": \"Document 1.1\" }\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"2\" } }\n{ \"name\": \"Document 1.2\" }\n\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"1\" } }\n{ \"name\": \"Document 2.1\" }\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"2\" } }\n{ \"name\": \"Document 2.2\" }\nConfirm the documents were indexed\nGET example_index*/_search\nCreate a snapshot repository\nPUT /_snapshot/example_index_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nCreate a snapshot of the two example indices\nPUT /_snapshot/example_index_backup/snapshot_1\n{\n    \"indices\": \"example_index1,example_index2\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\nVerify the snapshot status\nGET /_snapshot/example_index_backup/snapshot_1\nDelete the two known indices\nDELETE /example_index1\nDELETE /example_index2\nCheck that the two indexes are gone.\nGET /example_index*/_search\nRestore both indices from the snapshot\nPOST /_snapshot/example_index_backup/snapshot_1/_restore\nConfirm both indices were restored\nGET /example_index*/_search\nRestore one index from the snapshot\nDELETE /example_index1\nDELETE /example_index2\nPOST /_snapshot/example_index_backup/snapshot_1/_restore\n{\n  \"indices\": \"example_index2\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false\n}\n\n\n\n\n\nVerify the index has been restored\nGET /example_index2/_search\nVerify the integrity of the snapshot\nPOST /_snapshot/example_index_backup/_verify\nCheck the cluster health to ensure the index is properly allocated\nGET /_cluster/health/example_index2\n\n\n\n\n\nThe snapshot repository is configured with the fs (file system) type, which stores the backup data in the container’s local file system. For production use, you may want to use a more suitable repository type, such as s3 or gcs.\nThe snapshot name snapshot_1 is used to create a backup of the two indices.\n\n\n\n\n\nDelete the indices\nDELETE /example_index1\nDELETE /example_index2\nDelete the Backup JSON     DELETE /_snapshot/example_index_backup/snapshot_1\n\n\n\n\n\nSnapshot and Restore\nSnapshot Repository APIs\nSnapshot Restore API\n\n\n\n\n\n\n\n\nBack up the entire Elasticsearch cluster\nRestore specific indices from the backup\n\n\n\n\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client.\nCreate two sample indexes with some data\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"1\" } }\n{ \"name\": \"Document 1.1\" }\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"2\" } }\n{ \"name\": \"Document 1.2\" }\n\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"1\" } }\n{ \"name\": \"Document 2.1\" }\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"2\" } }\n{ \"name\": \"Document 2.2\" }\nConfirm the documents were indexed\nGET example_index*/_search\nCreate a snapshot repository\nPUT /_snapshot/example_cluster_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nCreate a snapshot of the entire cluster\nPUT /_snapshot/example_cluster_backup/full_cluster_backup\nVerify the snapshot status\nGET /_snapshot/example_cluster_backup/full_cluster_backup\nDelete one of the existing indices\nDELETE example_index2\nRestore that specific index from the snapshot with a different name\nPOST /_snapshot/example_cluster_backup/full_cluster_backup/_restore\n{\n    \"indices\": \"example_index2\",\n    \"rename_pattern\": \"example_index2\",\n    \"rename_replacement\": \"restored_example_index2\"\n}\n\n\n\n\n\nVerify the index has been restored\nGET /restored_example_index2/_search\nThe response should include the documents from the original example_index2.\nOptionally, you can delete the original index and verify that the restored index remains\nDELETE /example_index2\nGET /restored_example_index2/_search\nVerify the integrity of the snapshot\nPOST /_snapshot/example_cluster_backup/_verify\nCheck the cluster health to ensure the index is properly allocated\nGET /_cluster/health/restored_example_index2\n\n\n\n\n\nThe snapshot repository is configured with the fs (file system) type, which stores the backup data in the container’s local file system. For production use, you may want to use a more suitable repository type, such as s3 or gcs.\nThe snapshot name full_cluster_backup is used to create a backup of the entire cluster.\nDuring the restore process, the rename_pattern and rename_replacement options are used to rename the restored index to restored_example_index2.\n\n\n\n\n\nDelete the indices\nDELETE /example_index1\nDELETE /example_index2\nDELETE /restored_example_index2\nDelete the Backup JSON     DELETE /_snapshot/example_cluster_backup/full_cluster_backup\n\n\n\n\n\nSnapshot and Restore\nSnapshot Repository APIs\nSnapshot Restore API\n\n\n\n\n\n\n\n\nCreate a repository for storing snapshots.\nTake a snapshot of the specified index.\nRestore the snapshot to the cluster.\nVerify the integrity and availability of the restored data.\n\n\n\n\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client\nCreate two sample indexes with some data\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"1\" } }\n{ \"name\": \"Document 1.1\" }\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"2\" } }\n{ \"name\": \"Document 1.2\" }\n\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"1\" } }\n{ \"name\": \"Document 2.1\" }\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"2\" } }\n{ \"name\": \"Document 2.2\" }\nConfirm the documents were indexed\nGET example_index*/_search\nCreate a snapshot repository\nPUT /_snapshot/single_index_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nTake a snapshot of the specific index\nPUT /_snapshot/single_index_backup/snapshot_1\n{\n    \"indices\": \"example_index1\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\nVerify the snapshot status\nGET /_snapshot/single_index_backup/snapshot_1\nDelete the index to simulate data loss (optional for testing restore)\nDELETE /example_index1\nRestore the snapshot\nPOST /_snapshot/single_index_backup/snapshot_1/_restore\n{\n    \"indices\": \"example_index1\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\n\n\n\n\n\nVerify the index has been restored\nGET /example_index1/_search\nVerify the integrity of the snapshot\nPOST /_snapshot/single_index_backup/_verify\nCheck the cluster health to ensure the index is properly allocated\nGET /_cluster/health/example_index1\n\n\n\n\n\nThe repository type fs is used for simplicity; other types like s3 can be used depending on the environment.\nignore_unavailable ensures the snapshot process continues even if some indices are missing.\ninclude_global_state is set to false to avoid restoring cluster-wide settings unintentionally.\n\n\n\n\n\nDelete the indices\nDELETE /example_index1\nDELETE /example_index2\nDELETE /restored_example_index2\nDelete the Backup\nDELETE /_snapshot/single_index_backup/snapshot_1\n\n\n\n\n\nSnapshot and Restore\nCreate Snapshot API\nRestore Snapshot API"
  },
  {
    "objectID": "5-cluster-management.html#task-configure-a-snapshot-to-be-searchable",
    "href": "5-cluster-management.html#task-configure-a-snapshot-to-be-searchable",
    "title": "Cluster Management",
    "section": "",
    "text": "Sigh. This will only work if you have an Enterprise license.\n\n\n\nCreate a repository for storing snapshots.\nTake a snapshot of the specified index.\nMount the snapshot as a searchable index.\nVerify the index is searchable without restoring it to the cluster.\n\n\n\n\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client.\nCreate a sample index with some data\nPOST _bulk\n{ \"index\": { \"_index\": \"products\", \"_id\": \"1\" } }\n{ \"name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 999.99, \"stock\": 50, \"description\": \"A high-performance laptop with 16GB RAM and 512GB SSD.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"2\" } }\n{ \"name\": \"Smartphone\", \"category\": \"Electronics\", \"price\": 699.99, \"stock\": 100, \"description\": \"A latest model smartphone with a stunning display and powerful processor.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"3\" } }\n{ \"name\": \"Headphones\", \"category\": \"Accessories\", \"price\": 199.99, \"stock\": 200, \"description\": \"Noise-cancelling over-ear headphones with superior sound quality.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"4\" } }\n{ \"name\": \"Coffee Maker\", \"category\": \"Home Appliances\", \"price\": 89.99, \"stock\": 75, \"description\": \"A programmable coffee maker with a 12-cup capacity.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"5\" } }\n{ \"name\": \"Running Shoes\", \"category\": \"Footwear\", \"price\": 129.99, \"stock\": 150, \"description\": \"Lightweight running shoes with excellent cushioning and support.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"6\" } }\n{ \"name\": \"Backpack\", \"category\": \"Accessories\", \"price\": 49.99, \"stock\": 300, \"description\": \"Durable backpack with multiple compartments and ergonomic design.\" }\nConfirm the documents were indexed\nGET products/_search\nCreate a snapshot repository\nPUT /_snapshot/products_index_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nTake a snapshot of the specific index\nPUT /_snapshot/products_index_backup/snapshot_1\n{\n  \"indices\": \"products\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false\n}\nVerify the snapshot status\nGET /_snapshot/products_index_backup/snapshot_1\nDelete the index to simulate data loss (optional for testing restore)\nDELETE /products\nMount the snapshot as a searchable index\nPUT /_snapshot/products_index_backup/snapshot_1/_mount\n{\n    \"index\": \"products\",\n    \"renamed_index\": \"products_backup_searchable\"\n}\nIf you don’t have an Enterprise license the above will fail.\n\n\n\n\n\nVerify the mounted index is searchable\nGET /products_backup_searchable/_search\n{\n    \"query\": {\n        \"match_all\": {}\n    }\n}\nCheck the cluster health to ensure the searchable snapshot is properly allocated\nGET /_cluster/health/products_backup_searchable\n\n\n\n\n\nThe repository type fs is used for simplicity; other types like s3 can be used depending on the environment.\nignore_unavailable ensures the snapshot process continues even if some indices are missing.\ninclude_global_state is set to false to avoid restoring cluster-wide settings unintentionally.\nMounting the snapshot as a searchable index allows for searching the data without the need to fully restore it, saving resources and time.\n\n\n\n\n\nDelete the index\nDELETE /products\nDelete the Backup\nDELETE /_snapshot/products_index_backup/snapshot_1\n\n\n\n\n\nCreate Snapshot API\nMount Searchable Snapshot API\nSearch API\nSnapshot and Restore"
  },
  {
    "objectID": "5-cluster-management.html#task-configure-a-cluster-for-cross-cluster-search",
    "href": "5-cluster-management.html#task-configure-a-cluster-for-cross-cluster-search",
    "title": "Cluster Management",
    "section": "",
    "text": "FYI: This is similar to the example at Searching Data &gt; Write and execute a query that searches across multiple clusters\n\n\nThe following instructions are for two single-node clusters running locally on your computer.\n\n\n\nConfigure the remote cluster to be searchable from the local cluster.\nEnsure secure communication between clusters.\nVerify the cross-cluster search functionality.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nConfigure the remote cluster on the local cluster\nPUT /_cluster/settings\n{\n\"persistent\": {\n    \"cluster\": {\n    \"remote\": {\n        \"es01\": {\n        \"seeds\": [\n            \"es01:9300\"\n        ],\n        \"skip_unavailable\": true\n        },\n        \"es02\": {\n        \"seeds\": [\n            \"es02:9300\"\n        ],\n        \"skip_unavailable\": false\n        }\n    }\n    }\n}\n}\n(optional if you are doing this locally) Set up security settings where you have keystores properly setup. On the remote cluster:\nPUT /_cluster/settings\n{\n    \"persistent\": {\n        \"xpack.security.enabled\": true,\n        \"xpack.security.transport.ssl.enabled\": true,\n        \"xpack.security.transport.ssl.verification_mode\": \"certificate\",\n        \"xpack.security.transport.ssl.keystore.path\": \"/path/to/keystore.jks\",\n        \"xpack.security.transport.ssl.truststore.path\": \"/path/to/truststore.jks\"\n    }\n}\nOn the local cluster:\nPUT /_cluster/settings\n{\n    \"persistent\": {\n        \"xpack.security.enabled\": true,\n        \"xpack.security.transport.ssl.enabled\": true,\n        \"xpack.security.transport.ssl.verification_mode\": \"certificate\",\n        \"xpack.security.transport.ssl.keystore.path\": \"/path/to/keystore.jks\",\n        \"xpack.security.transport.ssl.truststore.path\": \"/path/to/truststore.jks\"\n    }\n}\nVerify the remote cluster configuration\nGET /_remote/info\nIndex product documents into each cluster.\n\n\nFor es01 (potentially the local cluster):\nPOST /products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\nFor es02 (potentially the “remote” cluster) through the command line:\ncurl -u elastic:[your password here] -X POST \"http://localhost:9201/products/_bulk?pretty\" -H 'Content-Type: application/json' -d'\n{ \"index\": { \"_id\": \"5\" } }\n{ \"product\": \"Elasticsearch Stickers\", \"category\": \"Accessories\", \"price\": 4.99 }\n{ \"index\": { \"_id\": \"6\" } }\n{ \"product\": \"Elasticsearch Notebook\", \"category\": \"Stationery\", \"price\": 7.99 }\n{ \"index\": { \"_id\": \"7\" } }\n{ \"product\": \"Elasticsearch Pen\", \"category\": \"Stationery\", \"price\": 3.49 }\n{ \"index\": { \"_id\": \"8\" } }\n{ \"product\": \"Elasticsearch Hoodie\", \"category\": \"Apparel\", \"price\": 45.99 }    '\n\n\nPerform a cross-cluster search query\nGET /remote_cluster:products/_search\n{\n    \"query\": {\n        \"match_all\": {}\n    }\n}\n\n\n\n\n\nVerify the remote cluster info\nGET /_remote/info\nSearch the remote cluster from the local cluster\nGET /remote_cluster:product_catalog/_search\n{\n    \"query\": {\n        \"match_all\": {}\n    }\n}\n\n\n\n\n\nEnsure that the nodes listed in the seeds setting are accessible from the local cluster.\nSecurity settings such as SSL/TLS should be configured to ensure secure communication between clusters.\nRegularly monitor the connection status between the clusters to ensure reliability.\n\n\n\n\n\nDelete the es01 index.\nDELETE products\nDelete the es02 index from the command line.\ncurl -u elastic:[your password here] -X DELETE \"http://localhost:9201/products?pretty\"\n\n\n\n\n\nCross-Cluster Search\nCluster Remote Info API\nSearch API\nSecurity Settings"
  },
  {
    "objectID": "5-cluster-management.html#task-implement-cross-cluster-replication",
    "href": "5-cluster-management.html#task-implement-cross-cluster-replication",
    "title": "Cluster Management",
    "section": "",
    "text": "There are a number of ways to set up cross-cluster replication and they can all be found here.\nCross-cluster replication needs an Enterprise license\n\n\n\nIn this example, we will run 2 single-node clusters locally using containers (as we have for all the other examples).\n\nThe es01 container instance will be considered\n\nleader\nremote\n\nThe es02 container instance will be considered\n\nfollower\nlocal\n\n\nYou may also need to get a free 30-day trial license of certain features including cross-cluster replication. Since the second cluster is not hooked up to Kibana execute this from the command line (assuming you called the docker instance es02 as we have been using in this guide):\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X POST \"http://localhost:9201/_license/start_trial?pretty&acknowledge=true\"\n\n\n\nConfigure remote cluster settings on both leader and follower clusters.\nSet up the leader index on the leader cluster.\nConfigure the follower index on the follower cluster to replicate from the leader index.\nEnsure secure communication between clusters.\nVerify replication and data consistency.\n\n\n\n\n\nOpen the Kibana Console or use a REST client.\nConfigure the remote cluster settings on the leader cluster (es01)\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n        \"seeds\": [\n            \"es01:9300\"\n        ],\n        \"skip_unavailable\": true\n        },\n        \"es02\": {\n        \"seeds\": [\n            \"es02:9300\"\n        ],\n        \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}\nConfigure the local cluster settings on the follower cluster (es02)\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X PUT \"http://localhost:9201/_cluster/settings?pretty\" -H \"Content-Type: application/json\" -d'\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n        \"seeds\": [\n            \"es01:9300\"\n        ],\n        \"skip_unavailable\": true\n        },\n        \"es02\": {\n        \"seeds\": [\n            \"es02:9300\"\n        ],\n        \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}'\nCreate the leader index on the leader cluster (es01)\nPUT /product_catalog\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n     \"properties\": {\n     \"product_id\": {\n       \"type\": \"keyword\"\n     },\n     \"name\": {\n       \"type\": \"text\"\n     },\n     \"description\": {\n       \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\nIndex sample documents in the leader index\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 20.0 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 30.0 }\nConfigure the follower index on the follower cluster through the command line\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X PUT \"http://localhost:9201/product_catalog_follower/_ccr/follow?pretty\" -H \"Content-Type: application/json\" -d'\n{\n  \"remote_cluster\": \"es01\",\n  \"leader_index\": \"product_catalog\"\n}'\n\n\n\n\n\nVerify the follower index (es02) is following the leader index\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] \"http://localhost:9201/product_catalog_follower/_stats?pretty\"\nCheck the data in the follower index (es02) to ensure it matches the leader (es01) index\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] \"http://localhost:9201/product_catalog_follower/_search?pretty\"\n\n\n\n\n\nEnsure the nodes listed in the seeds setting are accessible from the follower cluster.\nSecurity settings such as SSL/TLS should be configured to ensure secure communication between clusters (but not for this example given the YAML changes suggested in the Appendix).\nRegularly monitor the replication status and performance to ensure data consistency and reliability.\n\n\n\n\n\nDelete the follower configuration\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X DELETE \"http://localhost:9201/product_catalog_follower?pretty\"\nDelete the index\nDELETE product_catalog\n\n\n\n\n\nCross-Cluster Replication\nCreate Follower Index API\nCluster Remote Info API\nSearch API\nSecurity Settings"
  },
  {
    "objectID": "0.1-preface.html",
    "href": "0.1-preface.html",
    "title": "Preface",
    "section": "",
    "text": "Welcome to The Definitive Guide to the Elastic Certified Engineer Exam Study Guide! This guide shiould help you get a handle on the Elastic exam as described on the certification exam landing page and perhaps even pass it the first time.\nThere are a lot of examples for the various topic areas, but the main thrust is to see Elastisearch scenarios and what possible solutions there could be to them.\nFor those of you who have not noticed, this book is available for your browsing pleasure at https://\n\n\nThis guide is the product of many weeks of work with many LLMs that I pushed and prodded into giving me as many examples that should reasonably touch on the various areas described by Elastic. Since a guide did not exist I felt compelled to create one. I hope it helps you as much as it did me."
  },
  {
    "objectID": "0.1-preface.html#acknowledgments",
    "href": "0.1-preface.html#acknowledgments",
    "title": "Preface",
    "section": "",
    "text": "This guide is the product of many weeks of work with many LLMs that I pushed and prodded into giving me as many examples that should reasonably touch on the various areas described by Elastic. Since a guide did not exist I felt compelled to create one. I hope it helps you as much as it did me."
  },
  {
    "objectID": "6-appendix.html#how-to-create-an-index-with-unassigned-shards",
    "href": "6-appendix.html#how-to-create-an-index-with-unassigned-shards",
    "title": "Appendix",
    "section": "How to Create an Index with Unassigned Shards",
    "text": "How to Create an Index with Unassigned Shards\nCreating an unassigned shard in Elasticsearch typically involves manipulating the cluster’s shard allocation settings. Here’s a step-by-step guide to achieve this:\n\nCreate an Index with Unassigned Shards:\n\nYou can create an index with a primary shard but set the number of replicas to a value higher than the number of available nodes. This will result in unassigned replica shards.\n\nPUT /my-index\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 2\n  }\n}\nIn this example, if you only have one node, the second replica will remain unassigned.\nDisable Shard Allocation Temporarily:\n\nYou can disable shard allocation to create unassigned shards intentionally.\n\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.enable\": \"none\"\n  }\n}\n\nThen create an index:\n\nPUT /my-index\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  }\n}\n\nRe-enable shard allocation:\n\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.enable\": \"all\"\n  }\n}\nManually Move Shards:\n\nYou can manually move shards to create unassigned shards using the _cluster/reroute API.\n\nPOST /_cluster/reroute\n{\n  \"commands\": [\n    {\n      \"move\": {\n        \"index\": \"my-index\",\n        \"shard\": 0,\n        \"from_node\": \"node-1\",\n        \"to_node\": \"node-2\"\n      }\n    }\n  ]\n}\nIf the target node (node-2) is not available or suitable, the shard will remain unassigned.\nCheck Unassigned Shards:\n\nUse the _cat/shards API to view unassigned shards.\n\nGET /_cat/shards?v=true&h=index,shard,prirep,state,node,unassigned.reason&s=state\n\nThese methods can help you create unassigned shards for testing or troubleshooting purposes. Always ensure you understand the implications of having unassigned shards in your cluster, as they can affect data availability and cluster performance¹².\nSource: Conversation with Copilot, 7/9/2024\n\nReferences\n[Diagnose unassigned shards | Elasticsearch Guide 8.14 How to Find and Fix Elasticsearch Unassigned Shards Elasticsearch-shard"
  }
]