[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Elastic Certified Engineer Exam v8.1 Study Guide",
    "section": "",
    "text": "Welcome\nWelcome to the official website for The Definitive Guide to the Elastic Certified Engineer Exam v8.1. This resource aims to cover the essential topics you need to know for the Elastic Certified Engineer Exam, providing practical examples and insights to help you succeed.\nAs you prepare for the exam, you’ll also discover additional aspects of Elastic that might surprise you. Embrace the learning process, as every bit of knowledge gained will enhance your skills as an Elasticsearch developer.\nThis website is hosted on GitHub and will always be free. However, please note that the Elastic exam will eventually update from Elasticsearch v8.1 to newer versions as they become available.\nIf you encounter any issues or have suggestions, please report them here. I strive to update the content promptly and appreciate your patience!\nPlease remember that the content on this website is protected and may not be copied or reproduced without permission. The code examples are concise and meant to be copied directly from the HTML pages into the Elastic Kibana Console for easy use.\nIf you find this online guide helpful, consider supporting us by purchasing a paper copy or the eBook from your favorite vendor.\nHappy studying, and best of luck on your journey to becoming an Elastic Certified Engineer!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "0.1-preface.html",
    "href": "0.1-preface.html",
    "title": "Preface",
    "section": "",
    "text": "Acknowledgments\nCreating this guide involved many weeks of dedicated effort and collaboration with numerous Large Language Models (LLMs). I pushed and prodded these models to generate a wide range of examples that comprehensively cover the areas described by Elastic. The absence of a comprehensive guide inspired me to create this resource, and I sincerely hope it proves as beneficial to you as it has to me.\nThank you for choosing this guide, and I wish you the best of luck on your journey to becoming an Elastic Certified Engineer!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "0.2-intro.html",
    "href": "0.2-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Assumptions\nThis is a very barebones guide. It will list the various topics and tasks that need to be done with a brief explanation of the example, a potential solution, an explanation for certain concepts, optional clean-up since you will be creating a number of resources in your Elastic cluster, and a list of documentation specific to the solution that should help you to understand why the solution was recommended.\nThe study guide examples assume you have a foundational understanding of search, search technologies, and Elasticsearch. It is:\nThe examples are presented as REST API calls in JSON for the Elastic Kibana Console. In the Things You Must Do section, we will show you how to translate these REST API calls into curl commands. This is to ensure you understand how to execute the calls both ways, but curl commands will not be used in the examples.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "0.2-intro.html#assumptions",
    "href": "0.2-intro.html#assumptions",
    "title": "Introduction",
    "section": "",
    "text": "Not an introductory text on search and search technologies.\nNot an Elastic or Kibana tutorial.\nNot a JSON tutorial.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "0.2-intro.html#things-you-must-do",
    "href": "0.2-intro.html#things-you-must-do",
    "title": "Introduction",
    "section": "Things You Must Do",
    "text": "Things You Must Do\nRegardless of where you are running Elastic (locally or from the Elastic Cloud), you will need two pieces of information to access your deployment:\n\nUsername\nPassword\n\nHow and where you obtain these will depend on whether you have a local instance of Elasticsearch/Kibana or are using an Elastic Cloud deployment.\nInstructions for installing a local instance of Elasticsearch/Kibana can be found in the Appendix, along with basic instructions on obtaining the username/password when you create an Elastic Cloud deployment. These instructions are also available in the Elastic documentation.\nIf you decide to run these examples from the command line using curl, you must:\n\nHave curl installed.\nHave your Elasticsearch username (elastic) and password handy. The password will vary depending on whether you are running your own local copy of the Elastic Stack or using the Elastic Cloud.\nIf you are running curl against your local container instance, your command line should look like this:\n\ncurl --cacert http_ca.crt -u elastic:[container instance password here] -X [desired HTTP command] \"https://[elastic endpoint here]/[API path as appropriate]\"\nThe http_ca.crt file should be extracted from the container during deployment. If you haven’t done this, execute the following command in the location where you are doing your certification work (assuming your Elasticsearch container is named es01):\ndocker cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .\nKeep that file secret. Keep that file safe.\nAs a test, you should be able to run:\ncurl --cacert http_ca.crt -u elastic:[container instance password here] https://localhost:9200/\nYou should get some reasonable output.\n\nIf you are running curl against the Elastic Cloud, your command line should look like this:\n\ncurl -u elastic:[elastic cloud deployment password here] -X [desired HTTP command] \"https://[elastic endpoint here]/[API path as appropriate]\"\nThe only difference between the above and the local command is that you don’t need the certificate file if you are running curl against the Elastic Cloud.\nOriginally, the examples included both the REST API calls and the curl commands. Since the curl commands differ slightly between the local instance and the Elastic Cloud instance, they have been left out. If you want to run curl using the Elastic REST API, remember that the REST API looks like this in the Elastic Cloud console:\nGET / \nor\nPUT /example_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\nThe associated curl command would look like this:\n\nLocal\n\nThe first call:\ncurl --cacert http_ca.crt -u elastic:[container instance password here] -X GET https://localhost:9200/\nor for the second call (note the use of single quotes around the JSON):\ncurl --cacert http_ca.crt -u elastic:[container instance password here] -X PUT https://localhost:9200/example_index -H 'Content-Type: application/json' -d'\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}'\n\nElastic Cloud\n\nThe first call:\ncurl -u elastic:[elastic cloud deployment password here] -X GET \"https://[elastic endpoint here]/\"\nor for the second call (note the use of single quotes around the JSON):\ncurl -u elastic:[elastic cloud deployment password here] -X PUT \"[elastic endpoint here]\" -H 'Content-Type: application/json' -d'\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}'",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "0.2-intro.html#how-to-pass-the-test",
    "href": "0.2-intro.html#how-to-pass-the-test",
    "title": "Introduction",
    "section": "How to Pass the Test",
    "text": "How to Pass the Test\nDo the examples and more. Be over-prepared.\nGo through the documentation so you know where to look when a task shows up in the certification exam and you are not sure what the syntax or format of a given JSON might be. This is an open book test, but the only book you can use is the Elastic documentation.\nLearn the basics. The Elastic console has code completion, so you don’t have to remember everything—just what might be the appropriate JSON elements for the solution you are implementing.\nLearn the query syntax of Query DSL. This is a search engine, after all, and knowledge of querying is fundamental to the certification exam.\nWhile there is no magic bullet, the exam should not be that hard if you already have knowledge of:\n\nSearch and search technologies (preferably hands-on)\nJSON\n\nThings that will trip you up:\n\nSyntax\nDepth of various JSON nodes",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "0.2-intro.html#summary",
    "href": "0.2-intro.html#summary",
    "title": "Introduction",
    "section": "Summary",
    "text": "Summary\nThis book will not teach you Elasticsearch or Kibana or any of the other Elastic products available in the first half of 2024. It assumes you have an understanding of various search topics, JSON, REST APIs, and areas like regular expressions and web technologies. The examples are valid with the current documentation and were all confirmed as working in the same time period.\nIf you run into any problems with the examples, please send an email to support@brushedsteelconsulting.com. When in doubt, asking in the Elastic community or one of the many public LLMs available should help as well:\n\nChatGPT\nClaude.ai\nGemini\nMeta.ai\nPerplexity.ai\n\nAll of the examples were originally generated by the various LLMs listed above with many changes made by an actual human as the examples and the generated content left much to be desired.\nDisclaimer: this book was written with the assistance of various tools, including a host of LLMs, but always under the guidance of the author. Make of that what you will.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1-data-management.html",
    "href": "1-data-management.html",
    "title": "1  Data Management",
    "section": "",
    "text": "1.1 Task: Define an index that satisfies a given set of requirements",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "1-data-management.html#task-define-an-index-that-satisfies-a-given-set-of-requirements",
    "href": "1-data-management.html#task-define-an-index-that-satisfies-a-given-set-of-requirements",
    "title": "1  Data Management",
    "section": "",
    "text": "Example 1: Creating an Index for a Blogging Platform\n\nRequirements\n\nThe platform hosts articles, each with text content, a publication date, author details, and tags.\nArticles need to be searchable by content, title, and tags.\nThe application requires fast search responses and efficient storage.\nThe application should handle date-based queries efficiently.\nAuthor details are nested objects that include the author’s name and email.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nDefine Mappings:\n\nContent and Title: Use the text data type\nPublication Date: Use the date data type\nTags: Use the keyword data type for exact matching\nAuthor: Use a nested object to keep author details searchable and well-structured\n\nCreate the index\nPUT blog_articles\n{\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"content\": {\n        \"type\": \"text\"\n      },\n      \"publication_date\": {\n        \"type\": \"date\"\n      },\n      \"author\": {\n        \"type\": \"nested\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"text\"\n          },\n          \"email\": {\n            \"type\": \"keyword\"\n          }\n        }\n      },\n      \"tags\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\nOr insert the settings and mappings separately.\nPUT blog_articles\n{\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 1\n  }\n}\nAnd:\nPUT blog_articles/_mapping\n{\n  \"properties\": {\n    \"title\": {\n      \"type\": \"text\"\n    },\n    \"content\": {\n      \"type\": \"text\"\n    },\n    \"publication_date\": {\n      \"type\": \"date\"\n    },\n    \"author\": {\n      \"type\": \"nested\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"text\"\n        },\n        \"email\": {\n          \"type\": \"keyword\"\n        }\n      }\n    },\n    \"tags\": {\n      \"type\": \"text\"\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index\nGET /_cat/indices\nVerify the mappings\nGET /blog_articles/_mapping\nIndex and search for a document\n# Index\nPOST /blog_articles/_doc\n{\n  \"title\" : \"My First Blog Post\",\n  \"content\" : \"What an interesting way to go...\",\n  \"publication_date\" : \"2024-05-15\",\n  \"tags\" : \"superb\",\n  \"author\" : {\n    \"name\" : \"John Doe\",\n    \"email\" : \"john@doe.com\"\n  }\n}\n# Search like this\nGET /blog_articles/_search\n# Or search like this\nGET /blog_articles/_search?q=tags:superb\n# Or search like this\nGET blog_articles/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"default_field\": \"tags\",\n      \"query\": \"superb\"\n    }\n  }\n}\n# Or search like this\nGET blog_articles/_search\n{\n  \"query\": {\n    \"nested\": {\n      \"path\": \"author\",\n      \"query\": {\n        \"match\": {\n          \"author.name\": \"john\"\n        }\n      }\n    }\n  }\n}\nConsiderations\n\n\nShards and Replicas: Adjust these settings based on expected data volume and query load.\nNested Objects: These are crucial for maintaining the structure and searchability of complex data like author details.\n\n\n\nClean-up (optional)\n\nIn the console execute the following\nDELETE blog_articles\n\n\n\nDocumentation\n\ncat indices API\nCreate Index API\nGet mapping API\nSearch API\n\n\n\n\nExample 2: Creating an Index for Log Data\n\nRequirements\n\nStore log data with a timestamp field\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the index\nPUT /log_data\n{\n  \"settings\": {\n    \"number_of_shards\": 3\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"@timestamp\": {\n        \"type\": \"date\"\n      },\n      \"log_source\": {\n        \"type\": \"keyword\"\n      },\n      \"message\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index creation\nGET /log_data\nOr\nGET /_cat/indices\nVerify the field mapping\nGET /log_data/_mapping\nIndex and search for a sample document\n\nIndex\nPUT /log_data/_doc/1\n{\n  \"@timestamp\": \"2023-05-16T12:34:56Z\",\n  \"log_source\": \"web_server\",\n  \"message\": \"HTTP request received\"\n}\nSearch\nGET /log_data/_search\nThe response should show the indexed document.\n\n\n\n\nConsiderations\n\nIn settings, number_of_replicas doesn’t appear as its default is set to one 1 which is sufficient. The field number_of_shards should be higher than 1 depending on the requirements for a log index. No, you do not need to have a settings block for the index to be created.\nThe @timestamp field is mapped as a date type for time-based data management.\nThe log_source field is mapped as a keyword type to enable custom routing based on its value.\n\n\n\nClean-up (optional)\n\nIn the console execute the following\nDELETE log_data \n\n\n\nDocumentation\n\ncat indices API\nCreate Index API\nGet mapping API\nSearch API\n\n\n\n\nExample 3: Creating an index for e-commerce product data with daily updates\n\nRequirements\n\nStore product information including name, description, category, price, and stock_level.\nAllow filtering and searching based on product name, category, and price range.\nEnable aggregations to calculate average price per category.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nDefine mappings:\n\nUse the text data type for name and description to allow full-text search.\nUse the keyword data type for category to enable filtering by exact terms.\nUse the integer data type for price to allow for range queries and aggregations.\nUse the integer data type for stock_level for inventory management.\n\nCreate the index\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": { \"type\": \"text\" },\n      \"description\": { \"type\": \"text\" },\n      \"category\": { \"type\": \"keyword\" },\n      \"price\": { \"type\": \"integer\" },\n      \"stock_level\": { \"type\": \"integer\" }\n    }\n  }\n}\n\n\nConfigure analyzers (optional):\n\nYou can define custom analyzers for name and description to handle special characters or stemming based on your needs. Notice two things:\n\nHow the custom_analyzer refers to the filter and tokenizer (both of which are optional).\nThe fields that will use custom_analyzer, name and description, have an analyzer reference to custom_analyzer.\nPUT /products\n{\n  \"settings\": {\n    \"analysis\": {\n      \"tokenizer\": {\n        \"custom_tokenizer\": {\n          \"type\": \"standard\"\n        }\n      },\n      \"filter\": {\n        \"custom_stemmer\": {\n          \"type\": \"stemmer\",\n          \"name\": \"english\"\n        },\n        \"custom_stop\": {\n          \"type\": \"stop\",\n          \"stopwords\": \"_english_\"\n        }\n      },\n      \"analyzer\": {\n        \"custom_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"custom_tokenizer\",\n          \"filter\": [\n            \"lowercase\",\n            \"custom_stop\",\n            \"custom_stemmer\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      },\n      \"description\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"integer\"\n      },\n      \"stock_level\": {\n        \"type\": \"integer\"\n      }\n    }\n  }\n}\n\n\n\n\n\nTest\n\nVerify the index creation\nGET products\nOr\nGET /_cat/indices\nVerify the field mapping\nGET /products/_mapping\nIndex and search some sample product data\n\nIndex some products\nPOST /products/_bulk\n{ \"index\": { \"_index\": \"products\", \"_id\": \"1\" } }\n{ \"name\": \"Wireless Bluetooth Headphones\", \"description\": \"High-quality wireless Bluetooth headphones with noise-cancellation and long battery life.\", \"category\": \"electronics\", \"price\": 99, \"stock_level\": 250 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"2\" } }\n{ \"name\": \"Stainless Steel Water Bottle\", \"description\": \"Durable stainless steel water bottle, keeps drinks cold for 24 hours and hot for 12 hours.\", \"category\": \"home\", \"price\": 25, \"stock_level\": 500 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"3\" } }\n{ \"name\": \"Smartphone\", \"description\": \"Latest model smartphone with high-resolution display and fast processor.\", \"category\": \"electronics\", \"price\": 699, \"stock_level\": 150 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"4\" } }\n{ \"name\": \"LED Desk Lamp\", \"description\": \"Energy-efficient LED desk lamp with adjustable brightness and flexible neck.\", \"category\": \"home\", \"price\": 45, \"stock_level\": 300 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"5\" } }\n{ \"name\": \"4K Ultra HD TV\", \"description\": \"55-inch 4K Ultra HD TV with HDR support and smart features.\", \"category\": \"electronics\", \"price\": 499, \"stock_level\": 200 }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"6\" } }\n{ \"name\": \"Vacuum Cleaner\", \"description\": \"High-suction vacuum cleaner with multiple attachments for versatile cleaning.\", \"category\": \"home\", \"price\": 120, \"stock_level\": 100 }\nSearch\nGET /products/_search?q=name:desk\n\nUse aggregations to calculate the average price per category.\nPOST /products/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"average_price_per_category\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\nConsiderations\n\nUsing the appropriate data types ensures efficient storage and querying capabilities.\nText fields allow full-text search, while keyword fields enable filtering by exact terms.\n\n\n\nClean-up (optional)\n\nIn the console execute the following\nDELETE products\n\n\n\nDocumentation\n\nAggregations\ncat indices API\nCreate Index API\nGet mapping API\nSearch API",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "1-data-management.html#task-define-and-use-an-index-template-for-a-given-pattern-that-satisfies-a-given-set-of-requirements",
    "href": "1-data-management.html#task-define-and-use-an-index-template-for-a-given-pattern-that-satisfies-a-given-set-of-requirements",
    "title": "1  Data Management",
    "section": "1.2 Task: Define and use an index template for a given pattern that satisfies a given set of requirements",
    "text": "1.2 Task: Define and use an index template for a given pattern that satisfies a given set of requirements\n\nExample 1: Creating an index template for a user profile data\n\nRequirements\n\nCreate an index template named user_profile_template.\nThe template should apply to indices starting with user_profile-.\nThe template should have two shards and one replica.\nThe template should have a mapping for the name field as a text data type with an analyzer of standard.\nThe template should have a mapping for the age field as an integer data type.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\nPUT /_index_template/user_profile_template\n{\n  \"index_patterns\": [\"user_profile-*\"],\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 2,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"name\": {\n          \"type\": \"text\",\n          \"analyzer\": \"standard\"\n        },\n        \"age\": {\n          \"type\": \"integer\"\n        }\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index template was created\nGET _index_template/user_profile_template\nCreate an index named user_profile-2024 using the REST API:\nPUT /user_profile_2024\nVerify that the index was created with the expected settings and mappings:\nGET /user_profile_2024/_settings\nGET /user_profile_2024/_mapping\n\n\n\nConsiderations\n\nTwo shards are chosen to allow for parallel processing and improved search performance.\nOne replica is chosen for simplicity and development purposes; in a production environment, this would depend on the expected data volume and search traffic.\nThe standard analyzer is chosen for the name field to enable standard text analysis.\n\n\n\nClean-up (optional)\n\nIn the console execute the following\nDELETE /user_profile_2024\nDELETE /_index_template/user_profile_template\n\n\n\nDocumentation\n\nAnalyzers\nCreate Index API\nIndex templates\n\n\n\n\nExample 2: Creating a monthly product index template\n\nRequirements\n\nIndex name pattern: products-*\nIndex settings:\n\nNumber of shards: 3\nNumber of replicas: 2\n\nMapping:\n\nField name should be of type text\nField description should be of type text\nField price should be of type float\nField category should be of type keyword\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\nPUT _template/monthly_products\n{\n  \"index_patterns\": [\"products-*\"],\n  \"settings\": {\n    \"number_of_shards\": 3,\n    \"number_of_replicas\": 2\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index template was created\nGET _index_template/monthly_products\nCreate a new index matching the pattern (e.g., products-202305):\nPUT products-202305\nVerify that the index was created with the expected settings and mappings:\nGET /products-202305/_settings\nGET /products-202305/_mapping\nIndex a sample document and verify that the mapping is applied correctly:\n\nIndex\nPOST products-202305/_doc\n{\n  \"name\": \"Product A\",\n  \"description\": \"This is a sample product\",\n  \"price\": 19.99,\n  \"category\": \"Electronics\"\n}\nSearch\nGET products-202305/_search\n\n\nThe response should show the correct mapping for the fields specified in the index template.\n\n\nConsiderations\n\nThe index_patterns field specifies the pattern for index names to which this template should be applied.\nThe number_of_shards and number_of_replicas settings are chosen based on the expected data volume and high availability requirements.\nThe text type is used for name and description fields to enable full-text search and analysis.\nThe float type is used for the price field to support decimal values.\nThe keyword type is used for the category field to prevent analysis and treat the values as exact matches.\n\n\n\nClean-up (optional)\n\nIn the console execute the following\nDELETE products-202305\nDELETE _template/monthly_products\n\n\n\nDocumentation\n\nCreate Index API\nIndex templates\nSearch API\n\n\n\n\nExample 3: Creating an index template for log indices\n\nRequirements\n\nThe template should apply to any index starting with logs-.\nThe template must define settings for three primary shards and one replica.\nThe template should include mappings for fields @timestamp, log_level, and message.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\nPUT /_index_template/logs_template\n{\n  \"index_patterns\": [\"logs-*\"],\n  \"template\": {\n    \"settings\": {\n      \"index\": {\n        \"number_of_shards\": 3,\n        \"number_of_replicas\": 1\n      }\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\"\n        },\n        \"log_level\": {\n          \"type\": \"keyword\"\n        },\n        \"message\": {\n          \"type\": \"text\"\n        }\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index template was created\nGET _index_template/logs_template\nCreate a new index matching the pattern (e.g., logs-202405)\nPUT logs-202405\nVerify that the index was created with the expected settings and mappings\nGET /logs-202405/_settings\nGET /logs-202405/_mapping\nIndex a sample document and verify that the mapping is applied correctly:\n\nIndex\nPOST logs-202405/_doc\n{\n  \"@timestamp\": \"2024-05-16T12:34:56Z\",\n  \"log_level\": \"ERROR\",\n  \"message\": \"Help!\"\n}\nSearch\nGET logs-202405/_search\n\n\nThe response should show the correct mapping for the fields specified in the index template.\n\n\nConsiderations\n\nIndex Patterns: The template applies to any index starting with logs-, ensuring consistency across similar indices.\nNumber of Shards: Three shards provide a balance between performance and resource utilization.\nReplicas: A single replica ensures high availability and fault tolerance.\nMappings: Predefined mappings ensure that the fields are properly indexed and can be efficiently queried.\n\n\n\nClean-up (optional)\n\nIn the console execute the following\nDELETE logs-202405\nDELETE _index_template/logs_template\n\n\n\nDocumentation\n\nCreate Index API\nLogstash: Event Dependent Configuration\nIndex templates\nSearch API",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "1-data-management.html#task-define-and-use-a-dynamic-template-that-satisfies-a-given-set-of-requirements",
    "href": "1-data-management.html#task-define-and-use-a-dynamic-template-that-satisfies-a-given-set-of-requirements",
    "title": "1  Data Management",
    "section": "1.3 Task: Define and use a dynamic template that satisfies a given set of requirements",
    "text": "1.3 Task: Define and use a dynamic template that satisfies a given set of requirements\nFYI: The difference between index templates and dynamic templates is:\nAn index template is a way to define settings, mappings, and other configurations that should be applied automatically to new indices when they are created.  A dynamic template is part of the mapping definition within an index template or index mapping that allows Elasticsearch to dynamically infer the mapping of fields based on field names, data patterns, or the data type detected.\nThere is one example per field mapping type. They all use an explicit dynamic template, but Exercise 1 also shows the use of a dynamic template embedded in the index definition.\n\nExample 1: Create a Dynamic Template for Logging Using Field Name Patterns\n\nRequirements\n\nApply a specific text analysis to all fields that end with _log.\nUse a keyword type for all fields that start with status_.\nDefault to text with a standard analyzer for other string fields.\nDefine a custom log_analyzer for _log fields.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nDefine the dynamic template\n\n\nAs part of the index definition\nPUT /logs_index\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"log_fields\": {\n          \"match\": \"*_log\",\n          \"mapping\": {\n            \"type\": \"text\",\n            \"analyzer\": \"log_analyzer\"\n          }\n        }\n      },\n      {\n        \"status_fields\": {\n          \"match\": \"status_*\",\n          \"mapping\": {\n            \"type\": \"keyword\"\n          }\n        }\n      },\n      {\n        \"default_string\": {\n          \"match_mapping_type\": \"string\",\n          \"mapping\": {\n            \"type\": \"text\",\n            \"analyzer\": \"standard\"\n          }\n        }\n      }\n    ]\n  },\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"log_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\"lowercase\", \"stop\"]\n        }\n      }\n    }\n  }\n}\n\nor as a standalone definition to be added to indexes as needed using the index_pattern\n\nPUT /_index_template/logs_dyn_template\n{\n  \"index_patterns\": [\"logs_*\"],\n  \"template\": {\n    \"mappings\": {\n      \"dynamic_templates\": [\n        {\n          \"log_fields\": {\n            \"match\": \"*_log\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"log_analyzer\"\n            }\n          }\n        },\n        {\n          \"status_fields\": {\n            \"match\": \"status_*\",\n            \"mapping\": {\n              \"type\": \"keyword\"\n            }\n          }\n        },\n        {\n          \"default_string\": {\n            \"match_mapping_type\": \"string\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"standard\"\n            }\n          }\n        }\n      ]\n    },\n    \"settings\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"log_analyzer\": {\n            \"type\": \"custom\",\n            \"tokenizer\": \"standard\",\n            \"filter\": [\"lowercase\", \"stop\"]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the dynamic template was created\n\nIf you used the embedded version\n\nGET /logs_index/_mapping\n\nIf you used the standalone version\n\nGET /_index_template/logs_dyn_template\nCreate a new index matching the pattern (e.g., logs-202405)\n\nOptional if you used the embedded version\n\nPUT logs_index\nVerify that the created index has the expected settings and mappings\n\nEnsure error_log is of type text with log_analyzer\nEnsure status_code is of type keyword\nEnsure message is of type text with standard analyzer\n\nGET /logs_index/_mapping\nIndex a sample document and verify that the mapping is applied correctly\nPOST /logs_index/_doc/1\n{\n  \"error_log\": \"This is an error log message.\",\n  \"status_code\": \"200\",\n  \"message\": \"Regular log message.\"\n}\nPerform Searches:\n\nSearch within error_log and verify the custom analyzer is applied\n\nGET /logs_index/_search\n{\n  \"query\": {\n    \"match\": {\n      \"error_log\": \"error\"\n    }\n  }\n}\n\nCheck if status_code is searchable as a keyword\n\nGET /logs_index/_search\n{\n  \"query\": {\n    \"term\": {\n      \"status_code\": \"200\"\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe custom analyzer log_analyzer is used to provide specific tokenization and filtering for log fields.\nThe keyword type for status_* fields ensures they are treated as exact values, useful for status codes.\nThe default_string template ensures other string fields are analyzed with the standard analyzer, providing a balanced default.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE logs_index\nDelete the dynamic template\nDELETE /_index_template/logs_dyn_template\n\n\n\nDocumentation\n\nDynamic Templates\nCustom Analyzers\nPut Mapping API\nIndex API\nSearch API\n\n\n\n\nExample 2: Create Dynamic Template for Data Types\n\nRequirements\n\nAll string fields should be treated as text with a standard analyzer.\nAll long fields should be treated as integer.\nAll date fields should use a specific date format.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nDefine the Dynamic Template\nPUT /_index_template/data_type_template\n{\n  \"index_patterns\": [\"data_type_*\"],\n  \"template\": {\n    \"mappings\": {\n      \"dynamic_templates\": [\n        {\n          \"strings_as_text\": {\n            \"match_mapping_type\": \"string\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"standard\"\n            }\n          }\n        },\n        {\n          \"longs_as_integer\": {\n            \"match_mapping_type\": \"long\",\n            \"mapping\": {\n              \"type\": \"integer\"\n            }\n          }\n        },\n        {\n          \"dates_with_format\": {\n            \"match_mapping_type\": \"date\",\n            \"mapping\": {\n              \"type\": \"date\",\n              \"format\": \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nTest\n\nVerify the dynamic template was created\nGET /_index_template/data_type_template\nCreate a new index matching the pattern\nPUT data_type_202405\nCheck the Field Types\n\nVerify that all string fields are mapped as text with the standard analyzer.\nVerify that all long fields are mapped as integer.\nVerify that all date fields are mapped with the correct format.\n\nGET /data_type_202405/_mapping\nInsert sample documents to ensure that the dynamic template is applied correctly\nPOST /data_type_202405/_bulk\n{ \"index\": { \"_index\": \"data_type_202405\", \"_id\": \"1\" } }\n{ \"name\": \"Wireless Bluetooth Headphones\", \"release_date\": \"2024-05-28T14:35:00.000Z\", \"price\": 99 }\n{ \"index\": { \"_index\": \"data_type_202405\", \"_id\": \"2\" } }\n{ \"description\": \"Durable stainless steel water bottle\", \"launch_date\": \"2024-05-28T15:00:00.000Z\", \"quantity\": 500 }\nPerform Searches\n\nSearch launch_date\n\nGET /data_type_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"launch_date:\\\"2024-05-28T15:00:00.000Z\\\"\"\n    }\n  }\n}\n\nCheck if price is searchable as a value\n\nGET /data_type_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"price: 99\"\n    }\n  }\n}\n\n\n\nConsiderations\n\nDynamic Templates: Using dynamic templates based on data types allows for flexible and consistent field mappings without needing to know the exact field names in advance.\nData Types: Matching on data types (string, long, date) ensures that fields are mapped appropriately based on their content.\nDate Format: Specifying the date format ensures that date fields are parsed correctly, avoiding potential issues with date-time representation.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE data_type_202405\nDelete the dynamic template\nDELETE /_index_template/data_type_template\n\n\n\nDocumentation\n\nDynamic Templates\nMapping\nAnalyzers\n\n\n\n\nExample 3: Create a Dynamic Template for Logging Data for Data Patterns\n\nRequirements\n\nAutomatically map fields that end with “_ip” as IP type.\nMap fields that start with “timestamp_” as date type.\nMap any field containing the word “keyword” as a keyword type.\nUse a custom analyzer for fields ending with “_text”.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the dynamic template\nPUT /_index_template/logs_template\n{\n  \"index_patterns\": [\"logs*\"],\n  \"template\": {\n    \"settings\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"custom_analyzer\": {\n            \"type\": \"standard\",\n            \"stopwords\": \"_english_\"\n          }\n        }\n      }\n    },\n    \"mappings\": {\n      \"dynamic_templates\": [\n        {\n          \"ip_fields\": {\n            \"match\": \"*_ip\",\n            \"mapping\": {\n              \"type\": \"ip\"\n            }\n          }\n        },\n        {\n          \"date_fields\": {\n            \"match\": \"timestamp_*\",\n            \"mapping\": {\n              \"type\": \"date\"\n            }\n          }\n        },\n        {\n          \"keyword_fields\": {\n            \"match\": \"*keyword*\",\n            \"mapping\": {\n              \"type\": \"keyword\"\n            }\n          }\n        },\n        {\n          \"text_fields\": {\n            \"match\": \"*_text\",\n            \"mapping\": {\n              \"type\": \"text\",\n              \"analyzer\": \"custom_analyzer\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nTest\n\nVerify the dynamic template was created\nGET /_index_template/logs_template\nCreate a new index matching the pattern\nPUT logs_202405\nCheck the Field Types\n\nVerify that all _ip fields are mapped as ip\nVerify that all timestamp_ fields are mapped as date\nVerify that all fields that contain the string keyword are mapped as keyword\n\nGET /logs_202405/_mapping\nInsert sample documents to ensure that the dynamic template is applied correctly\nPOST /logs_202405/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"source_ip\": \"192.168.1.1\", \"timestamp_event\": \"2024-05-28T12:00:00Z\", \"user_keyword\": \"elastic\", \"description_text\": \"This is a log entry.\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"destination_ip\": \"10.0.0.1\", \"timestamp_access\": \"2024-05-28T12:05:00Z\", \"log_keyword\": \"search\", \"details_text\": \"Another log entry.\" }\nPerform Searches\n\nSearch source_ip\n\nGET /logs_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"source_ip:\\\"192.168.1.1\\\"\"\n    }\n  }\n}\n\nCheck if timestamp_event is searchable as a date\n\nGET /logs_202405/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"timestamp_event:\\\"2024-05-28T12:00:00Z\\\"\"\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe use of patterns in the dynamic template ensures that newly added fields matching the criteria are automatically mapped without the need for manual intervention.\nCustom analyzer configuration is critical for ensuring text fields are processed correctly, enhancing search capabilities.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE logs_202405\nDelete the dynamic template\nDELETE /_index_template/logs_template\n\n\n\nDocumentation\n\nDynamic Templates\nMapping Types\nAnalyzers",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "1-data-management.html#task-define-an-index-lifecycle-management-policy-for-a-timeseries-index",
    "href": "1-data-management.html#task-define-an-index-lifecycle-management-policy-for-a-timeseries-index",
    "title": "1  Data Management",
    "section": "1.4 Task: Define an Index Lifecycle Management policy for a timeseries index",
    "text": "1.4 Task: Define an Index Lifecycle Management policy for a timeseries index\n\nExample 1: Creating an ILM policy for log data indices\n\nRequirements\n\nIndices are prefixed with logstash-\nIndices should be rolled over daily (create a new index every day).\nOld indices should be deleted after 30 days.\n\n\n\nSteps using the Elastic/Kibana UI\n\nOpen the hamburger menu and click on Management &gt; Data &gt; Life Cycle Policies.\nPress + Create New Policy.\nEnter the following:\n\nPolicy name: logstash-example-policy.\nHot phase:\n\nChange Keep Data in the Phase Forever (the infinity icon) to Delete Data After This Phase (the trashcan icon).\nClick Advanced Settings.\nUnselect Use Recommended Defaults.\nSet Maximum Age to 1.\n\nDelete phase:\n\nMove data into phase when: 30 days old.\n\n\nPress Save Policy.\nOpen the Kibana Console or use a REST client.\nCreate an index template that will match on indices that match the pattern logstash-*.\nPUT /_index_template/ilm_logstash_index_template\n{\n  \"index_patterns\": [\"logstash-*\"]\n}\nReturn to the Management &gt; Data &gt; Life Cycle Policies page.\nPress the plus sign (+) to the right of logstash-example-policy.\n\nThe Add Policy “logstash-example-policy” to index template dialog opens.\nClick on the Index Template input field and type the first few letters of the index template created above.\nSelect the template created above (ilm_logstash_index_template).\nPress Add Policy.\n\nOpen the Kibana Console or use a REST client.\nList ilm_logs_index_template. Notice the ILM policy is now part of the index template.\nGET /_index_template/ilm_logstash_index_template\nOutput from the GET:\n{\n  \"index_templates\": [\n    {\n      \"name\": \"ilm_logstash_index_template\",\n      \"index_template\": {\n        \"index_patterns\": [\"logstash-*\"],\n        \"template\": {\n          \"settings\": {\n            \"index\": {\n              \"lifecycle\": {\n                \"name\": \"logstash-example-policy\"\n              }\n            }\n          }\n        },\n        \"composed_of\": []\n      }\n    }\n  ]\n}\nCreate an index.\nPUT logstash-2024.05.16\nVerify the policy is there.\nGET logstash-2024.05.16\nThe output should look something like this:\n{\n  \"logstash-2024.05.16\": {\n    \"aliases\": {},\n    \"mappings\": {},\n    \"settings\": {\n      \"index\": {\n        \"lifecycle\": {\n          \"name\": \"logstash-example-policy\"\n        },\n        \"routing\": {\n          \"allocation\": {\n            \"include\": {\n              \"_tier_preference\": \"data_content\"\n            }\n          }\n        },\n        \"number_of_shards\": \"1\",\n        \"provided_name\": \"logstash-2024.05.16\",\n        \"creation_date\": \"1717024100387\",\n        \"priority\": \"100\",\n        \"number_of_replicas\": \"1\",\n        \"uuid\": \"mslAKuZGTpSDdFr4hSpAAA\",\n        \"version\": {\n          \"created\": \"8503000\"\n        }\n      }\n    }\n  }\n}\n\n\n\nSteps Using the REST API (which I would not recommend)\n\nOpen the Kibana Console or use a REST client.\nCreate the ILM policy.\nPUT _ilm/policy/logstash-example-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_age\": \"1d\"\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\nCreate an index template that includes the above policy. The two fields within settings are required.\nPUT /_index_template/ilm_logstash_index_template\n{\n  \"index_patterns\": [\"logstash-*\"],\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"logstash-example-policy\",\n      \"index.lifecycle.rollover_alias\": \"logstash\"\n    }\n  }\n}\n\n\n\nTest\n\nVerify the ILM policy exists in Kibana under Management &gt; Data &gt; Index Lifecycle Policies.\nVerify the Index Lifecycle Management policy exists and references the index template.\nGET /_ilm/policy/logstash-example-policy\nVerify the policy is referenced in the index template.\nGET /_index_template/ilm_logstash_index_template\nCreate a new index that matches the pattern logstash-*.\nPUT /logstash-index\nVerify the index has the policy in its definition.\nGET /logstash-index\n\n\n\nConsiderations\n\nThe index template configures 1 shard and the ILM policy/alias for rollover.\nThe rollover action creates a new index when the max_age is reached.\nThe delete phase removes indices older than 30 days.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE logstash-index\nDelete the index template.\nDELETE /_index_template/ilm_logstash_index_template\nDelete the policy.\nDELETE /_ilm/policy/logstash-example-policy\n\n\n\nDocumentation\n\nCreate Index API\nCreate or Update Index Template API\nILM Settings\nIndex Lifecycle Management\n\n\n\n\nExample 2: Creating an ILM policy for logs indices retention for 7, 30 and 90 days\n\nRequirements\n\nThe policy should be named logs-policy.\nIt should have a hot phase with a duration of 7 days.\nIt should have a warm phase with a duration of 30 days.\nIt should have a cold phase with a duration of 90 days.\nIt should have a delete phase.\nThe policy should be assigned to indices matching the pattern ilm_logs_*.\n\nSteps using the Elastic/Kibana UI\n\nOpen the hamburger menu and click on Management &gt; Data &gt; Life Cycle Policies.\nPress + Create New Policy.\nEnter the following:\n\nPolicy name: logs-policy.\nHot phase:\n\nPress the garbage can icon to the right to delete data after this phase.\n\nWarm phase:\n\nMove data into phase when: 7 days old.\nLeave Delete data after this phase.\n\nCold phase:\n\nMove data into phase when: 30 days old.\nLeave Delete data after this phase.\n\nDelete phase:\n\nMove data into phase when: 90 days old.\n\n\nPress Save Policy.\nOpen the Kibana Console or use a REST client.\nCreate an index template that will match on indices that match the pattern ilm_logs_*.\nPUT /_index_template/ilm_logs_index_template\n{\n  \"index_patterns\": [\"ilm_logs_*\"]\n}\nReturn to the Management &gt; Data &gt; Life Cycle Policies page.\nPress the plus sign (+) to the right of logs_policy.\nThe Add Policy “logs-policy” to index template dialog opens.\nClick on the Index Template input field and type the first few letters of the index template created above.\nSelect the template created above (ilm_logs_index_template).\nPress Add Policy.\nOpen the Kibana Console or use a REST client.\nList ilm_logs_index_template. Notice the ILM policy is now part of the index template.\nGET /_index_template/ilm_logs_index_template\nOutput from the GET (look for the settings/index/lifecycle node):\n{\n  \"index_templates\": [\n    {\n      \"name\": \"ilm_logs_index_template\",\n      \"index_template\": {\n        \"index_patterns\": [\"ilm_logs_*\"],\n        \"template\": {\n          \"settings\": {\n            \"index\": {\n              \"lifecycle\": {\n                \"name\": \"logs-policy\"\n              }\n            }\n          }\n        },\n        \"composed_of\": []\n      }\n    }\n  ]\n}\nList logs-policy.\nGET _ilm/policy/logs-policy\nIn the in_use_by node you will see:\n\"in_use_by\": {\n  \"indices\": [],\n  \"data_streams\": [],\n  \"composable_templates\": [\n    \"ilm_logs_index_template\"\n  ]\n}\n\n\n\nSteps Using the REST API (which I would not recommend)\n\nOpen the Kibana Console or use a REST client.\nCreate the ILM policy.\nPUT _ilm/policy/logs-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"min_age\": \"0ms\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 100\n          }\n        }\n      },\n      \"warm\": {\n        \"min_age\": \"7d\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 50\n          }\n        }\n      },\n      \"cold\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 0\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"90d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\nAssign the policy to the indices matching the pattern “logs_*“.\nPUT /_index_template/ilm_logs_index_template\n{\n  \"index_patterns\": [\"ilm_logs_*\"],\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"logs-policy\",\n      \"index.lifecycle.rollover_alias\": \"logs\"\n    }\n  }\n}\n\n\n\nTest\n\nVerify the ILM policy exists in Kibana under Management &gt; Data &gt; Index Lifecycle Policies.\nVerify the Index Lifecycle Management policy exists and references the index template.\nGET /_ilm/policy/logs-policy\nVerify the policy is referenced in the index template.\nGET /_index_template/ilm_logs_index_template\nCreate a new index that matches the pattern ilm_logs_*.\nPUT /ilm_logs_index\nVerify the index has the policy in its definition.\nGET /ilm_logs_index\n\n\n\nConsiderations\n\nThe ILM policy will manage the indices matching the pattern ilm_logs_*.\nThe hot phase will keep the data for 7 days with high priority and rollover.\nThe warm phase will keep the data for 30 days with medium priority.\nThe cold phase will keep the data for 90 days with low priority.\nThe ILM policy will automatically manage the indices based on their age and size.\nThe policy can be adjusted based on the needs of the application and the data.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE ilm_logs_index\nDelete the index template.\nDELETE _index_template/ilm_logs_index_template\nDelete the policy.\nDELETE _ilm/policy/logs-policy\n\n\n\nDocumentation\n\nCreate Index API\nCreate or Update Index Template API\nILM Settings\nIndex Lifecycle Management\n\n\n\n\nExample 3: Creating an ILM policy for sensor data collected every hour, with daily rollover and retention for one month\n\nRequirements\n\nCreate a new index every day for sensor data (e.g., sensor_data-{date}).\nAutomatically roll over to a new index when the current one reaches a specific size.\nDelete rolled over indices after one month.\n\nSteps using the Elastic/Kibana UI\n\nOpen the hamburger menu and click on Management &gt; Data &gt; Life Cycle Policies.\nPress + Create New Policy.\nEnter the following:\n\nPolicy name: sensor-data-policy\nHot phase:\n\nChange Keep Data in the Phase Forever (the infinity icon) to Delete Data After This Phase (the trashcan icon).\nClick Advanced Settings.\nUnselect Use Recommended Defaults.\nSet Maximum Age to 1.\nSet Maximum Index Size to 10.\n\nDelete phase:\n\nMove data into phase when: 30 days old.\n\n\nPress Save Policy.\nOpen the Kibana Console or use a REST client.\nCreate an index template that will match on indices that match the pattern “sensor_data-*“.\nPUT /_index_template/sensor_data_index_template\n{\n  \"index_patterns\": [\"sensor_data-*\"]\n}\nReturn to the Management &gt; Data &gt; Life Cycle Policies page.\nPress the plus sign (+) to the right of sensor-data-policy.\nThe Add Policy “sensor-data-policy” to index template dialog opens.\n\nClick on the Index Template input field and type the first few letters of the index template created above.\nSelect the template created above (sensor_data_index_template).\nPress Add Policy.\n\nOpen the Kibana Console or use a REST client.\nList sensor_data_index_template. Notice the ILM policy is now part of the index template.\nGET /_index_template/sensor_data_index_template\nOutput from the GET:\n{\n  \"index_templates\": [\n    {\n      \"name\": \"sensor_data_index_template\",\n      \"index_template\": {\n        \"index_patterns\": [\"sensor_data-*\"],\n        \"template\": {\n          \"settings\": {\n            \"index\": {\n              \"lifecycle\": {\n                \"name\": \"sensor-data-policy\"\n              }\n            }\n          }\n        },\n        \"composed_of\": []\n      }\n    }\n  ]\n}\nList sensor-data-policy.\nGET /_ilm/policy/sensor-data-policy\nIn the in_use_by node you will see:\n\"in_use_by\": {\n  \"indices\": [],\n  \"data_streams\": [],\n  \"composable_templates\": [\n    \"sensor_data_index_template\"\n  ]\n}\n\nOR\n\n\nSteps Using the REST API (which I would not recommend)\n\nOpen the Kibana Console or use a REST client.\nDefine the ILM policy.\nPUT _ilm/policy/sensor-data-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"min_age\": \"0ms\",\n        \"actions\": {\n          \"rollover\": {\n            \"max_age\": \"1d\",\n            \"max_size\": \"10gb\"\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\nAssign the policy to the indices matching the pattern “sensor_data-*“.\nPUT /_index_template/sensor_data_index_template\n{\n  \"index_patterns\": [\"sensor_data-*\"],\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"sensor-data-policy\",\n      \"index.lifecycle.rollover_alias\": \"sensor\"\n    }\n  }\n}\n\n\n\nTest\n\nVerify the ILM policy exists in Kibana under Management &gt; Data &gt; Index Lifecycle Policies.\nVerify the Index Lifecycle Management policy exists and references the index template.\nGET /_ilm/policy/sensor-data-policy\nVerify the policy is referenced in the index template.\nGET /_index_template/sensor_data_index_template\nCreate a new index that matches the pattern sensor_data-*.\nPUT /sensor_data-20240516\nVerify the index has the policy in its definition.\nGET /sensor_data-20240516\n\n\n\nConsiderations\n\nThe hot phase size threshold determines the frequency of rollovers.\nThe delete phase retention period defines how long rolled over data is stored.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE sensor_data-20240516\nDelete the index template.\nDELETE /_index_template/sensor_data_index_template\nDelete the policy.\nDELETE /_ilm/policy/sensor-data-policy\n\n\n\nDocumentation\n\nCreate Index API\nCreate or Update Index Template API\nILM Settings\nIndex Lifecycle Management",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "1-data-management.html#task-define-an-index-template-that-creates-a-new-data-stream",
    "href": "1-data-management.html#task-define-an-index-template-that-creates-a-new-data-stream",
    "title": "1  Data Management",
    "section": "1.5 Task: Define an index template that creates a new data stream",
    "text": "1.5 Task: Define an index template that creates a new data stream\nData streams in Elasticsearch are used for managing time-series data such as logs, metrics, and events. They can handle large volumes of time-series data in an efficient and scalable manner.\nAn interesting aspect is that the creation of the data stream is pretty trivial. It normally looks like this in the index template that contains it:\n```json\n...\n  \"data_stream\" : {}\n...\n```\nYep, that’s it. The defaults take care of most circumstances.\nAlso, the data_stream must be created in an index template as the data_stream needs backing indices. Those backing indices are created when an index is created that matches the pattern in index_patterns (basically, any index created using the index template acts as an alias to the actual backing indices created).\n\nExample 1: Creating an index template for continuously flowing application logs\n\nRequirements\n\nCreate a new data stream named “app-logs” to store application logs.\nAutomatically create new backing indices within the data stream as needed.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nDefine the index template that will be used by the data stream to create new backing indices.\nPUT _index_template/app_logs_index_template\n{\n  \"index_patterns\": [\"app_logs*\"],\n  \"data_stream\": {}\n}\n\n\n\nTest\n\nVerify the index template creation.\nGET _index_template/app_logs_index_template\nConfirm there are no indices named app_logs*.\nGET /_cat/indices\nMock sending streaming data by just pushing a few documents to the stream. When sending documents using _bulk, they must use create instead of index. In addition, the documents must have a @timestamp field.\nPOST app_logs/_bulk\n{ \"create\":{} }\n{ \"@timestamp\": \"2099-05-06T16:21:15.000Z\", \"message\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\" }\n{ \"create\":{} }\n{ \"@timestamp\": \"2099-05-06T16:25:42.000Z\", \"message\": \"192.0.2.255 - - [06/May/2099:16:25:42 +0000] \\\"GET /favicon.ico HTTP/1.0\\\" 200 3638\" }\nThe response will list the name of the automatically created index, which will look something like this:\n{\n  \"errors\": false,\n  \"took\": 8,\n  \"items\": [\n    {\n      \"create\": {\n        \"_index\": \".ds-app_logs-2099.05.06-000001\",\n        \"_id\": \"OOazyo8BAvAOn4WaAfdD\",\n        \"_version\": 1,\n        \"result\": \"created\",\n        \"_shards\": {\n          \"total\": 2,\n          \"successful\": 1,\n          \"failed\": 0\n        },\n        \"_seq_no\": 2,\n        \"_primary_term\": 1,\n        \"status\": 201\n      }\n    },\n    {\n      \"create\": {\n        \"_index\": \".ds-app_logs-2099.05.06-000001\",\n        \"_id\": \"Oeazyo8BAvAOn4WaAfdD\",\n        \"_version\": 1,\n        \"result\": \"created\",\n        \"_shards\": {\n          \"total\": 2,\n          \"successful\": 1,\n          \"failed\": 0\n        },\n        \"_seq_no\": 3,\n        \"_primary_term\": 1,\n        \"status\": 201\n      }\n    }\n  ]\n}\nNotice the name of the index is .ds-app_logs-2099.05.06-000001 (it will probably be slightly different for you).\nRun:\nGET /_cat/indices\n\nYou will see the new index listed. This is the backing index created by the data stream.\n\nCheck for the app_logs data stream under Management &gt; Data &gt; Index Management &gt; Data Streams.\nVerify that the documents were indexed.\nGET app_logs/_search\nNotice in the results that _index has a different name than app_logs.\nYou can also run the following (using the backing index name your cluster created).\nGET .ds-app_logs-2024.07.25-000001/_search\n\n\n\nConsiderations\n\nData streams provide a more efficient way to handle continuously flowing data compared to daily indices. They are created implicitly through the use of index templates, and you must use the _bulk API when streaming data.\nNew backing indices are automatically created within the data stream as needed.\nLifecycle management policies can be applied to data streams for automatic deletion of older backing indices.\n\n\n\nClean-up (optional)\n\nDelete the data stream (deleting the data stream will also delete the backing index).\nDELETE /_data_stream/app_logs\nDelete the index template.\nDELETE _index_template/app_logs_index_template\n\n\n\nDocumentation\n\nIndex Templates\nSetting Up a Data Stream\n\n\n\n\nExample 2: Creating an index template for continuously flowing application logs with defined fields\n\nRequirements\n\nThe template should apply to any index matching the pattern logs*.\nThe template must create a data stream.\nThe template should define settings for two primary shards and one replica.\nThe template should include mappings for fields @timestamp, log_level, and message.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the index template\nPUT _index_template/log_application_index_template\n{\n  \"index_patterns\": [\"logs*\"],\n  \"data_stream\": {},\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 2,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\"\n        },\n        \"log_level\": {\n          \"type\": \"keyword\"\n        },\n        \"message\": {\n          \"type\": \"text\"\n        }\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index template creation\nGET _index_template/log_application_index_template\nConfirm there are no indices named logs*\nGET /_cat/indices\nIndex documents into the data stream\nPOST /logs/_doc\n{\n  \"@timestamp\": \"2024-05-16T12:34:56\",\n  \"log_level\": \"info\",\n  \"message\": \"Test log message\"\n}\nThis will return a result with the name of the backing index\n{\n  \"_index\": \".ds-logs-2024.05.16-000001\", // yours will be different\n  \"_id\": \"PObWyo8BAvAOn4WaC_de\",\n  \"_version\": 1,\n  \"result\": \"created\",\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 1,\n    \"failed\": 0\n  },\n  \"_seq_no\": 0,\n  \"_primary_term\": 1\n}\nRun\nGET /_cat/indices\nThe index will be listed.\nConfirm the configuration of the backing index matches the index template (your backing index name will be different)\nGET .ds-logs-2024.05.16-000001\nRun a search for the document that was indexed\nGET .ds-logs-2024.05.16-000001/_search\n\n\n\nConsiderations\n\nData streams provide a more efficient way to handle continuously flowing data compared to daily indices. They are created implicitly through the use of index templates, and you must use the _bulk API when streaming data.\nNew backing indices are automatically created within the data stream as needed.\nLifecycle management policies can be applied to data streams for automatic deletion of older backing indices (not shown but there is an example at Set Up a Data Stream).\n\n\n\nClean-up (optional)\n\nDelete the data stream (deleting the data stream will also delete the backing index)\nDELETE _data_stream/logs\nDelete the index template\nDELETE _index_template/log_application_index_template\n\n\n\nDocumentation\n\nIndex Templates\nSetting Up a Data Stream\n\n\n\n\nExample 3: Creating a metrics data stream for application performance monitoring\n\nRequirements\n\nCreate an index template named metrics_template.\nThe template should create a new data stream for indices named metrics-{suffix}.\nThe template should have one shard and one replica.\nThe template should have a mapping for the metric field as a keyword data type.\nThe template should have a mapping for the value field as a float data type.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the index template.\nPUT _index_template/metrics_template\n{\n  \"index_patterns\": [\"metrics-*\"],\n  \"data_stream\": {},\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"metric\": {\n          \"type\": \"keyword\"\n        },\n        \"value\": {\n          \"type\": \"float\"\n        }\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index template creation.\nGET _index_template/metrics_template\nConfirm there are no indices named metrics-*.\nGET /_cat/indices\nIndex documents into the data stream.\nPOST /metrics-ds/_doc\n{\n  \"@timestamp\": \"2024-05-16T12:34:56\",\n  \"metric\": \"cpu\",\n  \"value\": 0.5\n}\nNotice the use of the @timestamp field. That is required for any documents going into a data stream.\nThis will return a result with the name of the backing index.\n{\n  \"_index\": \".ds-metrics-ds-2024.05.16-000001\", // yours will be different\n  \"_id\": \"P-YFy48BAvAOn4WaUvef\",\n  \"_version\": 1,\n  \"result\": \"created\",\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 1,\n    \"failed\": 0\n  },\n  \"_seq_no\": 1,\n  \"_primary_term\": 1\n}\nRun:\nGET /_cat/indices\nThe index will be listed.\nConfirm the configuration of the backing index matches the index template (your backing index name will be different).\nGET .ds-metrics-ds-2024.05.16-000001\nRun a search for the document that was indexed.\nGET .ds-metrics-ds-2024.05.16-000001/_search\n\n\n\nConsiderations\n\nThe keyword data type is chosen for the metric field to enable exact matching and filtering.\nThe float data type is chosen for the value field to enable precise numerical calculations.\nOne shard and one replica are chosen for simplicity and development purposes; in a production environment, this would depend on the expected data volume and search traffic.\n\n\n\nClean-up (optional)\n\nDelete the data stream (deleting the data stream will also delete the backing index).\nDELETE _data_stream/metrics-ds\nDelete the index template.\nDELETE _index_template/metrics_template\n\n\n\nDocumentation\n\nIndex templates\nData streams\nMapping types\n\n\n\n\nExample 4: Defining a Data Stream with Specific Lifecycle Policies\n\nRequirements\n\nCreate an index template named logs_index_template.\nCreate a data stream named logs_my_app_production.\nConfigure the data lifecycle:\n\nData is hot for 3 minutes.\nData rolls to warm immediately after 3 minutes.\nData is warm for 5 minutes.\nData rolls to cold after 5 minutes.\nData is deleted 10 minutes after rolling to cold.\n\n\n\n\nSteps\n\nCreate the Index Template:\n\nDefine an index template named logs_index_template that matches the data stream logs_my_app_production.\n\nPUT _index_template/logs_index_template\n{\n  \"index_patterns\": [\"logs_my_app_production*\"],\n  \"data_stream\": {}\n}\nCreate the ILM Policy using the Elastic/Kibana UI{.unnumbered}\n\nOpen the hamburger menu and click on Management &gt; Data &gt; Index Life Cycle Policies.\nPress + Create New Policy.\nEnter the following:\n\n\nPolicy name: logs-policy\nHot phase:\n\nAdvanced Settings &gt; Use Recommended Defaults (disable) &gt; Maximum Age: 7 Days\n\nWarm phase (enable):\n\nMove data into phase when: 3 minutes old.\nLeave Delete data after this phase.\n\nCold phase:\n\nMove data into phase when: 5 minutes old.\nLeave Delete data after this phase.\n\nDelete phase:\n\nMove data into phase when: 10 minutes old.\n\n\n\nPress Save Policy.\nManagement &gt; Data &gt; Index Life Cycle Policies &gt; [plus sign]\nAdd Policy “logs-policy” to Index Template &gt; Index Template: logs_index_template &gt; Add Policy\n\n\nOR\n\nCreate the ILM Policy:\n\nDefine an Index Lifecycle Management (ILM) policy named logs_index_policy to manage the data lifecycle.\n\nPUT _ilm/policy/logs_index_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"min_age\": \"0ms\",\n        \"actions\": {\n          \"rollover\": {\n            \"max_age\": \"3m\"\n          }\n        }\n      },\n      \"warm\": {\n        \"min_age\": \"3m\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 50\n          }\n        }\n      },\n      \"cold\": {\n        \"min_age\": \"8m\",\n        \"actions\": {\n          \"set_priority\": {\n            \"priority\": 0\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"18m\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\nCreate the Data Stream:\n\nCreating the data stream is similar to creating an index using:\n\nPUT logs_my_app_production\n\nCreate the data stream\n\nPUT /_data_stream/logs_my_app_production\n\n\n\nTest\n\nIndex Sample Data:\n\nIndex some sample documents into the data stream to ensure it is working correctly.\n\nPOST /logs_my_app_production/_doc\n{\n  \"message\": \"This is a test log entry\",\n  \"@timestamp\": \"2024-07-10T23:00:00Z\"\n}\nVerify ILM Policy:\n\nCheck the status of the ILM policy to ensure it is being applied correctly.\n\nGET /_ilm/explain/logs-policy\nMonitor Data Lifecycle:\n\nMonitor the data stream to ensure that documents transition through the hot, warm, cold, and delete phases as expected.\n\n\n\n\nConsiderations\n\nThe rollover action in the hot phase ensures that the index rolls over after 3 minutes.\nThe set_priority action in the warm and cold phases helps manage resource allocation.\nThe delete action in the delete phase ensures that data is deleted 10 minutes after rolling to cold.\n\n\n\nClean-up (Optional)\n\nDelete the data stream and index template to clean up the resources.\nDELETE /_data_stream/logs_my_app_production\nDELETE /_index_template/logs_index_template\nDELETE /_ilm/policy/logs-policy\n\n\n\nDocumentation\n\nElasticsearch Data Streams\nElasticsearch Index Templates\nElasticsearch ILM Policies",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "2-searching-data.html",
    "href": "2-searching-data.html",
    "title": "2  Searching Data",
    "section": "",
    "text": "2.1 Task: Write and execute a search query for terms and/or phrases in one or more fields of an index\nThe following section will have only one full example, but will show variations of term and phrase queries. Also, bear in mind that when they say term they may not mean the Elasticsearch use of the word, but rather the generic search use of the word. There are a lot of ways to execute a search in Elasticsearch. Don’t get bogged down; focus on term and phrase searches for this section of the example.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Searching Data</span>"
    ]
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-a-search-query-for-terms-andor-phrases-in-one-or-more-fields-of-an-index",
    "href": "2-searching-data.html#task-write-and-execute-a-search-query-for-terms-andor-phrases-in-one-or-more-fields-of-an-index",
    "title": "2  Searching Data",
    "section": "",
    "text": "Example 1: Write and execute a basic term and phrase search\n\nRequirements\n\nCreate an index\nIndex some documents\nExecute a term query\nExecute a phrase query\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\nPOST /example_index/_bulk\n{ \"index\": {} }\n{ \"title\": \"The quick brown fox\", \"text\": \"The quick brown fox jumps over the lazy dog.\" }\n{ \"index\": {} }\n{ \"title\": \"Fast and curious\", \"text\": \"A fast and curious fox was seen leaping over a lazy dog.\" }\n{ \"index\": {} }\n{ \"title\": \"A fox in action\", \"text\": \"In a remarkable display of agility, a quick fox effortlessly jumped over a dog.\" }\n{ \"index\": {} }\n{ \"title\": \"Wildlife wonders\", \"text\": \"Observers were amazed as the quick brown fox jumped over the lazy dog.\" }\n{ \"index\": {} }\n{ \"title\": \"Fox tales\", \"text\": \"The tale of the quick fox that jumped over the lazy dog has become a legend.\" }\nExecute a term query\n\n\nUse the GET method to search for documents using 3 different term queries (there are 10 different ways currently. Refer to the Term-level Queries documentation for the full list).\nGET example_index/_search\n{\n  \"query\": {\n    \"term\": {\n      \"title\": {\n        \"value\": \"quick\"\n      }\n    }\n  }\n}\nGET example_index/_search\n{\n  \"query\": {\n    \"terms\": {\n      \"text\": [\"display\", \"amazed\"]\n    }\n  }\n}\n\n\nExecute a phrase query\n\n\nreturns 2 docs\nGET /example_index/_search\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"text\": \"quick brown fox\"\n    }\n  }\n}\nreturns 1 doc\nGET /example_index/_search\n{\n  \"query\": {\n    \"match_phrase_prefix\": {\n      \"text\": \"fast and curi\"\n    }\n  }\n}\nreturns 1 doc\nGET /example_index/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"default_field\": \"text\",\n      \"query\": \"\\\"fox jumps\\\"\"\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe default standard analyzer (lowercasing, whitespace tokenization, basic normalization) is used.\nThe term query is used for exact matches and is not analyzed, meaning it matches the exact term in the inverted index.\nThe match_phrase query analyzes the input text and matches it as a phrase, making it useful for finding exact sequences of terms.\n\n\n\nTest\n\nVerify the various queries return the proper results.\n\n\n\nClean-up (optional)\n\nDelete the example index\nDELETE example_index\n\n\n\nDocumentation\n\nFull Text Queries\nMatch Phrase Query\nMatch Phrase Prefix Query\nQuery DSL\nTerm-level Queries\n\n\n\n\nExample 2: Boosting Document Score When an Additional Field Matches\n\nRequirements\n\nPerform a search for beverage OR bar\nBoost the score of documents if the value snack exists in the tags field.\n\n\n\nSteps\n\nIndex Sample Documents Using _bulk Endpoint:\n\nIndex documents with fields such as name, description, and tags.\n\nPOST /products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"Yoo-hoo Beverage\", \"description\": \"A delicious, chocolate-flavored drink.\", \"tags\": [\"beverage\", \"chocolate\"] }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Apple iPhone 12\", \"description\": \"The latest iPhone model with advanced features.\", \"tags\": [\"electronics\", \"smartphone\"] }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"name\": \"Choco-Lite Bar\", \"description\": \"A light and crispy chocolate snack bar.\", \"tags\": [\"snack\", \"chocolate\"] }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"name\": \"Samsung Galaxy S21\", \"description\": \"A powerful smartphone with an impressive camera.\", \"tags\": [\"electronics\", \"smartphone\"] }\n{ \"index\": { \"_id\": \"5\" } }\n{ \"name\": \"Nike Air Max 270\", \"description\": \"Comfortable and stylish sneakers.\", \"tags\": [\"footwear\", \"sportswear\"] }\nPerform the query_string Query with Boosting:\n\nUse a query_string query to create an OR condition within the query.\nUse a function_score query to boost the score of documents where the tags field contains a specific value (e.g., \"chocolate\").\n\nGET /products/_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": {\n        \"query_string\": {\n          \"query\": \"beverage OR bar\"\n        }\n      },\n      \"functions\": [\n        {\n          \"filter\": {\n            \"term\": { \"tags\": \"snack\" }\n          },\n          \"weight\": 2\n        }\n      ],\n      \"boost_mode\": \"multiply\"\n    }\n  }\n}\n\n\n\nTest\n\nRun the above search query.\nRun the following query (which is missing the filter function)\n\nGET /products/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"beverage OR bar\"\n    }\n  }\n}\n\nCheck the boosted output to ensure that documents containing \"snack\" in the tags field have a higher score, and that documents are matched based on the OR condition in the query_string.\n\n\n\nConsiderations\n\nThe query_string query allows you to use a query syntax that includes operators such as OR, AND, and NOT to combine different search criteria.\nThe function_score query is used to boost the score of documents based on specific conditions—in this case, whether the tags field contains the value \"snack\".\nThe weight parameter in the function_score query determines the amount by which the score is boosted, and the boost_mode of \"multiply\" multiplies the original score by the boost value.\n\n\n\nClean-up (optional)\n\nDelete the example index\nDELETE products\n\n\n\nDocumentation\n\nQuery String Query\nFunction Score Query\nTerm Query",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Searching Data</span>"
    ]
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-a-search-query-that-is-a-boolean-combination-of-multiple-queries-and-filters",
    "href": "2-searching-data.html#task-write-and-execute-a-search-query-that-is-a-boolean-combination-of-multiple-queries-and-filters",
    "title": "2  Searching Data",
    "section": "2.2 Task: Write and execute a search query that is a Boolean combination of multiple queries and filters",
    "text": "2.2 Task: Write and execute a search query that is a Boolean combination of multiple queries and filters\n\nExample 1: Creating a Boolean search for documents in a book index\n\nRequirements\n\nSearch for documents with a term in the “title”, “description”, and “category” field\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\nPOST /books/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"title\": \"To Kill a Mockingbird\", \"description\": \"A novel about the serious issues of rape and racial inequality.\", \"category\": \"Fiction\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"title\": \"1984\", \"description\": \"A novel that delves into the dangers of totalitarianism.\", \"category\": \"Dystopian\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"title\": \"The Great Gatsby\", \"description\": \"A critique of the American Dream.\", \"category\": \"Fiction\" }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"title\": \"Moby Dick\", \"description\": \"The quest of Ahab to exact revenge on the whale Moby Dick.\", \"category\": \"Adventure\" }\n{ \"index\": { \"_id\": \"5\" } }\n{ \"title\": \"Pride and Prejudice\", \"description\": \"A romantic novel that also critiques the British landed gentry at the end of the 18th century.\", \"category\": \"Romance\" }\nCreate a boolean search query. The order in which the various clauses are added don’t matter to the final result.\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {}\n  }\n}\nAdd a must query for the description field. This will return 4 documents.\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\nAdd a filter query for the category field. This will return 2 documents.\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"fiction\"\n          }\n        }\n      ]\n    }\n  }\n}\nAdd a must_not filter for the title field. This will return 1 document.\nGET books/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"terms\": {\n            \"description\": [\n              \"novel\",\n              \"dream\",\n              \"critique\"\n            ]\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"fiction\"\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"term\": {\n            \"title\": {\n              \"value\": \"gatsby\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe bool query allows for combining multiple queries and filters with Boolean logic.\nThe must, must_not, and filter clauses ensure that all searches and filters must match for a document to be returned.\n\n\n\nTest\n\nVerify that the search query returns documents with the term “novel”, “dream”, and “critique” in the description field. Why are there no documents with the term “critique”?\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE books\n\n\n\nDocumentation\n\nElasticsearch Boolean Query\nElasticsearch Match Query\nElasticsearch Range Query\nElasticsearch Term Query\n\n\n\n\nExample 2: Creating a Boolean search for finding products within a specific price range and excluding discontinued items\n\nRequirements\n\nFind all documents where the name field exists (name: \\*) and the price field falls within a specified range.\nAdditionally, filter out any documents where the discontinued field is set to true.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Coffee Maker\",\"price\":49.99,\"discontinued\":false}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Gaming Laptop\",\"price\":1299.99,\"discontinued\":false}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Wireless Headphones\",\"price\":79.99,\"discontinued\":true}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Smartwatch\",\"price\":249.99,\"discontinued\":false}\nConstruct the first search query (the name field exists and the price field falls within a specified range)\nGET products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"exists\": {\n            \"field\": \"name\"\n          }\n        },\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 70,\n              \"lte\": 500\n            }\n          }\n        }\n      ]\n    }\n  }\n}\nConstruct the second search query (same as above, but check if discontinued is set to true)\nGET products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"exists\": {\n            \"field\": \"name\"\n          }\n        },\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 70,\n              \"lte\": 500\n            }\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"term\": {\n            \"discontinued\": {\n              \"value\": \"true\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nExplanation\n\nSimilar to the previous example, the bool query combines multiple conditions.\nThe must clause specifies documents that must match all conditions within it.\nThe range query ensures the price field is between $70 (inclusive) and $500 (inclusive).\nThe must_not clause excludes documents that match the specified criteria.\nThe term query filters out documents where discontinued is set to true.\n\n\n\nTest\n\nRun the search query and verify the results only include documents for products with:\n\nA price between $70 and $500 (inclusive).\ndiscontinued set to true (not discontinued).\n\n\nThis should return a single document with an ID of 4 (Smartwatch) based on the sample data.\n\n\nConsiderations\n\nThe chosen price range (gte: 70, lte: 500) can be adjusted based on your specific needs.\nYou can modify the match query for name to use more specific criteria if needed.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE products\n\n\n\nDocumentation\n\nElasticsearch Boolean Query\nElasticsearch Match Query\nElasticsearch Range Query\nElasticsearch Term Query\n\n\n\n\nExample 3: Creating a Boolean search for e-commerce products\n\nRequirements\n\nSearch for products that belong to the “Electronics” category.\nThe product name should contain the term “phone”.\nExclude products with a price greater than 500.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate an index.\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\" : {\n        \"type\": \"text\"\n      },\n      \"category\" : {\n        \"type\": \"text\"\n      },\n      \"price\" : {\n        \"type\": \"float\"\n      }\n    }\n  }\n}\nIndex some documents which will create an index at the same time. The Elastic Console doesn’t like properly formatted documents when calling _bulk so they need to be tightly packed.\nPOST /products/_bulk\n{\"index\": { \"_id\": 1 } }\n{ \"name\": \"Smartphone X\", \"category\": \"Electronics\", \"price\": 399.99 }\n{\"index\": { \"_id\": 2 } }\n{ \"name\": \"Laptop Y\", \"category\": \"Electronics\", \"price\": 799.99 }\n{\"index\": { \"_id\": 3 } }\n{ \"name\": \"Headphones Z\", \"category\": \"Electronics\", \"price\": 99.99 }\n{\"index\": { \"_id\": 4 } }\n{ \"name\": \"Gaming Console\", \"category\": \"Electronics\", \"price\": 299.99 }\nCreate a term query that only matches the category “electronics”. This returns all 4 documents.\nGET products/_search\n{\n  \"query\": {\n    \"term\": {\n      \"category\": {\n        \"value\": \"electronics\"\n      }\n    }\n  }\n}\nCreate another query using wildcard to return docs that includes “phone”. This returns only 2 documents.\nGET products/_search\n{\n  \"query\": {\n    \"wildcard\": {\n      \"name\": {\n        \"value\": \"*phone*\"\n      }\n    }\n  }\n}\nCreate another query using range that returns docs with any price less than $500. This returns 3 documents.\nGET products/_search\n{\n  \"query\": {\n    \"range\": {\n      \"price\": {\n        \"lt\": 500\n      }\n    }\n  }\n}\nCombine the above into one bool query with a single must that contains the three queries. This will return the 2 matching documents.\nGET products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"category\": {\n              \"value\": \"electronics\"\n            }\n          }\n        },\n        {\n          \"wildcard\": {\n            \"name\": {\n              \"value\": \"*phone*\"\n            }\n          }\n        },\n        {\n          \"range\": {\n            \"price\": {\n              \"lt\": 500\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nTest\n\nThe search results should include the following documents:\n\nSmartphone X\nHeadphones Z\n\n\n\n\nConsiderations\n\nThe term query is used for matches on the category field.\nThe wildcard query is used for matches on the name field.\nThe range query is used to filter out documents based on price.\nThe bool.must query combines these conditions using the specified occurrence types.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE products\n\n\n\nDocumentation\n\nBoolean Query\nMatch Query\nRange Query\nTerm Query\nWildcard Query\n\n\n\n\nExample 4: Creating a Boolean search for e-commerce products\n\nRequirements\n\nCreate an index named “products”.\nCreate at least 4 documents with varying categories, prices, ratings, and brands.\nCreate a boolean query\n\nUse the must:\n\nreturn just electronics\nproducts more than $500\n\nUse must_not:\n\nrating less than 4\n\nUse filter:\n\nonly Apple products\n\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the “products” index\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"brand\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"long\"\n      },\n      \"rating\": {\n        \"type\": \"float\"\n      }\n    }\n  }\n}\nAdd some sample documents using the _bulk endpoint.\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Laptop\",\"category\":\"Electronics\",\"price\":1200,\"rating\":4.5,\"brand\":\"Apple\"}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Smartphone\",\"category\":\"Electronics\",\"price\":800,\"rating\":4.2,\"brand\":\"Samsung\"}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Sofa\",\"category\":\"Furniture\",\"price\":1000,\"rating\":3.8,\"brand\":\"IKEA\"}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Headphones\",\"category\":\"Electronics\",\"price\":150,\"rating\":2.5,\"brand\":\"Sony\"}\n{\"index\":{\"_id\":5}}\n{\"name\":\"Dining Table\",\"category\":\"Furniture\",\"price\":600,\"rating\":4.1,\"brand\":\"Ashley\"}\nCreate a term query that only matches the category “electronics”. This returns 3 documents.\nGET products/_search\n{\n  \"query\": {\n    \"term\": {\n      \"category\": {\n        \"value\": \"electronics\"\n      }\n    }\n  }\n}\nCreate a range query to return products whose price is greater than $500. This should return 4 documents (why?).\nGET products/_search\n{\n  \"query\": {\n    \"range\": {\n      \"price\": {\n        \"gte\": 500\n      }\n    }\n  }\n}\nCreate another range query to return products with a rating less than 4. This will return 2 documents.\nGET products/_search\n{\n  \"query\": {\n    \"range\": {\n      \"rating\": {\n        \"lt\": 4\n      }\n    }\n  }\n}\nCreate another term query to return only Apple branded products. This will return 2 documents.\nGET products/_search\n{\n  \"query\": {\n    \"term\": {\n      \"brand\": {\n        \"value\": \"apple\"\n      }\n    }\n  }\n}\nAssemble the bool query by placing each query in their appropriate must, must_not and filter node.\nGET products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"category\": {\n              \"value\": \"electronics\"\n            }\n          }\n        },\n        {\n          \"range\": {\n            \"price\": {\n              \"gte\": 500\n            }\n          }\n        }\n      ],\n      \"must_not\": [\n        {\n          \"range\": {\n            \"rating\": {\n              \"lt\": 4\n            }\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"brand\": {\n              \"value\": \"apple\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nTest\n\nCheck the response from the search query to ensure that it returns the expected documents\n\nproducts in the “Electronics” category\na price greater than $500\nexcluding products with a rating less than 4\nfrom the brand “Apple”\n\n\n\n\nConsiderations\n\nThe filter clause is used to include only documents with the brand “Apple”.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE products\n\n\n\nDocumentation\n\nElasticsearch Boolean Query\nElasticsearch Term Query\nElasticsearch Range Query",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Searching Data</span>"
    ]
  },
  {
    "objectID": "2-searching-data.html#task-create-an-asynchronous-search",
    "href": "2-searching-data.html#task-create-an-asynchronous-search",
    "title": "2  Searching Data",
    "section": "2.3 Task: Create an asynchronous search",
    "text": "2.3 Task: Create an asynchronous search\nAsynchronous search uses the same parameters as regular search with a few extra features listed here. For example, in the solution below the documentation for the size option is here. There is only one example here as you can look up the other options as needed during the exam.\n\nExample 1: Executing an asynchronous search on a large log index\n\nRequirements\n\nAn Elasticsearch index named “logs” with a large number of documents (e.g., millions of log entries).\nPerform a search on the “logs” index that may take a long time to complete due to the size of the index.\nRetrieve the search results asynchronously without blocking the client.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nIf you were submitting a normal/synchronous search to an index called logs your request would look something like this:\nPOST /logs/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"size\": 10000\n}\nTo turn your request into an asynchronous search request turn _search to _async_search\nPOST /logs/_async_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"size\": 10000\n}\n\nThis request will return an id and a response object containing partial results if available.\n\nCheck the status of the asynchronous search using the id.\nGET /_async_search/status/{id}\nRetrieve the search results using the id.\nGET /_async_search/{id}\n\n\n\nTest\n\nIndex a large number of sample log documents or use an index with a large number of documents.\nExecute the asynchronous search request and store the returned id.\nPeriodically check the status of the search using the id and the /_async_search/status/{id} endpoint.\nGET /_async_search/status/{id}\nOnce the search is complete, retrieve the final results using the id and the /_async_search/{id} endpoint.\n\nGET /_async_search/{id}\n\n\nConsiderations\n\nThe _async_search endpoint is used to submit an asynchronous search request.\nThe id returned by the initial request is used to check the status and retrieve the final results.\nAsynchronous search is useful for long-running searches on large datasets, as it doesn’t block the client while the search is being processed.\n\n\n\nClean-up (optional)\n\nIf you created an index (for example, logs) for this example you might want to delete it.\nDELETE logs\n\n\n\nDocumentation\n\nAsync Search API\nSubmitting Async Search\nStatus Check Async Search\nRetrieving Async Search Results",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Searching Data</span>"
    ]
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-metric-and-bucket-aggregations",
    "href": "2-searching-data.html#task-write-and-execute-metric-and-bucket-aggregations",
    "title": "2  Searching Data",
    "section": "2.4 Task: Write and execute metric and bucket aggregations",
    "text": "2.4 Task: Write and execute metric and bucket aggregations\n\nExample 1: Creating Metric and Bucket Aggregations for Product Prices\n\nRequirements\n\nCreate an index called product_prices.\nIndex at least four documents using the _bulk endpoint.\nExecute metric and bucket aggregations in a single\n\nbucket the category field\ncalculate the average price per bucket\nfind the maximum price per bucket\nfind the minimum price per bucket\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\n\nEnsure you have access to Kibana or any REST client to execute the following requests.\n\nCreate an index with the following schema (needed for the aggregations to work properly).\nPUT product_prices\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\nIndex documents.\nPOST /product_prices/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\nExecute a simple aggregation (should return 2 buckets).\nGET product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"category_buckets\": {\n      \"terms\": {\n        \"field\": \"category\"\n      }\n    }\n  }\n}\nAdd and execute a single sub-aggregation to determine the average price per category (bucket).\nGET product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"category_buckets\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"avg_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\nAdd min and max sub-aggregations and execute the query.\nGET product_prices/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"category_buckets\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"avg_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        },\n        \"min_price\" : {\n          \"min\": {\n            \"field\": \"price\"\n          }\n        },\n        \"max_price\": {\n          \"max\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index creation.\nGET /product_prices\nVerify the documents have been indexed.\nGET /product_prices/_search\nExecute the aggregation query and verify the results.\n{\n  ...\n  \"aggregations\": {\n    \"category_buckets\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Apparel\",\n          \"doc_count\": 2,\n          \"avg_price\": {\n            \"value\": 16.49\n          },\n          \"min_price\": {\n            \"value\": 12.99\n          },\n          \"max_price\": {\n            \"value\": 19.99\n          }\n        },\n        {\n          \"key\": \"Books\",\n          \"doc_count\": 2,\n          \"avg_price\": {\n            \"value\": 34.99\n          },\n          \"min_price\": {\n            \"value\": 29.99\n          },\n          \"max_price\": {\n            \"value\": 39.99\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe category field must be of type keyword.\nThe terms aggregation creates buckets for each unique category.\nThe avg, min, and max sub-aggregations calculate the average, minimum, and maximum prices within each category bucket.\nSetting size to 0 ensures that only aggregation results are returned, not individual documents.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE product_prices\n\n\n\nDocumentation\n\nAggregations\nTerms Aggregation\nAvg Aggregation\nMax Aggregation\nMin Aggregation\n\n\n\n\nExample 2: Creating Metric and Bucket Aggregations for Website Traffic\n\nRequirements\n\nCreate a new index with four documents representing website traffic data.\nAggregate the following:\n\nGroup traffic by country.\nCalculate the total page views.\nCalculate the average page views per country.\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate a new index.\nPUT traffic\n{\n  \"mappings\": {\n    \"properties\": {\n      \"country\": {\n        \"type\": \"keyword\"\n      },\n      \"page_views\": {\n        \"type\": \"long\"\n      }\n    }\n  }\n}\nAdd four documents representing website traffic data.\nPOST /traffic/_bulk\n{\"index\":{}}\n{\"country\":\"USA\",\"page_views\":100}\n{\"index\":{}}\n{\"country\":\"USA\",\"page_views\":200}\n{\"index\":{}}\n{\"country\":\"Canada\",\"page_views\":50}\n{\"index\":{}}\n{\"country\":\"Canada\",\"page_views\":75}\nExecute the bucket aggregation for country (should return 2 buckets).\nGET traffic/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"country_bucket\": {\n      \"terms\": {\n        \"field\": \"country\"\n      }\n    }\n  }\n}\nAdd the sum aggregation for total page_views (should return 1 aggregation).\nGET traffic/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"country_bucket\": {\n      \"terms\": {\n        \"field\": \"country\"\n      }\n    },\n    \"total_page_views\": {\n      \"sum\": {\n        \"field\": \"page_views\"\n      }\n    }\n  }\n}\nAdd a sub-aggregation for average page_views per country (should appear in 2 buckets).\nGET traffic/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"country_bucket\": {\n      \"terms\": {\n        \"field\": \"country\"\n      },\n      \"aggs\": {\n        \"avg_page_views\": {\n          \"avg\": {\n            \"field\": \"page_views\"\n          }\n        }\n      }\n    },\n    \"total_page_views\": {\n      \"sum\": {\n        \"field\": \"page_views\"\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index creation.\nGET /traffic\nVerify the documents have been indexed.\nGET /traffic/_search\nVerify that the total page views are calculated correctly (should be 425).\nGET /traffic/_search\n{\n  \"aggs\": {\n    \"total_page_views\": {\n      \"sum\": {\n        \"field\": \"page_views\"\n      }\n    }\n  }\n}\nVerify that the traffic is grouped correctly by country and average page views are calculated.\nGET /traffic/_search\n{\n  \"aggs\": {\n    \"traffic_by_country\": {\n      \"terms\": {\n        \"field\": \"country\"\n      },\n      \"aggs\": {\n        \"avg_page_views\": {\n          \"avg\": {\n            \"field\": \"page_views\"\n          }\n        }\n      }\n    }\n  }\n}\nResponse:\n{\n  ...\n  \"aggregations\": {\n    \"country_bucket\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Canada\",\n          \"doc_count\": 2,\n          \"avg_page_views\": {\n            \"value\": 62.5\n          }\n        },\n        {\n          \"key\": \"USA\",\n          \"doc_count\": 2,\n          \"avg_page_views\": {\n            \"value\": 150\n          }\n        }\n      ]\n    },\n    \"total_page_views\": {\n      \"value\": 425\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe country field must be of type keyword.\nThe terms bucket aggregation is used to group traffic by country.\nThe sum metric aggregation is used to calculate the total page views.\nThe avg metric aggregation is used to calculate the average page views per country.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE traffic\n\n\n\nDocumentation\n\nAggregations\nMetric Aggregations\nBucket Aggregations\nTerms Aggregation\n\n\n\n\nExample 3: Creating Metric and Bucket Aggregations for Analyzing Employee Salaries\n\nRequirements\n\nAn Elasticsearch index named employees with documents containing fields name, department, position, salary, hire_date.\nCalculate the average salary across all employees.\nGroup the employees by department\nCalculate the maximum salary for each department.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate an index with the proper mapping for the department as we want to bucket by it.\nPUT employees\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"department\": {\n        \"type\": \"keyword\"\n      },\n      \"position\": {\n        \"type\": \"text\"\n      },\n      \"salary\": {\n        \"type\": \"integer\"\n      },\n      \"hire_date\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\nIndex sample employee documents using the /_bulk endpoint.\nPOST /employees/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"John Doe\", \"department\":\"Engineering\", \"position\":\"Software Engineer\", \"salary\":80000, \"hire_date\":\"2018-01-15\"}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Jane Smith\", \"department\":\"Engineering\", \"position\":\"DevOps Engineer\", \"salary\":75000, \"hire_date\":\"2020-03-01\"}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Bob Johnson\", \"department\":\"Sales\", \"position\":\"Sales Manager\", \"salary\":90000, \"hire_date\":\"2016-06-01\"}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Alice Williams\", \"department\":\"Sales\", \"position\":\"Sales Representative\", \"salary\":65000, \"hire_date\":\"2019-09-15\"}\nCalculate the average salary of all employees\nGET employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"avg_salary_all_emps\": {\n      \"avg\": {\n        \"field\": \"salary\"\n      }\n    }\n  }\n}\nAdd grouping the employees by department\nGET employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"avg_salary_all_emps\": {\n      \"avg\": {\n        \"field\": \"salary\"\n      }\n    },\n    \"employees_by_department\" : {\n      \"terms\": {\n        \"field\": \"department\"\n      }\n    }\n  }\n}\nAdd calculating the highest salary of all employees by department\nGET employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"avg_salary_all_emps\": {\n      \"avg\": {\n        \"field\": \"salary\"\n      }\n    },\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"max_salary_by_department\": {\n          \"max\": {\n            \"field\": \"salary\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index creation.\nGET /employees\nVerify the documents have been indexed.\nGET /employees/_search\nExecute the aggregation query, and it should return the following:\n{\n  ...\n  \"aggregations\": {\n    \"avg_salary_all_emps\": {\n      \"value\": 77500\n    },\n    \"employees_by_department\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Engineering\",\n          \"doc_count\": 2,\n          \"max_salary_by_department\": {\n            \"value\": 80000\n          }\n        },\n        {\n          \"key\": \"Sales\",\n          \"doc_count\": 2,\n          \"max_salary_by_department\": {\n            \"value\": 90000\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe department field must be of type keyword.\nThe size parameter is set to 0 to exclude hit documents from the response.\nThe avg_salary_all_emps metric aggregation calculates the average of the salary field across all documents.\nThe employees_by_department bucket aggregation groups the documents by the department field.\nThe max_salary_by_department sub-aggregation calculates the maximum value of the salary field for each department.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE employees\n\n\n\nDocumentation\n\nElasticsearch Aggregations\nMetric Aggregations\nBucket Aggregations\nTerms Aggregation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Searching Data</span>"
    ]
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-aggregations-that-contain-subaggregations",
    "href": "2-searching-data.html#task-write-and-execute-aggregations-that-contain-subaggregations",
    "title": "2  Searching Data",
    "section": "2.5 Task: Write and execute aggregations that contain subaggregations",
    "text": "2.5 Task: Write and execute aggregations that contain subaggregations\n\nExample 1: Creating aggregations and sub-aggregations for Product Categories and Prices\n\nRequirements\n\nCreate aggregations\n\nby category\nsub-aggregation of average price by category\n\nprice ranges: $0 to $20, $20-$40, $40 and up\n\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate an index.\nPUT /product_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\nIndex some sample documents.\nPOST /product_index/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\nCreate an aggregation by category.\nGET product_index/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"category_buckets\": {\n      \"terms\": {\n        \"field\": \"category\"\n      }\n    }\n  }\n}\nCreate a sub-aggregations of average price.\nGET product_index/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"category_buckets\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        }\n      }\n    }\n  }\n}\nCreate a sub-aggregations of price ranges ($0-$20, $10-$40, $40 and up).\nGET product_index/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"category_buckets\": {\n      \"terms\": {\n        \"field\": \"category\"\n      },\n      \"aggs\": {\n        \"average_price\": {\n          \"avg\": {\n            \"field\": \"price\"\n          }\n        },\n        \"price_ranges\" : {\n          \"range\": {\n            \"field\": \"price\",\n            \"ranges\": [\n              {\n                \"to\": 20\n              },\n              {\n                \"from\": 20,\n                \"to\": 40\n              },\n              {\n                \"from\": 40\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index creation and mappings.\nGET /product_index\nVerify the test documents are in the index.\nGET /product_index/_search\nExecute the aggregation query and confirm the results.\n{\n  ...\n  \"aggregations\": {\n    \"category_buckets\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Apparel\",\n          \"doc_count\": 2,\n          \"average_price\": {\n            \"value\": 16.49\n          },\n          \"price_ranges\": {\n            \"buckets\": [\n              {\n                \"key\": \"*-20.0\",\n                \"to\": 20,\n                \"doc_count\": 2\n              },\n              {\n                \"key\": \"20.0-40.0\",\n                \"from\": 20,\n                \"to\": 40,\n                \"doc_count\": 0\n              },\n              {\n                \"key\": \"40.0-*\",\n                \"from\": 40,\n                \"doc_count\": 0\n              }\n            ]\n          }\n        },\n        {\n          \"key\": \"Books\",\n          \"doc_count\": 2,\n          \"average_price\": {\n            \"value\": 34.99\n          },\n          \"price_ranges\": {\n            \"buckets\": [\n              {\n                \"key\": \"*-20.0\",\n                \"to\": 20,\n                \"doc_count\": 0\n              },\n              {\n                \"key\": \"20.0-40.0\",\n                \"from\": 20,\n                \"to\": 40,\n                \"doc_count\": 2\n              },\n              {\n                \"key\": \"40.0-*\",\n                \"from\": 40,\n                \"doc_count\": 0\n              }\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nConsiderations\n\nSetting size: 0 ensures the search doesn’t return any documents, focusing solely on the aggregations.\nThe category field must be of type keyword.\nThe terms aggregation creates buckets for each unique category.\nThe avg sub-aggregation calculates the average price within each category bucket.\nThe range sub-aggregation divides the prices into specified ranges within each category bucket.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE product_index\n\n\n\nDocumentation\n\nAggregations\nAvg Aggregation\nRange Aggregation\nTerms Aggregation\n\n\n\n\nExample 2: Creating aggregations and sub-aggregations for Employee Data Analysis\n\nRequirements\n\nUse the terms aggregation to group employees by department.\nUse the avg sub-aggregation to calculate the average salary per department.\nUse the filters sub-aggregation to group employees by job_title.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate a new index called employees.\nPUT employees\n{\n  \"mappings\": {\n    \"properties\": {\n      \"department\": {\n        \"type\": \"keyword\"\n      },\n      \"salary\": {\n        \"type\": \"integer\"\n      },\n      \"job_title\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\nInsert four documents representing employee data.\nPOST /employees/_bulk\n{\"index\":{}}\n{\"department\":\"Sales\",\"salary\":100000,\"job_title\":\"Manager\"}\n{\"index\":{}}\n{\"department\":\"Sales\",\"salary\":80000,\"job_title\":\"Representative\"}\n{\"index\":{}}\n{\"department\":\"Marketing\",\"salary\":120000,\"job_title\":\"Manager\"}\n{\"index\":{}}\n{\"department\":\"Marketing\",\"salary\":90000,\"job_title\":\"Coordinator\"}\nExecute an aggregation by department.\nGET employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      }\n    }\n  }\n}\nAdd the sub-aggregations for average salary by department.\nGET employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"avg_salary_by_department\": {\n          \"avg\": {\n            \"field\": \"salary\"\n          }\n        }\n      }\n    }\n  }\n}\nAdd a filters sub-aggregation for each job_title.\nGET employees/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"employees_by_department\": {\n      \"terms\": {\n        \"field\": \"department\"\n      },\n      \"aggs\": {\n        \"avg_salary_by_department\": {\n          \"avg\": {\n            \"field\": \"salary\"\n          }\n        },\n        \"employees_by_title\": {\n          \"filters\": {\n            \"filters\": {\n              \"Managers\": {\n                \"term\": {\n                  \"job_title\": \"Manager\"\n                }\n              },\n              \"Representative\" : {\n                \"term\": {\n                  \"job_title\": \"Representative\"\n                }\n              },\n              \"Coordinator\" : {\n                \"term\": {\n                  \"job_title\": \"Coordinator\"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index creation and mappings.\nGET /employees\nVerify the test documents are in the index.\nGET /employees/_search\nVerify that the employees are grouped correctly by department and job title and that the average salary is calculated correctly for each department.\n{\n  ...\n  \"aggregations\": {\n    \"employees_by_department\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Marketing\",\n          \"doc_count\": 2,\n          \"avg_salary_by_department\": {\n            \"value\": 105000\n          },\n          \"employees_by_title\": {\n            \"buckets\": {\n              \"Coordinator\": {\n                \"doc_count\": 1\n              },\n              \"Managers\": {\n                \"doc_count\": 1\n              },\n              \"Representative\": {\n                \"doc_count\": 0\n              }\n            }\n          }\n        },\n        {\n          \"key\": \"Sales\",\n          \"doc_count\": 2,\n          \"avg_salary_by_department\": {\n            \"value\": 90000\n          },\n          \"employees_by_title\": {\n            \"buckets\": {\n              \"Coordinator\": {\n                \"doc_count\": 0\n              },\n              \"Managers\": {\n                \"doc_count\": 1\n              },\n              \"Representative\": {\n                \"doc_count\": 1\n              }\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe department field must be of type keyword.\nSetting size to 0 ensures the search doesn’t return any documents, focusing solely on the aggregations.\nThe terms aggregation is used to group employees by department.\nThe avg sub-aggregation is used to calculate the average salary per department.\nThe filters sub-aggregation is used to group employees by job_title.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE employees\n\n\n\nDocumentation\n\nAggregations\nAvg Aggregation\nFilters Aggregation\nRange Aggregation\nTerms Aggregation\n\n\n\n\nExample 3: Creating aggregations and sub-aggregations for application logs by Hour and Log Level\n\nRequirements\n\nAnalyze application logs stored in an Elasticsearch index named app-logs.\nUse a date_histogram aggregation to group logs by the hour.\nWithin each hour bucket, create a sub-aggregation to group logs by their severity level (log_level).\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate a new index called app-logs.\nPUT app-logs\n{\n  \"mappings\": {\n    \"properties\": {\n      \"@timestamp\": {\n        \"type\": \"date\"\n      },\n      \"log_level\": {\n        \"type\": \"keyword\"\n      },\n      \"message\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\nInsert sample data.\nPOST /app-logs/_bulk\n{\"index\":{},\"_id\":\"1\"}\n{\"@timestamp\":\"2024-05-24T10:30:00\",\"log_level\":\"INFO\",\"message\":\"Application started successfully.\"}\n{\"index\":{},\"_id\":\"2\"}\n{\"@timestamp\":\"2024-05-24T11:15:00\",\"log_level\":\"WARNING\",\"message\":\"Potential memory leak detected.\"}\n{\"index\":{},\"_id\":\"3\"}\n{\"@timestamp\":\"2024-05-24T12:00:00\",\"log_level\":\"ERROR\",\"message\":\"Database connection failed.\"}\n{\"index\":{},\"_id\":\"4\"}\n{\"@timestamp\":\"2024-05-24T10:45:00\",\"log_level\":\"DEBUG\",\"message\":\"Processing user request.\"}\nUse a date_histogram aggregation to group logs by the hour.\nGET app-logs/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"logs_by_the_hour\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"fixed_interval\": \"1h\"\n      }\n    }\n  }\n}\nWithin each hour bucket, create a sub-aggregation to group logs by their severity level (log_level).\nGET app-logs/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"logs_by_the_hour\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"fixed_interval\": \"1h\"\n      },\n      \"aggs\": {\n        \"log_severity\": {\n          \"terms\": {\n            \"field\": \"log_level\"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index creation and mappings.\nGET /app-logs\nVerify the test documents are in the index.\nGET /app-logs/_search\nRun the search query and examine the response.\n{\n  ...\n  \"aggregations\": {\n    \"logs_by_the_hour\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2024-05-24T10:00:00.000Z\",\n          \"key\": 1716544800000,\n          \"doc_count\": 2,\n          \"log_severity\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n              {\n                \"key\": \"DEBUG\",\n                \"doc_count\": 1\n              },\n              {\n                \"key\": \"INFO\",\n                \"doc_count\": 1\n              }\n            ]\n          }\n        },\n        {\n          \"key_as_string\": \"2024-05-24T11:00:00.000Z\",\n          \"key\": 1716548400000,\n          \"doc_count\": 1,\n          \"log_severity\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n              {\n                \"key\": \"WARNING\",\n                \"doc_count\": 1\n              }\n            ]\n          }\n        },\n        {\n          \"key_as_string\": \"2024-05-24T12:00:00.000Z\",\n          \"key\": 1716552000000,\n          \"doc_count\": 1,\n          \"log_severity\": {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\": [\n              {\n                \"key\": \"ERROR\",\n                \"doc_count\": 1\n              }\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\nConsiderations\n\nSetting size to 0 ensures the search doesn’t return any documents, focusing solely on the aggregations.\nThe date_histogram aggregation groups documents based on the @timestamp field with an interval of one hour.\nThe nested terms aggregation within the logs_by_hour aggregation counts the occurrences of each unique log_level within each hour bucket.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE app-logs\n\n\n\nDocumentation\n\nBucket Aggregations\nDate Histogram Aggregation\nTerms Aggregation\n\n\n\n\nExample 4: Finding the Stock with the Highest Daily Volume of the Month\nThis is taken from a webinar by Elastic to show a sample question and answer to the Certified Engineer Exam. Their answer was wrong and didn’t need aggregations.\n\nRequirements\n\nCreate a query to find the stock with the highest daily volume for the current month.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nIndex sample data:\n\nUse the _bulk endpoint to index sample stock data.\nEnsure the data includes fields for stock_name, date, and volume.\nPOST _bulk\n{ \"index\": { \"_index\": \"stocks\", \"_id\": \"1\" } }\n{ \"stock_name\": \"AAPL\", \"date\": \"2024-07-01\", \"volume\": 1000000 }\n{ \"index\": { \"_index\": \"stocks\", \"_id\": \"2\" } }\n{ \"stock_name\": \"AAPL\", \"date\": \"2024-07-02\", \"volume\": 1500000 }\n{ \"index\": { \"_index\": \"stocks\", \"_id\": \"3\" } }\n{ \"stock_name\": \"GOOGL\", \"date\": \"2024-07-01\", \"volume\": 2000000 }\n{ \"index\": { \"_index\": \"stocks\", \"_id\": \"4\" } }\n{ \"stock_name\": \"GOOGL\", \"date\": \"2024-07-02\", \"volume\": 2500000 }\n{ \"index\": { \"_index\": \"stocks\", \"_id\": \"5\" } }\n{ \"stock_name\": \"MSFT\", \"date\": \"2024-07-01\", \"volume\": 3000000 }\n{ \"index\": { \"_index\": \"stocks\", \"_id\": \"6\" } }\n{ \"stock_name\": \"MSFT\", \"date\": \"2024-07-02\", \"volume\": 3500000 }\n{ \"index\": { \"_index\": \"stocks\", \"_id\": \"7\" } }\n{ \"stock_name\": \"TSLA\", \"date\": \"2024-07-01\", \"volume\": 4000000 }\n{ \"index\": { \"_index\": \"stocks\", \"_id\": \"8\" } }\n{ \"stock_name\": \"TSLA\", \"date\": \"2024-07-02\", \"volume\": 4500000 }\n{ \"index\": { \"_index\": \"stocks\", \"_id\": \"9\" } }\n{ \"stock_name\": \"AMZN\", \"date\": \"2024-07-01\", \"volume\": 5000000 }\n{ \"index\": { \"_index\": \"stocks\", \"_id\": \"10\" } }\n{ \"stock_name\": \"AMZN\", \"date\": \"2024-07-02\", \"volume\": 5500000 }\n\nCreate the query. The stocks in the index are all from July, but you want just the stocks for the latest month. Update the above dates so the query will work for you.\n  GET stocks/_search\n  {\n    \"size\": 1, \n    \"query\": {\n      \"range\": {\n        \"date\": {\n          \"gte\": \"now/M\",\n          \"lte\": \"now\"\n        }\n      }\n    }\n  }\nThe results of the query should be all the stocks from a given month. Now sort those stocks by their volume and display the top pick.\nGET stocks/_search\n{\n  \"size\": 1, \n  \"query\": {\n    \"range\": {\n      \"date\": {\n        \"gte\": \"now/M\",\n        \"lte\": \"now\"\n      }\n    }\n  },\n  \"sort\": [\n    {\n      \"volume\": {\n        \"order\": \"desc\"\n      }\n    }\n  ]\n}\n\n\n\nTest\n\nVerify the index creation and mappings.\nGET /stocks\nVerify the test documents are in the index.\nGET /stocks/_search\nRun the query and confirm that the stock with the highest daily volume of the month is displayed.\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"stocks\",\n        \"_id\": \"10\",\n        \"_score\": null,\n        \"_source\": {\n          \"stock_name\": \"AMZN\",\n          \"date\": \"2024-07-02\",\n          \"volume\": 5500000\n        },\n        \"sort\": [\n          5500000\n        ]\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nThe range clause returned the stocks for the current month\nThe sort clause brought the highest volume of any stock to the top and size of 1 displayed that one record\n\n\n\nClean-up (Optional)\n\nDelete the stocks index to clean up the data:\nDELETE /stocks\n\n\n\nDocumentation\n\nElasticsearch Bulk API\nElasticsearch Date Histogram Aggregation\nElasticsearch Max Aggregation\nElasticsearch Top Hits Aggregation\n\n\n\n\nExample 5: Aggregating Sales Data by Month with Sub-Aggregation of Total Sales Value\n\nRequirements\n\nAggregate e-commerce sales data by month, creating at least 12 date buckets.\nPerform a sub-aggregation to calculate the total sales value within each month.\n\n\n\nSteps\n\nIndex Sample Sales Documents Using _bulk Endpoint:\nPOST /sales_data/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"order_date\": \"2023-01-15\", \"product\": \"Yoo-hoo Beverage\", \"quantity\": 10, \"price\": 1.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"order_date\": \"2023-02-20\", \"product\": \"Apple iPhone 12\", \"quantity\": 1, \"price\": 799.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"order_date\": \"2023-03-05\", \"product\": \"Choco-Lite Bar\", \"quantity\": 25, \"price\": 0.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"order_date\": \"2023-04-10\", \"product\": \"Nike Air Max 270\", \"quantity\": 3, \"price\": 150.00 }\n{ \"index\": { \"_id\": \"5\" } }\n{ \"order_date\": \"2023-05-18\", \"product\": \"Samsung Galaxy S21\", \"quantity\": 2, \"price\": 699.99 }\n{ \"index\": { \"_id\": \"6\" } }\n{ \"order_date\": \"2023-06-22\", \"product\": \"Yoo-hoo Beverage\", \"quantity\": 15, \"price\": 1.99 }\n{ \"index\": { \"_id\": \"7\" } }\n{ \"order_date\": \"2023-07-03\", \"product\": \"Choco-Lite Bar\", \"quantity\": 30, \"price\": 0.99 }\n{ \"index\": { \"_id\": \"8\" } }\n{ \"order_date\": \"2023-08-25\", \"product\": \"Apple iPhone 12\", \"quantity\": 1, \"price\": 799.99 }\n{ \"index\": { \"_id\": \"9\" } }\n{ \"order_date\": \"2023-09-10\", \"product\": \"Nike Air Max 270\", \"quantity\": 4, \"price\": 150.00 }\n{ \"index\": { \"_id\": \"10\" } }\n{ \"order_date\": \"2023-10-15\", \"product\": \"Samsung Galaxy S21\", \"quantity\": 1, \"price\": 699.99 }\n{ \"index\": { \"_id\": \"11\" } }\n{ \"order_date\": \"2023-11-20\", \"product\": \"Yoo-hoo Beverage\", \"quantity\": 20, \"price\": 1.99 }\n{ \"index\": { \"_id\": \"12\" } }\n{ \"order_date\": \"2023-12-30\", \"product\": \"Choco-Lite Bar\", \"quantity\": 50, \"price\": 0.99 }\nBucket the order_date using a Date Histogram Aggregation with Sub-Aggregation:\n\nUse a date_histogram to create monthly buckets and a sum sub-aggregation to calculate total sales within each month.\nGET /sales_data/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"sales_over_time\": {\n      \"date_histogram\": {\n        \"field\": \"order_date\",\n        \"calendar_interval\": \"month\",\n        \"format\": \"yyyy-MM\"\n      },\n      \"aggs\": {\n        \"total_sales\": {\n          \"sum\": {\n            \"field\": \"total_value\"\n          }\n        }\n      }\n    }\n  }\n}\n\nCalculate the Total Value:\n\nBefore running the above aggregation, ensure that each document includes a total_value field. You could either compute it on the client side or dynamically compute it using an ingest pipeline or a script during the aggregation process.\nFor simplicity, let’s assume the total_value is calculated as quantity * price:\nPOST /sales_data/_update_by_query\n{\n  \"script\": {\n    \"source\": \"ctx._source.total_value = ctx._source.quantity * ctx._source.price\"\n  },\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\n\nTest\n\nRun the above GET /sales_data/_search query.\nCheck the output to see 12 date buckets, one for each month, with the total_sales value for each bucket.\n\n\n\nConsiderations\n\nThe date_histogram aggregation is ideal for grouping records by time intervals such as months, weeks, or days.\nThe sum sub-aggregation allows you to calculate the total value of sales within each date bucket.\nEnsure that the total_value field is correctly calculated, as this impacts the accuracy of the sub-aggregation.\n\n\n\nClean-up (Optional)\n\nDelete the stocks index to clean up the data:\nDELETE /sales_data\n\n\n\nDocumentation\n\nDate Histogram Aggregation\nSum Aggregation\nUpdate By Query API\nBulk API",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Searching Data</span>"
    ]
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-a-query-that-searches-across-multiple-clusters",
    "href": "2-searching-data.html#task-write-and-execute-a-query-that-searches-across-multiple-clusters",
    "title": "2  Searching Data",
    "section": "2.6 Task: Write and execute a query that searches across multiple clusters",
    "text": "2.6 Task: Write and execute a query that searches across multiple clusters\nIf you are running your instance of Elasticsearch locally, and need to create an additional cluster so that you can run these examples, go to the Appendix: Adding a Cluster to your Elasticsearch Instance for information on how to set up an additional single-node cluster.\n\nExample 1: Creating search queries for Products in Multiple Clusters\n\nRequirements\n\nSet up two single-node clusters on localhost or Elastic Cloud.\nCreate an index in each cluster.\nIndex at least four documents in each cluster using the _bulk endpoint.\nConfigure cross-cluster search.\nExecute a cross-cluster search query.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nSet up multiple clusters on localhost.\n\n\nAssume you have two clusters, es01 and es02 and they have been set up as directed in the Appendix.\nIn the local cluster, configure communication between the clusters by updating the local cluster settings.\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n          \"seeds\": [\n            \"es01:9300\"\n          ],\n          \"skip_unavailable\": true\n        },\n        \"es02\": {\n          \"seeds\": [\n            \"es02:9300\"\n          ],\n          \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}\n\n\nCreate a product index in each cluster.\n\n\nFrom the Kibana Console (es01)\nPUT /products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\nFrom the command line (es02).\ncurl -u elastic:[your password here] -X PUT \"http://localhost:9201/products?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}'\n\n\nIndex product documents into each cluster.\n\n\nFor es01:\nPOST /products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\nFor es02 through the command line (note that the final single quote is on a line by itself):\ncurl -u elastic:[your password here] -X POST \"http://localhost:9201/products/_bulk?pretty\" -H 'Content-Type: application/json' -d'\n{ \"index\": { \"_id\": \"5\" } }\n{ \"product\": \"Elasticsearch Stickers\", \"category\": \"Accessories\", \"price\": 4.99 }\n{ \"index\": { \"_id\": \"6\" } }\n{ \"product\": \"Elasticsearch Notebook\", \"category\": \"Stationery\", \"price\": 7.99 }\n{ \"index\": { \"_id\": \"7\" } }\n{ \"product\": \"Elasticsearch Pen\", \"category\": \"Stationery\", \"price\": 3.49 }\n{ \"index\": { \"_id\": \"8\" } }\n{ \"product\": \"Elasticsearch Hoodie\", \"category\": \"Apparel\", \"price\": 45.99 }\n'\n\n\nConfigure Cross-Cluster Search (CCS).\n\n\nIn the local cluster, ensure the remote cluster is configured by checking the settings:\nGET /_cluster/settings?include_defaults=true&filter_path=defaults.cluster.remote\n\n\nExecute a Cross-Cluster Search query.\nGET /products,es02:products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"product\": \"Elasticsearch\"\n    }\n  }\n}\n\n\n\nTest\n\nVerify the index creation.\nGET /products\nFrom the command line execute:\ncurl -u elastic:[your password here] -X GET \"http://localhost:9201/products?pretty\"\nVerify that the documents have been indexed.\nGET /products/_search\nGET /es02:products/_search\nEnsure the remote cluster is correctly configured and visible from the local cluster.\nGET /_remote/info\nExecute a Cross-Cluster Search query.\nGET /products,es02:products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"product\": \"Elasticsearch\"\n    }\n  }\n}\n\n\n\nConsiderations\n\nCross-cluster search is useful for querying data across multiple Elasticsearch clusters, providing a unified search experience.\nEnsure the remote cluster settings are correctly configured in the cluster settings.\nProperly handle the index names to avoid conflicts and ensure clear distinction between clusters.\n\n\n\nClean-up (optional)\n\nDelete the es01 index.\nDELETE products\nDelete the es02 index from the command line.\ncurl -u elastic:[your password here] -X DELETE \"http://localhost:9201/products?pretty\"\n\n\n\nDocumentation\n\nBulk API\nCross-Cluster Search\nCreate Index API\nIndex Document API",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Searching Data</span>"
    ]
  },
  {
    "objectID": "2-searching-data.html#task-write-and-execute-a-search-that-utilizes-a-runtime-field",
    "href": "2-searching-data.html#task-write-and-execute-a-search-that-utilizes-a-runtime-field",
    "title": "2  Searching Data",
    "section": "2.7 Task: Write and execute a search that utilizes a runtime field",
    "text": "2.7 Task: Write and execute a search that utilizes a runtime field\n\nExample 1: Creating search queries for products with a runtime field for discounted prices\n\nRequirements\n\nCreate an index.\nIndex four documents.\nDefine a runtime field.\nExecute a search query that creates a query-time runtime field with a 10% discount\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate an index.\n\nPUT /product_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\nIndex some documents.\n\nPOST /product_index/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"price\": 29.99, \"category\": \"Books\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"price\": 39.99, \"category\": \"Books\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"price\": 19.99, \"category\": \"Apparel\" }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"price\": 12.99, \"category\": \"Apparel\" }\n\nDefine a query-time runtime field to return a discounted price.\nGET product_index/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\n    \"product\", \"price\", \"discounted_price\"\n  ], \n  \"runtime_mappings\": {\n    \"discounted_price\": {\n      \"type\": \"double\",\n      \"script\": {\n        \"source\": \"emit(doc['price'].value * 0.9)\"\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify the creation of the index and its mappings.\nGET /product_index\nVerify the indexed documents.\nGET /product_index/_search\nExecute the query and confirm the discounted_price.\n{\n  ...\n    \"hits\": [\n      {\n        ...\n        \"fields\": {\n          \"product\": [\n            \"Elasticsearch Guide\"\n          ],\n          \"price\": [\n            29.99\n          ],\n          \"discounted_price\": [\n            26.991\n          ]\n        }\n      },\n      {\n        ...\n        \"fields\": {\n          \"product\": [\n            \"Advanced Elasticsearch\"\n          ],\n          \"price\": [\n            39.99\n          ],\n          \"discounted_price\": [\n            35.991\n          ]\n        }\n      },\n      {\n        ...\n        \"fields\": {\n          \"product\": [\n            \"Elasticsearch T-shirt\"\n          ],\n          \"price\": [\n            19.99\n          ],\n          \"discounted_price\": [\n            17.991\n          ]\n        }\n      },\n      {\n        ...\n        \"fields\": {\n          \"product\": [\n            \"Elasticsearch Mug\"\n          ],\n          \"price\": [\n            12.99\n          ],\n          \"discounted_price\": [\n            11.691\n          ]\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nRuntime fields allow for dynamic calculation of field values at search time, useful for complex calculations or when the field values are not stored.\nThe script in the runtime field calculates the discounted price by applying a 10% discount to the price field.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE product_index\n\n\n\nDocumentation\n\nCreate Index API\nBulk API\nIndex Document API\nRuntime Fields\n\n\n\n\nExample 2: Creating search queries for employees with a calculated total salary\nIn this example, the runtime field is defined as part of the index that executes code when documents are indexed. The salary field is read at index time to create a new value for the runtime field total_salary.\n\nRequirements\n\nAn index (employees) with documents containing employee information (name, department, salary)and a runtime field (total_salary) to calculate the total salary of each employee.\nA search query to retrieve employees with a total salary above $65,000.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the employees index with a mapping for the runtime field.\nPUT employees\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"department\": {\n        \"type\": \"text\"\n      },\n      \"salary\": {\n        \"type\": \"integer\"\n      },\n      \"total_salary\": {\n        \"type\": \"long\",\n        \"script\": {\n          \"source\": \"emit(doc['salary'].value * 12)\"\n        }\n      }\n    }\n  }\n}\nIndex some documents that contain a monthly salary.\nPOST /employees/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"John Doe\", \"department\": \"Sales\", \"salary\": 4000 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Jane Smith\", \"department\": \"Marketing\", \"salary\": 6000 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"name\": \"Bob Johnson\", \"department\": \"IT\", \"salary\": 7000 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"name\": \"Alice Brown\", \"department\": \"HR\", \"salary\": 5000 }\nExecute a search query with a runtime field.\nGET employees/_search\n{\n  \"query\": {\n    \"range\": {\n      \"total_salary\": {\n        \"gte\": 65000\n      }\n    }\n  },\n  \"fields\": [\n    \"total_salary\"\n  ]\n}\n\n\n\nTest\n\nVerify the creation of the index and its mappings.\nGET /employees\nVerify the indexed documents.\nGET /employees/_search\nExecute the query and verify the search results contain only employees with a total salary above 65000.\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"employees\",\n        \"_id\": \"2\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Jane Smith\",\n          \"department\": \"Marketing\",\n          \"salary\": 6000\n        },\n        \"fields\": {\n          \"total_salary\": [\n            72000\n          ]\n        }\n      },\n      {\n        \"_index\": \"employees\",\n        \"_id\": \"3\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Bob Johnson\",\n          \"department\": \"IT\",\n          \"salary\": 7000\n        },\n        \"fields\": {\n          \"total_salary\": [\n            84000\n          ]\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nRuntime fields are calculated on the fly and can be used in search queries, aggregations, and sorting.\nThe script used in the runtime field calculates the total salary by multiplying the monthly salary by 12 months.\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE employees\n\n\n\nDocumentation\n\nMap a Runtime Field\nScript Fields\n\n\n\n\nExample 3: Creating search queries with a runtime field for restaurant data\n\nRequirements\n\nCreate a search query for restaurants in New York City.\nInclude the restaurant’s name, cuisine, and a calculated rating_score in the search results.\n\nthe rating_score is calculated by taking the square root of the product of the review_score and number_of_reviews.\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate a restaurant index.\nPUT restaurants\n{\n  \"mappings\": {\n    \"properties\": {\n      \"city\": {\n        \"type\": \"keyword\"\n      },\n      \"cuisine\": {\n        \"type\": \"text\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"number_of_reviews\": {\n        \"type\": \"long\"\n      },\n      \"review_score\": {\n        \"type\": \"float\"\n      },\n      \"state\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\nIndex some sample restaurant documents.\nPOST /restaurants/_bulk\n{ \"index\": { \"_id\": 1 } }\n{ \"name\": \"Tasty Bites\", \"city\": \"New York\", \"state\": \"NY\", \"cuisine\": \"Italian\", \"review_score\": 4.5, \"number_of_reviews\": 200 }\n{ \"index\": { \"_id\": 2 } }\n{ \"name\": \"Spicy Palace\", \"city\": \"Los Angeles\", \"state\": \"CA\", \"cuisine\": \"Indian\", \"review_score\": 4.2, \"number_of_reviews\": 150 }\n{ \"index\": { \"_id\": 3 } }\n{ \"name\": \"Sushi Spot\", \"city\": \"San Francisco\", \"state\": \"CA\", \"cuisine\": \"Japanese\", \"review_score\": 4.7, \"number_of_reviews\": 300 }\n{ \"index\": { \"_id\": 4 } }\n{ \"name\": \"Burger Joint\", \"city\": \"Chicago\", \"state\": \"IL\", \"cuisine\": \"American\", \"review_score\": 3.8, \"number_of_reviews\": 100 }\nCreate a query to return restaurants based from New York City.\nGET restaurants/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"city\": {\n              \"value\": \"New York\"\n            }\n          }\n        },\n        {\n          \"term\": {\n            \"state\": {\n              \"value\": \"NY\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\nDefine a runtime field named weighted_rating to calculate a weighted rating score for New York restaurants.\nGET restaurants/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"city\": {\n              \"value\": \"New York\"\n            }\n          }\n        },\n        {\n          \"term\": {\n            \"state\": {\n              \"value\": \"NY\"\n            }\n          }\n        }\n      ]\n    }\n  }, \n  \"runtime_mappings\": {\n    \"rating_score\": {\n      \"type\": \"double\",\n      \"script\": {\n        \"source\": \"emit(Math.sqrt(doc['review_score'].value * doc['number_of_reviews'].value))\"\n      }\n    }\n  },\n  \"fields\": [\n    \"rating_score\"\n  ]\n}\n\n\n\nTest\n\nVerify the creation of the index and its mappings.\nGET /restaurants\nVerify the indexed documents.\nGET /restaurants/_search\nExecute the query and verify the restaurant name, cuisine type, and the calculated weighted rating score for restaurants located in New York, NY.\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"restaurants\",\n        \"_id\": \"1\",\n        \"_score\": 2.4079456,\n        \"_source\": {\n          \"name\": \"Tasty Bites\",\n          \"city\": \"New York\",\n          \"state\": \"NY\",\n          \"cuisine\": \"Italian\",\n          \"review_score\": 4.5,\n          \"number_of_reviews\": 200\n        },\n        \"fields\": {\n          \"rating_score\": [\n            30\n          ]\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nThe runtime_mappings section defines a new field weighted_rating that calculates a weighted rating score based on the review_score and number_of_reviews fields.\nThe query section uses the term query to search for restaurants in New York, NY.\nThe fields section specifies the fields to include in the search results (in this case, the runtime field weighted_rating).\n\n\n\nClean-up (optional)\n\nDelete the index.\nDELETE restaurants\n\n\n\nDocumentation\n\nRuntime Fields in the Search Request",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Searching Data</span>"
    ]
  },
  {
    "objectID": "3-developing-search-applications.html",
    "href": "3-developing-search-applications.html",
    "title": "3  Developing Search Applications",
    "section": "",
    "text": "3.1 Task: Highlight the search terms in the response of a query",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Developing Search Applications</span>"
    ]
  },
  {
    "objectID": "3-developing-search-applications.html#task-highlight-the-search-terms-in-the-response-of-a-query",
    "href": "3-developing-search-applications.html#task-highlight-the-search-terms-in-the-response-of-a-query",
    "title": "3  Developing Search Applications",
    "section": "",
    "text": "Example 1: Creating search queries w/highlighting for blog posts\n\nRequirements\n\nPerform a search query which highlights the search term “elasticsearch”\n\n\n\nSteps\n\nOpen the Kibana Console or Use a REST Client\nCreate and populate the Index\nPOST /blog_posts/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"title\": \"Introduction to Elasticsearch\", \"content\": \"Elasticsearch is a powerful search engine.\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"title\": \"Advanced Elasticsearch Techniques\", \"content\": \"This guide covers advanced features of Elasticsearch.\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"title\": \"Elasticsearch Performance Tuning\", \"content\": \"Learn how to optimize Elasticsearch for better performance.\" }\nCreate a search query using the highlight clause (the field being searched must match the field to be highlighted)\nGET /blog_posts/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content\": \"elasticsearch\"\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"content\": {}\n    }\n  }\n}\n\n\n\nTest\n\nConfirm the index exists\nGET /blog_posts\nExecute the query and confirm that the content field has highlighting\n{\n  ...\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_id\": \"1\",\n        \"_source\": {\n          \"title\": \"Introduction to Elasticsearch\",\n          \"content\": \"Elasticsearch is a powerful search engine.\"\n        },\n        \"highlight\": {\n          \"content\": [\n            \"&lt;em&gt;Elasticsearch&lt;/em&gt; is a powerful search engine.\"\n          ]\n        }\n      }\n      // Additional documents...\n    ]\n  }\n}\n\n\n\nConsiderations\n\nField Selection: The highlight field in the search request specifies which fields to highlight. In this example, we highlight the content field.\nPerformance: Highlighting can impact search performance, especially on large datasets. It is essential to balance the need for highlighting with performance considerations.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE blog_posts\n\n\n\nDocumentation\n\nBulk API\nHighlighting\nMatch Query\n\n\n\n\nExample 2: Creating search queries w/highlighting for customer order data\n\nRequirements\n\nAn orders index with documents containing customer order information including customer_name, order_date, products, total_price.\nA search query to retrieve orders\n\nSearch for Product A and a range for price called total_price\nHighlight the search terms in the products nested object\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the orders index by indexing some documents\nPOST /orders/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"customer_name\": \"John Doe\", \"order_date\": \"2022-01-01\", \"products\": [{ \"name\": \"Product A\", \"price\": 10.99 },{ \"name\": \"Product B\", \"price\": 5.99 }], \"total_price\": 16.98 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"customer_name\": \"Jane Smith\", \"order_date\": \"2022-01-15\", \"products\": [{ \"name\": \"Product B\", \"price\": 5.99 },{ \"name\": \"Product C\", \"price\": 7.99 }], \"total_price\": 13.98 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"customer_name\": \"Bob Johnson\", \"order_date\": \"2022-02-01\", \"products\": [{ \"name\": \"Product A\", \"price\": 10.99 },{ \"name\": \"Product C\", \"price\": 7.99 }], \"total_price\": 18.98 }\nExecute a search query with highlighting including custom pre_tags and post_tags\nGET orders/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"match_phrase\": {\n            \"products.name\": \"product a\"\n          }\n        },\n        {\n          \"range\": {\n            \"total_price\": {\n              \"gt\": 10\n            }\n          }\n        }\n      ]\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"products.name\": {\n        \"pre_tags\": [\n          \"&lt;b&gt;\"\n        ],\n        \"post_tags\": [\n          \"&lt;/b&gt;\"\n        ]\n      }\n    }\n  }\n}\n\n\n\nTest\n\nConfirm the index exists\nGET /orders\nExecute the query and confirm that products.name has highlighting\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"orders\",\n        \"_id\": \"1\",\n        \"_score\": 1.603535,\n        \"_source\": {\n          \"customer_name\": \"John Doe\",\n          \"order_date\": \"2022-01-01\",\n          \"products\": [\n            {\n              \"name\": \"Product A\", \"price\": 10.99\n            },\n            {\n              \"name\": \"Product B\", \"price\": 5.99\n            }\n          ],\n          \"total_price\": 16.98\n        },\n        \"highlight\": {\n          \"products.name\": [\n            \"&lt;b&gt;Product A&lt;/b&gt;\"\n          ]\n        }\n      },\n      {\n        \"_index\": \"orders\",\n        \"_id\": \"3\",\n        \"_score\": 1.603535,\n        \"_source\": {\n          \"customer_name\": \"Bob Johnson\",\n          \"order_date\": \"2022-02-01\",\n          \"products\": [\n            {\n              \"name\": \"Product A\", \"price\": 10.99\n            },\n            {\n              \"name\": \"Product C\", \"price\": 7.99\n            }\n          ],\n          \"total_price\": 18.98\n        },\n        \"highlight\": {\n          \"products.name\": [\n            \"&lt;b&gt;Product A&lt;/b&gt;\"\n          ]\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nHighlighting is used to emphasize the search terms in the response, making it easier to see why a document matched the query.\nThe highlight section in the search query specifies which fields to highlight and how to format the highlighted text.\nNested objects (products) are highlighted using the fields section with dot notation (products.name, products.price).\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE orders\n\n\n\nDocumentation\n\nBulk API\nHighlighting\nMatch Query\nNested Objects\nSearch API",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Developing Search Applications</span>"
    ]
  },
  {
    "objectID": "3-developing-search-applications.html#task-sort-the-results-of-a-query-by-a-given-set-of-requirements",
    "href": "3-developing-search-applications.html#task-sort-the-results-of-a-query-by-a-given-set-of-requirements",
    "title": "3  Developing Search Applications",
    "section": "3.2 Task: Sort the results of a query by a given set of requirements",
    "text": "3.2 Task: Sort the results of a query by a given set of requirements\n\nExample 1: Creating Search Queries w/ Sorting for e-commerce products\n\nRequirements\n\nSearch for e-commerce product data in an index named products.\nSort the results by two criteria:\n\nPrimary Sort: In descending order by product price (highest to lowest).\nSecondary Sort: In ascending order by product name (alphabetically).\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the products index\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"keyword\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      }\n    }\n  }\n}\nIndex some documents\nPUT /products/_bulk\n{\"index\":{},\"action\":\"index\",\"_id\":\"1\"}\n{\"name\":\"Headphones\",\"price\":79.99}\n{\"index\":{},\"action\":\"index\",\"_id\":\"2\"}\n{\"name\":\"Smartwatch\",\"price\":249.99}\n{\"index\":{},\"action\":\"index\",\"_id\":\"3\"}\n{\"name\":\"Laptop\",\"price\":1299.99}\n{\"index\":{},\"action\":\"index\",\"_id\":\"4\"}\n{\"name\":\"Wireless Speaker\",\"price\":99.99}\nDefine a query to match_all and then perform the primary sort of price highest to lowest.\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"desc\"\n      }\n    }\n  ]\n}\nDefine a query to perform the secondary sort of name in alphabetical order (asc).\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"name\": {\n        \"order\": \"asc\"\n      }\n    }\n  ]\n}\nCombine the two sorts and their impact on the results (try the sort with name first and price second and see how the results change)\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"desc\"\n      }\n    },\n    {\n      \"name\": {\n        \"order\": \"asc\"\n      }\n    }\n  ]\n}\n\n\n\nTest\n\nConfirm the index exists\nGET /products\nRun the search queries and examine the response\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"products\",\n        \"_id\": \"nXpN-pABRRh1FLFi7Ks8\",\n        \"_score\": null,\n        \"_source\": {\n          \"name\": \"Laptop\",\n          \"price\": 1299.99\n        },\n        \"sort\": [\n          1299.99,\n          \"Laptop\"\n        ]\n      },\n      {\n        \"_index\": \"products\",\n        \"_id\": \"nHpN-pABRRh1FLFi7Ks8\",\n        \"_score\": null,\n        \"_source\": {\n          \"name\": \"Smartwatch\",\n          \"price\": 249.99\n        },\n        \"sort\": [\n          249.99,\n          \"Smartwatch\"\n        ]\n      },\n      {\n        \"_index\": \"products\",\n        \"_id\": \"nnpN-pABRRh1FLFi7Ks8\",\n        \"_score\": null,\n        \"_source\": {\n          \"name\": \"Wireless Speaker\",\n          \"price\": 99.99\n        },\n        \"sort\": [\n          99.99,\n          \"Wireless Speaker\"\n        ]\n      },\n      {\n        \"_index\": \"products\",\n        \"_id\": \"m3pN-pABRRh1FLFi7Ks8\",\n        \"_score\": null,\n        \"_source\": {\n          \"name\": \"Headphones\",\n          \"price\": 79.99\n        },\n        \"sort\": [\n          79.99,\n          \"Headphones\"\n        ]\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nThe sort clause defines the sorting criteria.\nAn array of sort definitions is specified, prioritizing them from top to bottom.\nIn this example, price is sorted in descending order (desc), while name is sorted in ascending order (asc).\n\n\n\nClean-up (optional)\n\nDelete the index\n\nDELETE products\n\n\nDocumentation\n\nSort Search Results",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Developing Search Applications</span>"
    ]
  },
  {
    "objectID": "3-developing-search-applications.html#task-implement-pagination-of-the-results-of-a-search-query",
    "href": "3-developing-search-applications.html#task-implement-pagination-of-the-results-of-a-search-query",
    "title": "3  Developing Search Applications",
    "section": "3.3 Task: Implement pagination of the results of a search query",
    "text": "3.3 Task: Implement pagination of the results of a search query\nThere is only one example here as pagination is rather simple with very few configuration options.\n\nExample 1: Creating pagination queries for an e-commerce product catalog\n\nRequirements\n\nAn index named products with documents containing fields like name, price, category, description, etc.\nImplement pagination to retrieve search results in batches of 2 documents at a time.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nIndex sample products documents\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Product A\",\"price\":99.99,\"category\":\"Electronics\",\"description\":\"High-quality product\"}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Product B\",\"price\":49.99,\"category\":\"Books\",\"description\":\"Best-selling novel\"}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Product C\",\"price\":149.99,\"category\":\"Electronics\",\"description\":\"Top-rated gadget\"}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Product D\",\"price\":29.99,\"category\":\"Clothing\",\"description\":\"Stylish t-shirt\"}\n{\"index\":{\"_id\":5}}\n{\"name\":\"Product E\",\"price\":19.99,\"category\":\"Books\",\"description\":\"Classic literature\"}\nDefine the initial search query with pagination (notice the use of size)\nGET products/_search\n{\n  \"size\": 2, \n  \"query\": {\n    \"match_all\": {}\n  },\n  \"from\": 0\n}\nTo retrieve the next page of results, use one of two methods:\n\nUpdate the from field with the document count to proceed from (not the document id)\nGET products/_search\n{\n  \"size\": 2, \n  \"query\": {\n    \"match_all\": {}\n  },\n  \"from\": 2\n}\nOR If you are sorting the documents as well as paginating then you can use the search_after parameter along with the sort values from the last hit in the previous page\n// search with sort on page 1\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    \"_doc\"\n  ],\n  \"size\": 2\n}\n// second search using the sort value \n// from the last document of the previous search\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    \"_doc\"\n  ],\n  \"size\": 2,\n  \"search_after\": [6]\n}\n\n\n\n\nTest\n\nConfirm the index exists\nGET /products\nExecute the initial search query to retrieve the first 2 documents\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"products\",\n        \"_id\": \"1\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Product A\",\n          \"price\": 99.99,\n          \"category\": \"Electronics\",\n          \"description\": \"High-quality product\"\n        }\n      },\n      {\n        \"_index\": \"products\",\n        \"_id\": \"2\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Product B\",\n          \"price\": 49.99,\n          \"category\": \"Books\",\n          \"description\": \"Best-selling novel\"\n        }\n      }\n    ]\n  }\n}\nChange from to 2 and execute it again to get the next 2 items.\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"products\",\n        \"_id\": \"3\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Product C\",\n          \"price\": 149.99,\n          \"category\": \"Electronics\",\n          \"description\": \"Top-rated gadget\"\n        }\n      },\n      {\n        \"_index\": \"products\",\n        \"_id\": \"4\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Product D\",\n          \"price\": 29.99,\n          \"category\": \"Clothing\",\n          \"description\": \"Stylish t-shirt\"\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nThe size parameter specifies the number of documents to retrieve per page.\nThe from parameter is used for the initial query to start from the beginning.\nThe search_after parameter can be used for subsequent queries to retrieve the next page of results based on the sort values from the last hit or simply update the from parameter to start with the next group starting from a certain number of items in the search results. The following are required to use search_after\n\nThe sort parameter is used to ensure consistent ordering of results across pages.\nThe _doc field is used as a tiebreaker to ensure a stable sort order.\n\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE products\n\n\n\nDocumentation\n\nPagination\nSearch After API\nSort Search Results",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Developing Search Applications</span>"
    ]
  },
  {
    "objectID": "3-developing-search-applications.html#task-define-and-use-index-aliases",
    "href": "3-developing-search-applications.html#task-define-and-use-index-aliases",
    "title": "3  Developing Search Applications",
    "section": "3.4 Task: Define and use index aliases",
    "text": "3.4 Task: Define and use index aliases\n\nExample 1: Creating Index Aliases for Customer Data\nThis is an example of the simplest kind of alias.\n\nRequirements\n\nCreate multiple indices for customer data (e.g., customers-2024-01, customers-2024-02).\nCreate an alias that points to these indices.\nUse the alias to perform search operations across all customer indices.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST Client.\nCreate the 2 indices by indexing sample documents\nPOST /customers-2024-01/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"John Doe\", \"email\": \"john.doe@example.com\", \"signup_date\": \"2024-01-15\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Jane Smith\", \"email\": \"jane.smith@example.com\", \"signup_date\": \"2024-01-20\" }\nPOST /customers-2024-02/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"Alice Johnson\", \"email\": \"alice.johnson@example.com\", \"signup_date\": \"2024-02-05\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Bob Brown\", \"email\": \"bob.brown@example.com\", \"signup_date\": \"2024-02-10\" }\nCreate an alias for the two indices\nPOST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"customers-*\",\n        \"alias\": \"customers\"\n      }\n    }\n  ]\n}\nExecute a search query using the alias and confirm 4 documents returned\nGET /customers/_search\nExecute a search query for John Doe’s record\nGET customers/_search\n{\n  \"query\": {\n    \"match\": {\n      \"name\": \"john doe\"\n    }\n  }\n}\n\n\n\nTest\n\nVerify the alias was created\nGET /_alias/customers\nConfirm 4 documents returned when executing the test query\nGET /customers/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\nOr just\nGET /customers/_search\n\n\n\nConsiderations\n\nAlias Flexibility: Using an alias allows for flexibility in managing indices. The alias can point to multiple indices, making it easier to manage and query data across time-based indices.\nIndex Patterns: Ensure that the alias name (customers) is descriptive and clearly indicates its purpose.\nPerformance: Searching using an alias is efficient and does not introduce significant overhead compared to searching directly on indices.\n\n\n\nClean-up (optional)\n\nDelete the aliases\nDELETE customers-2024-01/_alias/customers\nDELETE customers-2024-02/_alias/customers\nDelete the 2 indices\nDELETE customers-2024-01\nDELETE customers-2024-02\n\n\n\nDocumentation\n\nBulk API\nDelete Aliases\nIndex Aliases\nSearch API\n\n\n\n\nExample 2: Creating index aliases for logging data with filtering and routing\nThis is a slightly more complex use of an index alias. It includes a custom configuration for each index defined in the alias and any custom filtering and/or routing that is required.\n\nRequirements\n\nThree indices (logs_2022, logs_2023, and logs_2024) with documents containing log data (message, level, timestamp)\nAn index alias (logs) that points to all three indices with filtering and routing based on the log level\n\nlogs_2022\n\nfilter on level ERROR\nrouting to error\n\nlogs_2023\n\nfilter on level INFO\nrouting to info\n\nlogs_2024\n\nfilter on level DEBUG\nrouting to debug\n\n\nA search query against the message field to retrieve documents from the alias\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the logs_2022, logs_2023, and logs_2024 indices\nPUT logs_2022\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"level\": {\n        \"type\": \"keyword\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\nPUT logs_2023\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"level\": {\n        \"type\": \"keyword\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\nPUT logs_2024\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"level\": {\n        \"type\": \"keyword\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\nCreate an index alias (logs) with filtering and routing (this must be done before indexing any documents)\nPOST /_aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"logs_2022\",\n        \"alias\": \"logs\",\n        \"filter\": {\n          \"term\": {\n            \"level\": \"ERROR\"\n          }\n        },\n        \"routing\": \"error\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"logs_2023\",\n        \"alias\": \"logs\",\n        \"filter\": {\n          \"term\": {\n            \"level\": \"INFO\"\n          }\n        },\n        \"routing\": \"info\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"logs_2024\",\n        \"alias\": \"logs\",\n        \"filter\": {\n          \"term\": {\n            \"level\": \"DEBUG\"\n          }\n        },\n        \"routing\": \"debug\"\n      }\n    }\n  ]\n}\nIndex sample documents\nPOST /logs_2022/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"message\": \"Error occurred\", \"level\": \"ERROR\", \"timestamp\": \"2022-01-01T12:00:00Z\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"message\": \"Error occurred\", \"level\": \"ERROR\", \"timestamp\": \"2022-01-01T12:00:00Z\" }\nPOST /logs_2023/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"message\": \"Info message\", \"level\": \"INFO\", \"timestamp\": \"2023-01-01T12:00:01Z\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"message\": \"Info message\", \"level\": \"INFO\", \"timestamp\": \"2023-01-01T12:00:01Z\" }\nPOST /logs_2024/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"message\": \"Debug message\", \"level\": \"DEBUG\", \"timestamp\": \"2024-01-01T12:00:01Z\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"message\": \"Debug message\", \"level\": \"DEBUG\", \"timestamp\": \"2024-01-01T12:00:01Z\" }\nCreate a general search using the logs alias (all log messages should be returned)\nGET logs/_search\nCreate a search query using the logs alias to search for error and info messages\nGET /logs/_search\n{\n  \"query\": {\n    \"terms\": {\n      \"message\": [\n        \"error\",\n        \"info\"\n      ]\n    }\n  }\n}\n\n\n\nTest\n\nVerify the alias was created\nGET /_alias/logs\nConfirm 4 documents returned when executing the query from Step 6: 2 from logs_2022 and 2 from logs_2023\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"logs_2022\",\n        \"_id\": \"1\",\n        \"_score\": 1,\n        \"_source\": {\n          \"message\": \"Error occurred\",\n          \"level\": \"ERROR\",\n          \"timestamp\": \"2022-01-01T12:00:00Z\"\n        }\n      },\n      {\n        \"_index\": \"logs_2022\",\n        \"_id\": \"2\",\n        \"_score\": 1,\n        \"_source\": {\n          \"message\": \"Error occurred\",\n          \"level\": \"ERROR\",\n          \"timestamp\": \"2022-01-01T12:00:00Z\"\n        }\n      },\n      {\n        \"_index\": \"logs_2023\",\n        \"_id\": \"1\",\n        \"_score\": 1,\n        \"_source\": {\n          \"message\": \"Info message\",\n          \"level\": \"INFO\",\n          \"timestamp\": \"2023-01-01T12:00:01Z\"\n        }\n      },\n      {\n        \"_index\": \"logs_2023\",\n        \"_id\": \"2\",\n        \"_score\": 1,\n        \"_source\": {\n          \"message\": \"Info message\",\n          \"level\": \"INFO\",\n          \"timestamp\": \"2023-01-01T12:00:01Z\"\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nThe index must be set up in the proper order for the query using the alias with filter and routing to work:\n\ncreate the index\ncreate the alias using filtering and/or routing\nindex the documents\n\nIndex aliases with filtering and routing allow you to control which documents are included in the alias based on specific criteria.\nIn this example, we created an alias that points to three indices with filtering based on the log level and routing to separate indices.\n\n\n\nClean-up (optional)\n\nDelete the aliases\nDELETE logs_2022/_alias/logs\nDELETE logs_2023/_alias/logs\nDELETE logs_2024/_alias/logs\nDelete the indices\nDELETE logs_2022\nDELETE logs_2023\nDELETE logs_2024\n\n\n\nDocumentation\n\nBulk API\nDelete Aliases\nIndex Aliases\nSearch API",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Developing Search Applications</span>"
    ]
  },
  {
    "objectID": "3-developing-search-applications.html#task-define-and-use-a-search-template",
    "href": "3-developing-search-applications.html#task-define-and-use-a-search-template",
    "title": "3  Developing Search Applications",
    "section": "3.5 Task: Define and use a search template",
    "text": "3.5 Task: Define and use a search template\n\nExample 1: Creating Search Templates for Product Search\n\nRequirements\n\nCreate an index and populate it with example product documents\nDefine a search template for querying products using description\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST Client\nCreate the index as a side-effect of indexing sample documents\nPOST /products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"Laptop\", \"description\": \"A high-performance laptop\", \"price\": 999.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Smartphone\", \"description\": \"A latest model smartphone\", \"price\": 799.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"name\": \"Tablet\", \"description\": \"A new tablet with excellent features\", \"price\": 499.99 }\nDefine a search template\nPUT _scripts/product_search_template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"match\": {\n          \"description\": \"{{query_string}}\"\n        }\n      }\n    }\n  }\n}\nUse the search template\nGET products/_search/template\n{\n  \"id\": \"product_search_template\",\n  \"params\": {\n    \"query_string\": \"laptop\"\n  }\n}\n\n\n\nTest\n\nVerify the documents are indexed\nGET products/_search\nVerify the template was created\nGET _scripts/product_search_template\nPerform a search using the template (results below)\n{\n  \"hits\": {\n    \"total\": {\n      \"value\": 1,\n      \"relation\": \"eq\"\n    },\n    \"hits\": [\n      {\n        \"_index\": \"products\",\n        \"_id\": \"1\",\n        \"_source\": {\n          \"name\": \"Laptop\",\n          \"description\": \"A high-performance laptop\",\n          \"price\": 999.99\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nTemplate Flexibility: Using a search template allows for reusable and parameterized queries, reducing the need to write complex queries multiple times.\nPerformance: Search templates can improve performance by reusing the query logic and reducing the overhead of constructing queries dynamically.\nTemplate Language: Mustache is used as the templating language, providing a simple and powerful way to define dynamic queries.\n\n\n\nClean-up (optional)\n\nDelete the search template\nDELETE _scripts/product_search_template\nDelete the index\nDELETE products\n\n\n\nDocumentation\n\nBulk API\nMustache\nSearch API\nSearch Templates\n\n\n\n\nExample 2: Creating search templates for an e-commerce product catalog with sorting and pagination\n\nRequirements\n\nAn Elasticsearch index named products with documents containing the fields name, price, category, description, rating.\nDefine a search template to search for products based on a user-provided query string, category filter, sort order, and pagination.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nIndex sample product documents using the /_bulk endpoint:\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Product A\", \"price\":99.99, \"category\":\"Electronics\", \"description\":\"High-quality product\", \"rating\":4.2}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Product B\", \"price\":49.99, \"category\":\"Books\", \"description\":\"Best-selling novel\", \"rating\":4.5}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Product C\", \"price\":149.99, \"category\":\"Electronics\", \"description\":\"Top-rated gadget\", \"rating\":3.8}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Product D\", \"price\":29.99, \"category\":\"Clothing\", \"description\":\"Stylish t-shirt\", \"rating\":4.1}\nIteratively create a search query that satisfies the various requirements (query string, category filter, sort order, pagination)\n// just search for a product using the name field\nGET products/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"default_field\": \"name\",\n      \"query\": \"product\"\n    }\n  }\n}\n// add the filter which entails also changing the query set-up\nGET products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"query_string\": {\n            \"default_field\": \"name\",\n            \"query\": \"product\"\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ]\n    }\n  }\n}\n// add sort\nGET products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"query_string\": {\n            \"default_field\": \"name\",\n            \"query\": \"product\"\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ]\n    }\n  },\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"desc\"\n      }\n    }\n  ]\n}\n// add pagination (such as it is)\nGET products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"query_string\": {\n            \"default_field\": \"name\",\n            \"query\": \"product\"\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"electronics\"\n          }\n        }\n      ]\n    }\n  },\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"desc\"\n      }\n    }\n  ],\n  \"size\": 1,\n  \"from\": 1\n}\nUsing the above, define the search template:\nPUT _scripts/product_search_template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"query_string\": {\n                \"default_field\": \"name\",\n                \"query\": \"{{query_string}}\"\n              }\n            }\n          ],\n          \"filter\": [\n            {\n              \"term\": {\n                \"category\": \"{{category}}\"\n              }\n            }\n          ]\n        }\n      },\n      \"sort\": [\n        {\n          \"price\": {\n            \"order\": \"{{sort_order}}\"\n          }\n        }\n      ],\n      \"from\": \"{{from}}\",\n      \"size\": \"{{size}}\"\n    }\n  }\n}\nUse the _render endpoint to take a look at the formatting of the query\nPOST _render/template\n{\n  \"id\": \"product_search_template\",\n  \"params\": {\n    \"query_string\": \"product\",\n    \"category\" : \"electronics\",\n    \"sort_order\" : \"desc\",\n    \"from\": 0,\n    \"size\": 1\n  }\n}\nUse the search template with sorting and pagination:\nGET products/_search/template\n{\n  \"id\": \"product_search_template\",\n  \"params\": {\n    \"query_string\": \"product\",\n    \"category\" : \"books\",\n    \"sort_order\" : \"desc\",\n    \"from\": 0,\n    \"size\": 1\n  }\n}\n\n\n\nTest\n\nVerify the documents are indexed\nGET products/_search\nVerify the template is created\nGET _scripts/product_search_template\nExecute a query for product, category of books, sort order of desc, and pagination starting at item 0, with a result size of 1.\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"products\",\n        \"_id\": \"2\",\n        \"_score\": null,\n        \"_source\": {\n          \"name\": \"Product B\",\n          \"price\": 49.99,\n          \"category\": \"Books\",\n          \"description\": \"Best-selling novel\",\n          \"rating\": 4.5\n        },\n        \"sort\": [\n          49.99\n        ]\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nThe search template includes sorting and pagination parameters (sort_field, sort_order, from, size).\nThe sort parameter in the template specifies the field and order for sorting the results.\nThe from and size parameters control the pagination of the results.\nThe params object in the search template request provides the values for all placeholders in the template.\n\n\n\nClean-up (optional)\n\nDelete the search template\nDELETE _scripts/product_search_template\nDelete the index\nDELETE products\n\n\n\nDocumentation\n\nSearch Template\nMustache Language\nPaginate Search Results\nScripting in Elasticsearch\nSort Search Results\n\n\n\n\nExample 3: Creating search templates for an e-commerce product catalog with nested queries, sorting, pagination, and aggregations\n\nRequirements\n\nAn index named products with documents containing the fields name, price, category, description, rating, tags, specifications, specifications.ram and specifications.storage.\nDefine a search template to search for products based on:\n\na user-provided query string,\ncategory filter\ntag filter\nsort order\npagination\n\ninclude aggregations for category, tags, and price_range\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the index with two fields (category and tags) of type keyword for use in the aggregation\nPUT products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"price\" : {\n        \"type\": \"float\"\n      },\n      \"category\" : {\n        \"type\": \"keyword\"\n      },\n      \"description\" : {\n        \"type\": \"text\"\n      },\n      \"rating\" : {\n        \"type\": \"float\"\n      },\n      \"tags\" : {\n        \"type\": \"keyword\"\n      },\n      \"specifications\" : {\n        \"properties\": {\n          \"ram\" : {\n            \"type\" : \"text\"\n          },\n          \"storage\" : {\n            \"type\" : \"text\"\n          }\n        }\n      }\n    }\n  }\n}\nIndex sample products documents\nPOST /products/_bulk\n{\"index\":{\"_id\":1}}\n{\"name\":\"Product A\", \"price\":99.99, \"category\":\"Electronics\", \"description\":\"High-quality product\", \"rating\":4.2, \"tags\":[\"electronics\", \"gadget\"], \"specifications\":{\"ram\":\"8GB\", \"storage\":\"256GB\"}}\n{\"index\":{\"_id\":2}}\n{\"name\":\"Product B\", \"price\":49.99, \"category\":\"Books\", \"description\":\"Best-selling novel\", \"rating\":4.5, \"tags\":[\"book\", \"fiction\"]}\n{\"index\":{\"_id\":3}}\n{\"name\":\"Product C\", \"price\":149.99, \"category\":\"Electronics\", \"description\":\"Top-rated gadget\", \"rating\":3.8, \"tags\":[\"electronics\", \"laptop\"], \"specifications\":{\"ram\":\"16GB\", \"storage\":\"512GB\"}}\n{\"index\":{\"_id\":4}}\n{\"name\":\"Product D\", \"price\":29.99, \"category\":\"Clothing\", \"description\":\"Stylish t-shirt\", \"rating\":4.1, \"tags\":[\"clothing\", \"tshirt\"]}\nDefine a search query that satisfies the requirements\nGET products/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"query_string\": {\n            \"default_field\": \"name\",\n            \"query\": \"product\"\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"category\": \"Electronics\"\n          }\n        },\n        {\n          \"term\": {\n            \"tags\": \"electronics\"\n          }\n        }\n      ]\n    }\n  },\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"desc\"\n      }\n    }\n  ],\n  \"size\": 1,\n  \"from\": 0,\n  \"aggs\": {\n    \"category_aggs\": {\n      \"terms\": {\n        \"field\": \"category\"\n      }\n    },\n    \"tags_aggs\": {\n      \"terms\": {\n        \"field\": \"tags\"\n      }\n    },\n    \"price_range_aggs\": {\n      \"range\": {\n        \"field\": \"price\",\n        \"ranges\": [\n          {\n            \"to\": 30\n          },\n          {\n            \"from\": 30,\n            \"to\": 100\n          },\n          {\n            \"from\": 100\n          }\n        ]\n      }\n    }\n  }\n}\nUse the above query to create the search template\nPUT _scripts/products_search_template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"query_string\": {\n                \"default_field\": \"name\",\n                \"query\": \"{{query}}\"\n              }\n            }\n          ],\n          \"filter\": [\n            {\n              \"term\": {\n                \"category\": \"{{category}}\"\n              }\n            },\n            {\n              \"term\": {\n                \"tags\": \"{{tags}}\"\n              }\n            }\n          ]\n        }\n      },\n      \"sort\": [\n        {\n          \"price\": {\n            \"order\": \"{{sort_order}}\"\n          }\n        }\n      ],\n      \"size\": \"{{size}}\",\n      \"from\": \"{{from}}\",\n      \"aggs\": {\n        \"category_aggs\": {\n          \"terms\": {\n            \"field\": \"category\"\n          }\n        },\n        \"tags_aggs\": {\n          \"terms\": {\n            \"field\": \"tags\"\n          }\n        },\n        \"price_range_aggs\": {\n          \"range\": {\n            \"field\": \"price\",\n            \"ranges\": [\n              {\n                \"to\": 30\n              },\n              {\n                \"from\": 30,\n                \"to\": 100\n              },\n              {\n                \"from\": 100\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\nUse the search template with sorting, pagination, and aggregations:\nGET products/_search/template\n{\n  \"id\": \"products_search_template\",\n  \"params\": {\n    \"query\" : \"product\",\n    \"category\" : \"Electronics\",\n    \"tags\" : \"electronics\",\n    \"sort_order\" : \"desc\",\n    \"size\" : 2,\n    \"from\" : 0\n  }\n}\n\n\n\nTest\n\nVerify the index is created\nGET products\nVerify the documents are indexed\nGET products/_search\nVerify the template is created\nGET _scripts/products_search_template\nExecute a search using the search template query, and it should return the first 2 documents matching the provided query string (“*“), category filter (”Electronics”), tag filter (“electronics”), sorted by price in descending order with aggregations.\n\nThe response should also include aggregations for category, tags, and price.\n{\n  ...\n  \"hits\": [\n    {\n      \"_index\": \"products\",\n      \"_id\": \"3\",\n      \"_score\": null,\n      \"_source\": {\n        \"name\": \"Product C\",\n        \"price\": 149.99,\n        \"category\": \"Electronics\",\n        \"description\": \"Top-rated gadget\",\n        \"rating\": 3.8,\n        \"tags\": [\n          \"electronics\",\n          \"laptop\"\n        ],\n        \"specifications\": {\n          \"ram\": \"16GB\",\n          \"storage\": \"512GB\"\n        }\n      },\n      \"sort\": [\n        149.99\n      ]\n    },\n    {\n      \"_index\": \"products\",\n      \"_id\": \"1\",\n      \"_score\": null,\n      \"_source\": {\n        \"name\": \"Product A\",\n        \"price\": 99.99,\n        \"category\": \"Electronics\",\n        \"description\": \"High-quality product\",\n        \"rating\": 4.2,\n        \"tags\": [\n          \"electronics\",\n          \"gadget\"\n        ],\n        \"specifications\": {\n          \"ram\": \"8GB\",\n          \"storage\": \"256GB\"\n        }\n      },\n      \"sort\": [\n        99.99\n      ]\n    }\n  ]\n},\n\"aggregations\": {\n  \"category_aggs\": {\n    \"doc_count_error_upper_bound\": 0,\n    \"sum_other_doc_count\": 0,\n    \"buckets\": [\n      {\n        \"key\": \"Electronics\",\n        \"doc_count\": 2\n      }\n    ]\n  },\n  \"tags_aggs\": {\n    \"doc_count_error_upper_bound\": 0,\n    \"sum_other_doc_count\": 0,\n    \"buckets\": [\n      {\n        \"key\": \"electronics\",\n        \"doc_count\": 2\n      },\n      {\n        \"key\": \"gadget\",\n        \"doc_count\": 1\n      },\n      {\n        \"key\": \"laptop\",\n        \"doc_count\": 1\n      }\n    ]\n  },\n  \"price_range_aggs\": {\n    \"buckets\": [\n      {\n        \"key\": \"*-30.0\",\n        \"to\": 30,\n        \"doc_count\": 0\n      },\n      {\n        \"key\": \"30.0-100.0\",\n        \"from\": 30,\n        \"to\": 100,\n        \"doc_count\": 1\n      },\n      {\n        \"key\": \"100.0-*\",\n        \"from\": 100,\n        \"doc_count\": 1\n      }\n    ]\n  }\n}\n}\n\n\n\n\nConsiderations\n\nThe search template includes queries, filters, sorting, pagination, and aggregations.\nThe tags filter uses a terms query to match documents with any of the specified tags.\nThe aggs section in the template defines the aggregations to be included in the search results.\nThe params object in the search template request provides the values for all placeholders in the template.\n\n\n\nClean-up (optional)\n\nDelete the search template\nDELETE _scripts/product_search_template\nDelete the index\nDELETE products\n\n\n\nDocumentation\n\nAggregations\nMustache Language\nNested Aggregations\nPaginate Search Results\nScripting in Elasticsearch\nSearch Template\nSort Search Results",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Developing Search Applications</span>"
    ]
  },
  {
    "objectID": "4-data-processing.html",
    "href": "4-data-processing.html",
    "title": "4  Data Processing",
    "section": "",
    "text": "4.1 Task: Define a mapping that satisfies a given set of requirements",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "4-data-processing.html#task-define-a-mapping-that-satisfies-a-given-set-of-requirements",
    "href": "4-data-processing.html#task-define-a-mapping-that-satisfies-a-given-set-of-requirements",
    "title": "4  Data Processing",
    "section": "",
    "text": "Example 1: Defining Index Mappings for a Product Catalog\n\nRequirements\n\nCreate a mapping for an index named product_catalog\nDefine fields for product ID, name, description, price, and availability status.\nEnsure the price field is a numeric type.\nUse a text type for description with a keyword sub-field for exact matches.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nCreate the index with mappings:\nPUT /product_catalog\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\nCreate sample documents using the _bulk endpoint:\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 19.99, \"availability_status\": true }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 29.99, \"availability_status\": false }\n\n\n\nTest\n\nRetrieve the mappings to verify:\nGET /product_catalog/_mapping\nSearch for documents to confirm they are indexed correctly:\nGET /product_catalog/_search\nOR\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\nOR\nGET product_catalog/_search\n{\n  \"query\": {\n    \"term\": {\n      \"description\": \"product\"\n    }\n  }\n}\nOR\nGET product_catalog/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description.keyword\": \"Description of product 1\"\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe price field is set to integer to handle whole numbers.\nThe description field includes a keyword sub-field for exact match searches.\n\n\n\nClean-up (optional)\n\nDelete the index (which will also delete the mapping)\nDELETE product_catalog\n\n\n\nDocumentation\n\nBulk API\nElasticsearch Index Mappings\n\n\n\n\nExample 2: Creating a mapping for a social media platform\n\nRequirements\n\nCreate a mapping for an index named users\nThe mapping should have a field called username of type keyword\nThe mapping should have a field called email of type keyword\nThe mapping should have a field called posts of type array containing object values\nThe posts array should have a property called content of type text\nThe posts array should have a property called likes of type integer\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nCreate an index with the desired mapping:\nPUT /users\n{\n  \"mappings\": {\n    \"properties\": {\n      \"username\": {\n        \"type\": \"keyword\"\n      },\n      \"email\": {\n        \"type\": \"keyword\"\n      },\n      \"posts\": {\n        \"properties\": {\n          \"content\": {\n            \"type\": \"text\"\n          },\n          \"likes\": {\n            \"type\": \"integer\"\n          }\n        }\n      }\n    }\n  }\n}\nIndex a document:\nPOST /users/_doc\n{\n  \"username\": \"john_doe\",\n  \"email\": \"john.doe@example.com\",\n  \"posts\": [\n    {\n      \"content\": \"Hello World!\",\n      \"likes\": 10\n    },\n    {\n      \"content\": \"This is my second post\",\n      \"likes\": 5\n    }\n  ]\n}\n\n\n\nTest\n\nVerify the mapping\n\nGET users\n\nUse the _search API to verify that the mapping is correct and the data is indexed:\nGET /users/_search\n{\n  \"query\": {\n    \"match\": {\n      \"username\": \"john_doe\"\n    }\n  }\n}\nAnd\nGET users/_search\n{\n  \"size\": 0, \n  \"aggs\": {\n    \"total_likes\": {\n      \"sum\": {\n        \"field\": \"posts.likes\"\n      }\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe username and email fields are of type keyword to enable exact matching.\nThe posts field is of type array with object values to enable storing multiple posts per user.\nThe content field is of type text to enable full-text search.\nThe likes field is of type integer to enable aggregations and sorting.\n\n\n\nClean-up (optional)\n\nDelete the index (which will also delete the mapping)\nDELETE users\n\n\n\nDocumentation\n\nElasticsearch Index Mappings\n\n\n\n\nExample 3: Creating a mapping for storing and searching restaurant data\n\nRequirements\n\nCreate a mapping for an index named restaurants.\nThe mapping should include fields for:\n\nname (text field for restaurant name)\ndescription (text field for restaurant description)\nlocation (geolocation field for restaurant location)\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nDefine the mapping using a REST API call:\nPUT /restaurants\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"location\": {\n        \"type\": \"geo_point\"\n      }\n    }\n  }\n}\n\n\n\nTest\n\nVerify that the mapping is created successfully by using the following API call:\nGET /restaurants/_mapping\nTry indexing a sample document with the defined fields:\nPUT /restaurants/_doc/1\n{\n  \"name\": \"Pizza Palace\",\n  \"description\": \"Delicious pizzas and Italian cuisine\",\n  \"location\": {\n    \"lat\": 40.7128,\n    \"lon\": -74.0059\n  }\n}\nUse search queries to test text search on name and description fields, and utilize geoqueries to search based on the location field.\nGET /restaurants/_search\n{\n  \"query\": {\n    \"match\": {\n      \"name\": \"Pizza Palace\"\n    }\n  }\n}\nGET /restaurants/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"Italian cuisine\"\n    }\n  }\n}\nGET /restaurants/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": {\n        \"geo_distance\": {\n          \"distance\": \"5km\",\n          \"location\": {\n            \"lat\": 40.7128,\n            \"lon\": -74.0059\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\nConsiderations\n\ntext is a generic field type suitable for textual data like names and descriptions.\ngeo_point is a specialized field type for storing and searching geospatial data like latitude and longitude coordinates.\n\n\n\nClean-up (optional)\n\nDelete the index (which will also delete the mapping)\nDELETE restaurants\n\n\n\nDocumentation\n\nData Types\nGeolocation Queries",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "4-data-processing.html#task-define-and-use-a-custom-analyzer-that-satisfies-a-given-set-of-requirements",
    "href": "4-data-processing.html#task-define-and-use-a-custom-analyzer-that-satisfies-a-given-set-of-requirements",
    "title": "4  Data Processing",
    "section": "4.2 Task: Define and use a custom analyzer that satisfies a given set of requirements",
    "text": "4.2 Task: Define and use a custom analyzer that satisfies a given set of requirements\n\nExample 1: Custom Analyzer for Restaurant Reviews\n\n4.2.0.1 Requirements\n\nCreate a mapping for an index named restaurant_reviews\nCreate a custom analyzer named custom_review_analyzer.\nThe analyzer should:\n\nUse the standard tokenizer.\nInclude a lowercase filter.\nInclude a stop filter to remove common English stop words.\nInclude a synonym filter to handle common synonyms.\n\n\n\n\n4.2.0.2 Steps\n\nOpen the Kibana Console or use a REST client\nCreate the index with a custom analyzer defined in the index settings.\nPUT /restaurant_reviews\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_review_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"stop\",\n            \"synonym\"\n          ]\n        }\n      },\n      \"filter\": {\n        \"synonym\": {\n          \"type\": \"synonym\",\n          \"synonyms\": [\n            \"delicious, tasty\",\n            \"restaurant, eatery\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"review_id\": {\n        \"type\": \"keyword\"\n      },\n      \"restaurant_name\": {\n        \"type\": \"text\"\n      },\n      \"review_text\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_review_analyzer\"\n      },\n      \"rating\": {\n        \"type\": \"integer\"\n      },\n      \"review_date\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\nAdd some sample documents to the index to test the custom analyzer\nPOST /restaurant_reviews/_bulk\n{ \"index\": {} }\n{ \"review_id\": \"1\", \"restaurant_name\": \"Pizza Palace\", \"review_text\": \"The pizza was delicious and the service was excellent.\", \"rating\": 5, \"review_date\": \"2024-07-01\" }\n{ \"index\": {} }\n{ \"review_id\": \"2\", \"restaurant_name\": \"Burger Haven\", \"review_text\": \"Tasty burgers and friendly staff.\", \"rating\": 4, \"review_date\": \"2024-07-02\" }\nPerform a search query to verify the custom analyzer is working as expected.\nGET /restaurant_reviews/_search\n{\n  \"query\": {\n    \"match\": {\n      \"review_text\": \"tasty\"\n    }\n  }\n}\n\n\n\n4.2.0.3 Considerations\n\nStandard Tokenizer: Chosen for its ability to handle most text inputs effectively.\nLowercase Filter: Ensures case-insensitive search.\nStop Filter: Removes common stop words to improve search relevance.\nSynonym Filter: Handles common synonyms to enhance search matching.\n\n\n\n4.2.0.4 Test\n\nVerify the analyzer was created\nGET /restaurant_reviews/_settings\nVerify the custom analyzer configuration using the _analyze API to test the custom analyzer directly.\nGET /restaurant_reviews/_analyze\n{\n  \"analyzer\": \"custom_review_analyzer\",\n  \"text\": \"The pizza was delicious and the service was excellent.\"\n}\nPerform a search queries to ensure the custom analyzer processes the text as expected.\nGET /restaurant_reviews/_search\n{\n  \"query\": {\n    \"match\": {\n      \"review_text\": \"tasty\"\n    }\n  }\n}\n\n\n\n4.2.0.5 Clean-up (optional)\n\nDelete the Index\nDELETE /restaurant_reviews\n\n\n\n4.2.0.6 Documentation\n\nAnalyzers\nCustom Analyzers\nIndex Settings\n\n\n\n\nExample 2: Creating a custom analyzer for product descriptions\n\nRequirements\n\nCreate a mapping for an index named products with a description field containing product descriptions\nThe custom analyzer should:\n\nLowercase all text\nRemove stop words (common words like the, and, a, etc.)\nSplit text into individual words (tokenize)\nStem words (reduce words to their root form, e.g., running - run)\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nCreate the products index with a custom analyzer for the description field:\n\nPUT /products\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"product_description_analyzer\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"stop\",\n            \"stemmer\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"description\": {\n        \"type\": \"text\",\n        \"analyzer\": \"product_description_analyzer\"\n      }\n    }\n  }\n}\n\nIndex some sample documents using the _bulk endpoint:\n\nPOST /products/_bulk\n{ \"index\": { \"_id\": 1 } }\n{ \"description\": \"The quick brown fox jumps over the lazy dog.\" }\n{ \"index\": { \"_id\": 2 } }\n{ \"description\": \"A high-quality product for running enthusiasts.\" }\n\n\nTest\n\nSearch for documents containing the term run\n\nGET /products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"run\"\n    }\n  }\n}\nThis should return the document with _id 2, as the custom analyzer has stemmed running to run.\n\nSearch for documents containing the term the\n\nGET /products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"the\"\n    }\n  }\n}\nThis should not return any documents, as the custom analyzer has removed stop words like the.\n\n\nConsiderations\n\nThe custom analyzer is defined in the index settings using the analysis section.\nThe tokenizer parameter specifies how the text should be split into tokens (individual words).\nThe filter parameter specifies the filters to be applied to the tokens, such as lowercasing, stop word removal, and stemming.\nThe custom analyzer is applied to description by specifying it in the field mapping.\n\n\n\nClean-up (optional)\n\nDelete the Index\nDELETE /products\n\n\n\nDocumentation\n\nAnalyzers\nCustom Analyzers\nToken Filters\nTokenizers\n\n\n\n\nExample 3: Creating a custom analyzer for product descriptions in an ecommerce catalog\n\nRequirements\n\nDefine an index called product_catalog with a description field.\nCreate a custom tokenizer that splits text on non-letter characters.\nInclude a lowercase filter to normalize text.\nAdd a stopword filter to remove common English stopwords.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nDefine the custom analyzer in the index settings\nPUT product_catalog\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"lowercase\",\n          \"filter\": [\n            \"english_stop\"\n          ]\n        }\n      },\n      \"filter\": {\n        \"english_stop\": {\n          \"type\": \"stop\",\n          \"stopwords\": \"_english_\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"description\" : {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      }\n    }\n  }\n}\nCreate sample documents using the _bulk endpoint:\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"description\": \"This is a great product! It works perfectly.\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"description\": \"An amazing gadget, with excellent features.\" }\n\n\n\nTest\n\nAnalyze a sample text to verify the custom analyzer:\nGET product_catalog/_analyze\n{\n  \"analyzer\" : \"custom_analyzer\",\n  \"text\" : \"i2can2RUN4the6MARATHON!\"\n}\n// response\n{\n  \"tokens\": [\n    {\n      \"token\": \"i\",\n      \"start_offset\": 0,\n      \"end_offset\": 1,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"can\",\n      \"start_offset\": 2,\n      \"end_offset\": 5,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"run\",\n      \"start_offset\": 6,\n      \"end_offset\": 9,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"marathon\",\n      \"start_offset\": 14,\n      \"end_offset\": 22,\n      \"type\": \"word\",\n      \"position\": 4\n    }\n  ]\n}\nSearch for documents to confirm they are indexed correctly:\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"great product\"\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe custom tokenizer splits text on non-letter characters, ensuring that punctuation does not affect tokenization.\n\nThe lowercase tokenizer splits text on non-letter characters and turns uppercase characters into lowercase\n\nThe lowercase filter normalizes text to lower case, providing case-insensitive searches.\nThe custom_stop stopword filter removes common English stopwords, improving search relevance by ignoring less important words.\n\n\n\nClean-up (optional)\n\nDelete the ecommerce_products index:\nDELETE /ecommerce_products\n\n\n\nDocumentation\n\nBulk API\nCustom Analyzers\nLowercase Tokenizer\nLowercase Filter\nStop word Filter\n\n\n\n\nExample 4: Create a Custom Analyzer for E-commerce Product Data\n\nRequirements\n\nIndex e-commerce product data with fields such as name, category, description, and sku.\nCustom analyzer to normalize text for consistent search results, including handling special characters and case sensitivity.\nUse the _bulk endpoint to ingest multiple documents.\nTwo example searches to verify that the custom analyzer handles both hyphenated and non-hyphenated queries.\n\n\n\nSteps\n\nDefine the Custom Analyzer:\n\nSet up the analyzer to lowercase text, remove special characters, and tokenize the content.\n\nPUT /ecommerce_products\n{\n  \"settings\": {\n    \"analysis\": {\n      \"char_filter\": {\n        \"remove_special_chars\": {\n          \"type\": \"pattern_replace\",\n          \"pattern\": \"[^\\\\w\\\\s]\",\n          \"replacement\": \"\"\n        }\n      },\n      \"filter\": {\n        \"my_lowercase\": {\n          \"type\": \"lowercase\"\n        }\n      },\n      \"analyzer\": {\n        \"custom_analyzer\": {\n          \"char_filter\": [\"remove_special_chars\"],\n          \"tokenizer\": \"standard\",\n          \"filter\": [\"my_lowercase\"]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      },\n      \"category\": {\n        \"type\": \"keyword\"\n      },\n      \"description\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\"\n      },\n      \"sku\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\nIndex Sample Documents Using _bulk Endpoint:\n\nUse the _bulk endpoint to ingest multiple documents.\n\nPOST /ecommerce_products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"name\": \"Choco-Lite Bar\", \"category\": \"Snacks\", \"description\": \"A light and crispy chocolate snack bar.\", \"sku\": \"SNACK-CHOCOLITE-001\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"name\": \"Apple iPhone 12\", \"category\": \"Electronics\", \"description\": \"The latest iPhone model with advanced features.\", \"sku\": \"ELEC-IPH12-256GB\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"name\": \"Samsung Galaxy S21\", \"category\": \"Electronics\", \"description\": \"A powerful smartphone with an impressive camera.\", \"sku\": \"ELEC-SG-S21\" }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"name\": \"Nike Air Max 270\", \"category\": \"Footwear\", \"description\": \"Comfortable and stylish sneakers.\", \"sku\": \"FTWR-NIKE-AM270\" }\n\n\n\nTest\n\nQuery without Hyphen:\nGET /ecommerce_products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"name\": \"chocolite\"\n    }\n  }\n}\nQuery with Hyphen:\nGET /ecommerce_products/_search\n{\n  \"query\": {\n    \"match\": {\n      \"name\": \"choco-lite\"\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe pattern_replace character filter removes non-alphanumeric characters (excluding whitespace) to normalize data for indexing and searching.\nThe lowercase filter ensures case-insensitivity, providing consistent search results regardless of the case of the input.\nThe use of the _bulk endpoint allows efficient indexing of multiple documents in a single request, which is especially useful for large datasets.\n\n\n\nDocumentation\n\nCustom Analyzer\nPattern Replace Char Filter\nLowercase Token Filter\nStandard Tokenizer\nBulk API",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "4-data-processing.html#task-define-and-use-multi-fields-with-different-data-types-andor-analyzers",
    "href": "4-data-processing.html#task-define-and-use-multi-fields-with-different-data-types-andor-analyzers",
    "title": "4  Data Processing",
    "section": "4.3 Task: Define and use multi-fields with different data types and/or analyzers",
    "text": "4.3 Task: Define and use multi-fields with different data types and/or analyzers\n\nExample 1: Creating multi-fields for product names in an e-commerce catalog\n\nRequirements\n\nDefine an index called product_catalog\nDefine a field with a text type for full-text search.\nInclude a keyword sub-field for exact matches.\nAdd a custom analyzer to the text field to normalize the text.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nDefine the multi-fields in the index mappings\nPUT /product_catalog\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_name\": {\n        \"type\": \"text\",\n        \"analyzer\": \"custom_analyzer\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      }\n    }\n  }\n}\nCreate sample documents using the _bulk endpoint:\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_name\": \"Deluxe Toaster\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_name\": \"Premium Coffee Maker\" }\n\n\n\nTest\n\nRetrieve the index configuration to verify the custom analyzer and the sub-field:\nGET product_catalog\nSearch for documents using the text field:\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match\": {\n      \"product_name\": \"deluxe\"\n    }\n  }\n}\nSearch for documents using the keyword sub-field:\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"term\": {\n      \"product_name.keyword\": \"Deluxe Toaster\"\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe custom analyzer (standard) includes the lowercase filter for case-insensitive searches.\nThe keyword sub-field allows for exact matches, which is useful for aggregations and sorting.\n\n\n\nClean-up (optional)\n\nDelete the Index\nDELETE /product_catalog\n\n\n\nDocumentation\n\nBulk API\nCustom Analyzers\nMulti-fields\n\n\n\n\nExample 2: Creating a multi-field for a title with different analyzers\n\nRequirements\n\nCreate a mapping for a index named myindex\nThe title field should have a sub-field for exact matching (keyword)\nThe title field should have a sub-field for full-text search (text) with standard analyzer\nThe title field should have a sub-field for full-text search (text) with english analyzer\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nCreate an index with the desired mapping:\nPUT /myindex\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"exact\": {\n            \"type\": \"keyword\"\n          },\n          \"std\": {\n            \"type\": \"text\",\n            \"analyzer\": \"standard\"\n          },\n          \"english\": {\n            \"type\": \"text\",\n            \"analyzer\": \"english\"\n          }\n        }\n      }\n    }\n  }\n}\nAdd documents using the appropriate endpoint:\nPOST /myindex/_bulk\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"title\": \"The Quick Brown Fox\" }\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"title\": \"The Quick Brown Fox Jumps\" }\n\n\n\nTest\n\nVerify the index was created with its associated multi-fields\nGET myindex\nUse the _search API to verify that the multi-field is working correctly\nGET /myindex/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title.exact\": \"The Quick Brown Fox\"\n    }\n  }\n}\n\nGET /myindex/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title.std\": \"Quick Brown\"\n    }\n  }\n}\n\nGET /myindex/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title.english\": \"Quick Brown\"\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe title.exact sub-field is used for exact matching.\nThe title.std sub-field is used for full-text search with the standard analyzer.\nThe title.english sub-field is used for full-text search with the English analyzer.\n\n\n\nClean-up (optional)\n\nDelete the Index\nDELETE /myindex\n\n\n\nDocumentation\n\nBulk API\nMulti-Field\n\n\n\n\nExample 3: Creating multi-fields for analyzing text data\n\nRequirements\n\nCreate a mapping for a index named text_data\nStore the original text data in content for display purposes\nAnalyze the text data for full-text search\nAnalyze the text data for filtering and aggregations\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nDefine the multi-fields in the index mapping\nPUT /text_data\n{\n  \"mappings\": {\n    \"properties\": {\n      \"content\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"raw\": {\n            \"type\": \"keyword\"\n          },\n          \"analyzed\": {\n            \"type\": \"text\",\n            \"analyzer\": \"english\"\n          },\n          \"ngram\": {\n            \"type\": \"text\",\n            \"analyzer\": \"ngram_analyzer\"\n          }\n        }\n      }\n    }\n  },\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"ngram_analyzer\": {\n          \"tokenizer\": \"ngram_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"ngram_tokenizer\": {\n          \"type\": \"ngram\",\n          \"min_gram\": 2,\n          \"max_gram\": 3\n        }\n      }\n    }\n  }\n}\nIndex some documents using the text_data index:\nPOST /text_data/_bulk\n{ \"index\": {} }\n{ \"content\": \"This is a sample text for analyzing.\" }\n{ \"index\": {} }\n{ \"content\": \"Another example of text data.\" }\n\n\n\nTest\n\nVerify the index was created with its associated multi-fields\nGET text_data\nTest the multi-fields by querying and aggregating the data:\nGET /text_data/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content.analyzed\": \"sample\"\n    }\n  },\n  \"aggs\": {\n    \"filter_agg\": {\n      \"filter\": {\n        \"term\": {\n          \"content.ngram\": \"ex\"\n        }\n      }\n    }\n  }\n}\nThe output should show a single document in the search results matching the analyzed text and the aggregation results based on the ngram analysis.\nThe following:\nGET /text_data/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content.ngram\": \"ex\"\n    }\n  },\n  \"aggs\": {\n    \"filter_agg\": {\n      \"filter\": {\n        \"term\": {\n          \"content.ngram\": \"ex\"\n        }\n      }\n    }\n  }\n}\nwill show 2 documents as the search is looking for the substring “ex” which can be found in both documents, but only if you search against content.ngram.\n// edited response\n{\n  ...\n  \"hits\": {\n    \"total\": {\n      \"value\": 1,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": 0.7361701,\n    \"hits\": [\n      {\n        \"_index\": \"text_data\",\n        \"_id\": \"qnqiBJEBRRh1FLFiJKsV\",\n        \"_score\": 0.7361701,\n        \"_source\": {\n          \"content\": \"This is a sample text for analyzing.\"\n        }\n      }\n    ]\n  },\n  \"aggregations\": {\n    \"filter_agg\": {\n      \"doc_count\": 1\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe content field has multiple sub-fields: raw (keyword), analyzed (text with English analyzer), and ngram (text with ngram analyzer).\nThe raw sub-field is used for storing the original text data without analysis.\nThe analyzed sub-field is used for full-text search using the English analyzer.\nThe ngram sub-field is used for filtering and aggregations based on ngram analysis.\n\n\n\nClean-up (optional)\n\nDelete the Index\nDELETE text_data\n\n\n\nDocumentation\n\nAnalyzers\nMulti-fields\nNgram Tokenizer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "4-data-processing.html#task-use-the-reindex-api-and-update-by-query-api-to-reindex-andor-update-documents",
    "href": "4-data-processing.html#task-use-the-reindex-api-and-update-by-query-api-to-reindex-andor-update-documents",
    "title": "4  Data Processing",
    "section": "4.4 Task: Use the Reindex API and Update By Query API to reindex and/or update documents",
    "text": "4.4 Task: Use the Reindex API and Update By Query API to reindex and/or update documents\n\nExample 1: Moving and updating product data to a new index with a new field\n\nRequirements\n\nReindex data from an existing index named products_old to a new index named products_new.\nDuring the reindexing process, add a new field named stock_level with a default value of 10 for each product.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nCreate the indices (notice that they both look identical){target=“_blank”}\nPUT /products_old\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\nPUT /products_new\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\nAdd products to products_old\nPOST /products_old/_bulk\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"1\" } }\n{ \"product_id\": \"1\", \"name\": \"Wireless Mouse\", \"description\": \"A high-quality wireless mouse with ergonomic design.\", \"price\": 29.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"2\" } }\n{ \"product_id\": \"2\", \"name\": \"Gaming Keyboard\", \"description\": \"Mechanical gaming keyboard with customizable RGB lighting.\", \"price\": 79.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"3\" } }\n{ \"product_id\": \"3\", \"name\": \"USB-C Hub\", \"description\": \"A versatile USB-C hub with multiple ports.\", \"price\": 49.99, \"availability_status\": true }\nUse the Reindex API with a script to update documents during the copy process:\nPOST /_reindex\n{\n  \"source\": {\n    \"index\": \"products_old\"\n  },\n  \"dest\": {\n    \"index\": \"products_new\"\n  },\n  \"script\": {\n    \"source\": \"ctx._source.stock_level = 10\"\n  }\n}\nWait for the reindexing or update operation to complete.\n\n\n\nTest\n\nVerify that the documents from products_old do not contain stock_level\nGET /products_old/_search\n// edited response\n{\n ...\n     \"hits\": [\n       {\n         \"_index\": \"products_old\",\n         \"_id\": \"1\",\n         \"_score\": 1,\n         \"_source\": {\n           \"product_id\": \"1\",\n           \"name\": \"Wireless Mouse\",\n           \"description\": \"A high-quality wireless mouse with ergonomic design.\",\n           \"price\": 29.99,\n           \"availability_status\": true\n         }\n       },\n       {\n         \"_index\": \"products_old\",\n         \"_id\": \"2\",\n         \"_score\": 1,\n         \"_source\": {\n           \"product_id\": \"2\",\n           \"name\": \"Gaming Keyboard\",\n           \"description\": \"Mechanical gaming keyboard with customizable RGB lighting.\",\n           \"price\": 79.99,\n           \"availability_status\": true\n         }\n       },\n       {\n         \"_index\": \"products_old\",\n         \"_id\": \"3\",\n         \"_score\": 1,\n         \"_source\": {\n           \"product_id\": \"3\",\n           \"name\": \"USB-C Hub\",\n           \"description\": \"A versatile USB-C hub with multiple ports.\",\n           \"price\": 49.99,\n           \"availability_status\": true\n         }\n       }\n     ]\n   }\n }\nVerify that the data is successfully migrated to the products_new index with the addition of stock_level\nGET /products_new/_search\n// edited response\n {\n   ...\n     \"hits\": [\n       {\n         \"_index\": \"products_new\",\n         \"_id\": \"1\",\n         \"_score\": 1,\n         \"_source\": {\n           \"availability_status\": true,\n           \"price\": 29.99,\n           \"product_id\": \"1\",\n           \"stock_level\": 10,\n           \"name\": \"Wireless Mouse\",\n           \"description\": \"A high-quality wireless mouse with ergonomic design.\"\n         }\n       },\n       {\n         \"_index\": \"products_new\",\n         \"_id\": \"2\",\n         \"_score\": 1,\n         \"_source\": {\n           \"availability_status\": true,\n           \"price\": 79.99,\n           \"product_id\": \"2\",\n           \"stock_level\": 10,\n           \"name\": \"Gaming Keyboard\",\n           \"description\": \"Mechanical gaming keyboard with customizable RGB lighting.\"\n         }\n       },\n       {\n         \"_index\": \"products_new\",\n         \"_id\": \"3\",\n         \"_score\": 1,\n         \"_source\": {\n           \"availability_status\": true,\n           \"price\": 49.99,\n           \"product_id\": \"3\",\n           \"stock_level\": 10,\n           \"name\": \"USB-C Hub\",\n           \"description\": \"A versatile USB-C hub with multiple ports.\"\n         }\n       }\n     ]\n   }\n }\n\n\n\nConsiderations\n\nThe Reindex API with a script allows copying data and applying transformations during the process.\n\n\n\nClean-up (optional)\n\nDelete the two indices\nDELETE products_old\nDELETE products_new\n\n\n\nDocumentation\n\nReindex API\nUpdate By Query API\n\n\n\n\nExample 2: Reindexing and updating product data\n\nRequirements\n\nReindex data from an existing index named products_old to a new index named products_new.\nBoth indices have the following fields:\n\nname (text)\nprice (float)\ninventory_count (integer)\n\nThe products_new index has an additional boolean field called in_stock\nIn products_new, update the in_stock field for products with a low inventory count (less than 10 items)\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nCreate the old index with some sample data:\nPUT /products_old\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      },\n      \"inventory_count\": {\n        \"type\": \"integer\"\n      }\n    }\n  }\n}\nPOST /products_old/_bulk\n{ \"index\": {} }\n{ \"name\": \"Product A\", \"price\": 19.99, \"inventory_count\": 10 }\n{ \"index\": {} }\n{ \"name\": \"Product B\", \"price\": 29.99, \"inventory_count\": 5 }\n{ \"index\": {} }\n{ \"name\": \"Product C\", \"price\": 39.99, \"inventory_count\": 20 }\nCreate the new index with an updated mapping:\nPUT /products_new\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"float\"\n      },\n      \"inventory_count\": {\n        \"type\": \"integer\"\n      },\n      \"in_stock\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\nReindex the data from the old index to the new index. This updates the in_stock field as it migrates the content.\nPOST /_reindex\n{\n  \"source\": {\n    \"index\": \"products_old\"\n  },\n  \"dest\": {\n    \"index\": \"products_new\"\n  },\n  \"script\": {\n    \"source\": \"\"\"\n      if (ctx._source.inventory_count &lt; 10) {\n        ctx._source.in_stock = false;\n      } else {\n        ctx._source.in_stock = true;\n      }\n    \"\"\"\n  }\n}\nYou also update the in_stock field for products with low inventory after the content is reindexed/migrated.\nPOST /products_new/_update_by_query\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock = false\"\n  },\n  \"query\": {\n    \"range\": {\n      \"inventory_count\": {\n        \"lt\": 10\n      }\n    }\n  }\n}\n\n\n\nTest\n\nSearch the new index to verify the reindexed data and updated in_stock field\nGET /products_new/_search\n// edited response\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"products_new\",\n        \"_id\": \"rHqtBJEBRRh1FLFi_quh\",\n        \"_score\": 1,\n        \"_source\": {\n          \"price\": 19.99,\n          \"inventory_count\": 10,\n          \"name\": \"Product A\",\n          \"in_stock\": true\n        }\n      },\n      {\n        \"_index\": \"products_new\",\n        \"_id\": \"rXqtBJEBRRh1FLFi_qui\",\n        \"_score\": 1,\n        \"_source\": {\n          \"price\": 29.99,\n          \"inventory_count\": 5,\n          \"name\": \"Product B\",\n          \"in_stock\": false\n        }\n      },\n      {\n        \"_index\": \"products_new\",\n        \"_id\": \"rnqtBJEBRRh1FLFi_qui\",\n        \"_score\": 1,\n        \"_source\": {\n          \"price\": 39.99,\n          \"inventory_count\": 20,\n          \"name\": \"Product C\",\n          \"in_stock\": true\n        }\n      }\n    ]\n  }\n}\nThe response should show the reindexed products with in_stock set correctly based on the inventory count.\nSearch products_old to verify the original data and the absence of in_stock\nGET /products_old/_search\n// edited response\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"products_old\",\n        \"_id\": \"rHqtBJEBRRh1FLFi_quh\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Product A\",\n          \"price\": 19.99,\n          \"inventory_count\": 10\n        }\n      },\n      {\n        \"_index\": \"products_old\",\n        \"_id\": \"rXqtBJEBRRh1FLFi_qui\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Product B\",\n          \"price\": 29.99,\n          \"inventory_count\": 5\n        }\n      },\n      {\n        \"_index\": \"products_old\",\n        \"_id\": \"rnqtBJEBRRh1FLFi_qui\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Product C\",\n          \"price\": 39.99,\n          \"inventory_count\": 20\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nThe Reindex API is used to copy data from the old index to the new index while applying a script to set the “in_stock” field based on the inventory count.\nThe Update By Query API is used to update the in_stock field for products with an inventory count lower than 10.\n\n\n\nClean-up (optional)\n\nDelete the two indices\nDELETE products_old\nDELETE products_new\n\n\n\nDocumentation\n\nReindex API\nUpdate By Query API\nScripting\n\n\n\n\nExample 3: Reindexing documents from an old product catalog to a new one with updated mappings and updating prices in the new catalog\n\nRequirements\n\nCreate the products_old index and add sample products.\nCreate the products_new index using the products_old mapping.\nReindex documents from products_old to products_new.\n\nIncrease the price of all products in products_new by 10%.\n\n\n\n\nSteps\n\nCreate the products_old index and add sample products\nPUT /products_old\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nPOST /products_old/_bulk\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"1\" } }\n{ \"product_id\": \"1\", \"name\": \"Wireless Mouse\", \"description\": \"A high-quality wireless mouse with ergonomic design.\", \"price\": 29.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"2\" } }\n{ \"product_id\": \"2\", \"name\": \"Gaming Keyboard\", \"description\": \"Mechanical gaming keyboard with customizable RGB lighting.\", \"price\": 79.99, \"availability_status\": true }\n{ \"index\": { \"_index\": \"products_old\", \"_id\": \"3\" } }\n{ \"product_id\": \"3\", \"name\": \"USB-C Hub\", \"description\": \"A versatile USB-C hub with multiple ports.\", \"price\": 49.99, \"availability_status\": true }\nCreate the new index with updated mappings\n\nDefine the new index products_new with the desired mappings.\n\nPUT /products_new\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"availability_status\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\nReindex Documents from products_old to products_new while updating price\nPOST _reindex\n{\n  \"source\": {\n    \"index\": \"products_old\"\n  },\n  \"dest\": {\n    \"index\": \"products_new\"\n  },\n  \"script\": {\n    \"source\": \"ctx._source.price *= 1.1;\"\n  }\n}\nOR Migrate the content and then update price in the new index using the Update By Query API to increase the price of all products in products_new by 10%.\nPOST _reindex\n{\n  \"source\": {\n    \"index\": \"products_old\"\n  },\n  \"dest\": {\n    \"index\": \"products_new\"\n  }\n}\nPOST /products_new/_update_by_query\n{\n  \"script\": {\n    \"source\": \"ctx._source.price *= 1.10\",\n    \"lang\": \"painless\"\n  },\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n\n\nTest\n\nVerify the reindexing\nGET /products_old/_count\nGET /products_new/_count\n// responses for both indices\n# GET /products_old/_count 200 OK\n{\n  \"count\": 3,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  }\n}\n# GET /products_new/_count 200 OK\n{\n  \"count\": 3,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  }\n}\nVerify the price update\nGET /products_old,products_new/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"_source\": [\n    \"price\"\n    ]\n}\n// edited response\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"products_new\",\n        \"_id\": \"1\",\n        \"_score\": 1,\n        \"_source\": {\n          \"price\": 32.989000000000004\n        }\n      },\n      {\n        \"_index\": \"products_new\",\n        \"_id\": \"2\",\n        \"_score\": 1,\n        \"_source\": {\n          \"price\": 87.989\n        }\n      },\n      {\n        \"_index\": \"products_new\",\n        \"_id\": \"3\",\n        \"_score\": 1,\n        \"_source\": {\n          \"price\": 54.989000000000004\n        }\n      },\n      {\n        \"_index\": \"products_old\",\n        \"_id\": \"1\",\n        \"_score\": 1,\n        \"_source\": {\n          \"price\": 29.99\n        }\n      },\n      {\n        \"_index\": \"products_old\",\n        \"_id\": \"2\",\n        \"_score\": 1,\n        \"_source\": {\n          \"price\": 79.99\n        }\n      },\n      {\n        \"_index\": \"products_old\",\n        \"_id\": \"3\",\n        \"_score\": 1,\n        \"_source\": {\n          \"price\": 49.99\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nMappings Update: Ensure the new index products_new has the updated mappings to accommodate any changes in the document structure.\nPrice Update Script: The script in the Update By Query API uses the painless language to increase the price by 10%. This is a simple and efficient way to update document fields.\n\n\n\nClean-up (optional)\n\nDelete the indices\nDELETE /products_old\nDELETE /products_new\n\n\n\nDocumentation\n\nIndex Settings\nPainless Scripting Language\nReindex API\nUpdate By Query API",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "4-data-processing.html#task-define-and-use-an-ingest-pipeline-that-satisfies-a-given-set-of-requirements-including-the-use-of-painless-to-modify-documents",
    "href": "4-data-processing.html#task-define-and-use-an-ingest-pipeline-that-satisfies-a-given-set-of-requirements-including-the-use-of-painless-to-modify-documents",
    "title": "4  Data Processing",
    "section": "4.5 Task: Define and use an ingest pipeline that satisfies a given set of requirements, including the use of Painless to modify documents",
    "text": "4.5 Task: Define and use an ingest pipeline that satisfies a given set of requirements, including the use of Painless to modify documents\n\nExample 1: Create an ingest pipeline for enriching and modifying product data in an e-commerce catalog\n\nRequirements\n\nCreate an ingest pipeline named product_pipeline to process incoming documents.\nApply a Painless script to modify price to add 10% to the price\nEnrich the data by adding the ingest time to a timestamp field\nCreate a product_catalog index\n\nNotes: the use of the ctx object which represents a single document being processed. When updating a field (meaning the doc already exists in the index) you use the following form:\nctx._source.[field name]\nvs. directly accessing the field in question prior to it being indexed:\nctx.[field name]\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nDefine the ingest pipeline with a Painless script and additional processors:\nPUT /_ingest/pipeline/product_pipeline\n{\n  \"processors\": [\n    {\n      \"script\": {\n        \"lang\": \"painless\",\n        \"source\": \"\"\"\n          if (ctx.price != null) {\n            ctx.price *= 1.1;\n          }\n        \"\"\"\n      }\n    },\n    {\n      \"set\": {\n        \"field\": \"timestamp\",\n        \"value\": \"{{_ingest.timestamp}}\"\n      }\n    }\n  ]\n}\nCreate the product_catalog index\nPUT /product_catalog\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      },\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\nIndex documents using the ingest pipeline\nPOST /product_catalog/_bulk?pipeline=product_pipeline\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 20.0 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 30.0 }\n\n\n\nTest\n\nVerify the ingest pipeline configuration:\nGET /_ingest/pipeline/product_pipeline\nSearch the indexed documents to ensure the modifications have been applied:\nGET /product_catalog/_search\n\n\n\nConsiderations\n\nThe Painless script modifies the price field to contain a 10% higher price\nThe set processor adds a timestamp to each document to track when it was ingested.\nThe inkjest pipeline processes all incoming documents to maintain data consistency.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE product_catalog\nDelete the pipeline\nDELETE _ingest/pipeline/product_pipeline\n\n\n\nDocumentation\n\nBulk API\nIngest Node Pipelines\nPainless Scripting Language\n\n\n\n\nExample 2: Creating an ingest pipeline to extract and transform data for a logging index\nThis example creates another ingest pipeline, but this time adds it directly into the index definition.\nThis is also an example of how helpful it is to know more about scripting in Elasticsearch. The examples may or may not be trivial/complex, but an understanding of how to write script is required.\n\nRequirements\n\nCreate an ingest pipeline named logging-pipeline\nExtract from the log message:\n\nthe log level (DEBUG, INFO, WARNING, ERROR)\nthe log timestamp in ISO format\n\nAdd a new field log_level_tag with a value based on the log level (e.g. DEBUG -&gt; DEBUG_LOG).\nAdd a new field log_timestamp_in_seconds with the timestamp in seconds.\nCreate a logging-index index\n\nDeclare the ingest pipeline as the defaultin the logging-index index settings\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nCreate an ingest pipeline:\nPUT /_ingest/pipeline/logging-pipeline\n{\n  \"description\": \"Extract and transform log data\",\n  \"processors\": [\n    {\n      \"grok\": {\n        \"field\": \"message\",\n        \"patterns\": [\"%{LOGLEVEL:log_level} %{TIMESTAMP_ISO8601:log_timestamp} %{GREEDYDATA:message}\"]\n      }\n    },\n    {\n      \"script\": {\n        \"source\": \"\"\"\n          ctx.log_level_tag = ctx.log_level.toUpperCase() + '_LOG';\n          ctx.log_timestamp_in_seconds = ZonedDateTime.parse(ctx.log_timestamp).toEpochSecond();\n        \"\"\",\n        \"lang\": \"painless\"\n      }\n    }\n  ]\n}\nCreate an index with the ingest pipeline:\nPUT /logging-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"log_level\": {\n        \"type\": \"keyword\"\n      },\n      \"log_timestamp\": {\n        \"type\": \"date\"\n      },\n      \"log_level_tag\": {\n        \"type\": \"keyword\"\n      },\n      \"log_timestamp_in_seconds\": {\n        \"type\": \"long\"\n      }\n    }\n  },\n  \"settings\": {\n    \"index\": {\n      \"default_pipeline\": \"logging-pipeline\"\n    }\n  }\n}\nAdd documents to the index:\nPOST /logging-index/_bulk\n{ \"index\": { \"_index\": \"logging-index\" } }\n{ \"message\": \"DEBUG 2022-05-25T14:30:00.000Z This is a debug message\" }\n{ \"index\": { \"_index\": \"logging-index\" } }\n{ \"message\": \"INFO 2022-05-25T14:30:00.000Z This is an info message\" }\n\n\n\nTest\n\nVerify that the documents have been processed correctly:\nGET /logging-index/_search\n// edited response\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"logging-index\",\n        \"_id\": \"uXpCBpEBRRh1FLFiQ6s4\",\n        \"_score\": 1,\n        \"_source\": {\n          \"log_level\": \"DEBUG\",\n          \"log_timestamp\": \"2022-05-25T14:30:00.000Z\",\n          \"log_level_tag\": \"DEBUG_LOG\",\n          \"message\": \"This is a debug message\",\n          \"log_timestamp_in_seconds\": 1653489000\n        }\n      },\n      {\n        \"_index\": \"logging-index\",\n        \"_id\": \"unpCBpEBRRh1FLFiQ6s4\",\n        \"_score\": 1,\n        \"_source\": {\n          \"log_level\": \"INFO\",\n          \"log_timestamp\": \"2022-05-25T14:30:00.000Z\",\n          \"log_level_tag\": \"INFO_LOG\",\n          \"message\": \"This is an info message\",\n          \"log_timestamp_in_seconds\": 1653489000\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nThe ingest pipeline uses the Grok processor to extract the log level and timestamp from the log message.\nThe Painless script processor is used to transform the log level and timestamp into new fields.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE logging-index\nDelete the pipeline\nDELETE _ingest/pipeline/logging-pipeline\n\n\n\nDocumentation\n\nIngest Node Pipelines\nPainless Scripting Language\n\n\n\n\nExample 3: Creating an ingest pipeline for product data\n\nRequirements\n\nCreate an index mapping for products with fields like name, price, category, description, discounted_price.\nPreprocess incoming product data using an ingest pipeline called product_pipeline:\n\nLowercase the name and category fields\nRemove HTML tags from the description field\nCalculate a discounted_price field based on the price field and a discount percentage stored in a pipeline variable\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nDefine the ingest pipeline:\nPUT _ingest/pipeline/product_pipeline\n{\n  \"processors\": [\n    {\n      \"lowercase\": {\n        \"field\": \"name\"\n      },\n      \"html_strip\": {\n        \"field\": \"description\"\n      },\n      \"script\": {\n        \"source\": \"double discount = 0.1; ctx.discounted_price = ctx.price * (1 - discount);\"\n      }\n    },\n    {\n      \"lowercase\": {\n        \"field\": \"category\"\n      }\n    }\n  ]\n}\nIndex a sample document using the ingest pipeline:\nPUT /products/_doc/1?pipeline=product_pipeline\n{\n  \"name\": \"Product A\",\n  \"price\": 99.99,\n  \"category\": \"Electronics\",\n  \"description\": \"A &lt;b&gt;high-quality&lt;/b&gt; product for running enthusiasts.\"\n}\n\n\n\nTest\n\nSearch the products index and verify that the document has been processed by the ingest pipeline:\nGET /products/_search\n// edited response\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"products\",\n        \"_id\": \"1\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"product a\",\n          \"description\": \"A high-quality product for running enthusiasts.\",\n          \"category\": \"electronics\",\n          \"price\": 99.99,\n          \"discounted_price\": 89.991\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nThe ingest pipeline is defined with a list of processors that perform specific operations on incoming documents.\nThe lowercase processor lowercases the name and category fields.\nThe html_strip processor removes HTML tags from description\nThe script processor uses the Painless scripting language to calculate the discounted_price field based on the price field and a discount percentage variable.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE products\nDelete the pipeline\nDELETE _ingest/pipeline/product_pipeline\n\n\n\nDocumentation\n\nIngest Node\nIngest Pipelines\nIngest Processors\nPainless Scripting Language\n\n\n\n\nExample 4: Merge content from two indices into a third index\n\nRequirements\nThe movie index has content that looks like this:\n{\n  \"movie_id\": 1,\n  \"title\": \"The Adventure Begins\",\n  \"release_year\": 2021,\n  \"genre_code\": \"ACT\"\n}\nThe genre index has content that looks like this:\n{\n  \"genre_code\": \"ACT\",\n  \"description\": \"Action - Movies with high energy and lots of physical activity\"\n}\nMerge movie and genre into a third index called movie_with_genre that includes the genre.description in each movie record:\n{\n  \"movie_id\": 1,\n  \"title\": \"The Adventure Begins\",\n  \"release_year\": 2021,\n  \"genre_code\": \"ACT\",\n  \"genre_description\": \"Action - Movies with high energy and lots of physical activity\"\n}\n\n\nSteps\nIn order to merge two or more indices into a third index you will need to create an ingest pipeline that uses an index management enrich policy.\n\nCreate an enrich policy that contains the index with the additional content to be used\nExecute the policy to create an enrich index as a temporary location for the enrich content\nCreate an ingest pipeline that points to the enrich policy and the input index that will be merged with the enrich index\n\nFROM THE KIBANA UI\n\nOpen the Kibana Console or use a REST client\n\nCreate the movie index with sample documents\nPUT /movie\n{\n  \"mappings\": {\n    \"properties\": {\n      \"movie_id\": { \"type\": \"integer\" },\n      \"title\": { \"type\": \"text\" },\n      \"release_year\": { \"type\": \"integer\" },\n      \"genre_code\": { \"type\": \"keyword\" }\n    }\n  }\n}\n\nPOST /movie/_bulk\n{ \"index\": { \"_id\": 1 } }\n{ \"movie_id\": 1, \"title\": \"The Adventure Begins\", \"release_year\": 2021, \"genre_code\": \"ACT\" }\n{ \"index\": { \"_id\": 2 } }\n{ \"movie_id\": 2, \"title\": \"Drama Unfolds\", \"release_year\": 2019, \"genre_code\": \"DRM\" }\n{ \"index\": { \"_id\": 3 } }\n{ \"movie_id\": 3, \"title\": \"Comedy Night\", \"release_year\": 2020, \"genre_code\": \"COM\" }\n{ \"index\": { \"_id\": 4 } }\n{ \"movie_id\": 4, \"title\": \"Epic Adventure\", \"release_year\": 2022, \"genre_code\": \"ACT\" }\n{ \"index\": { \"_id\": 5 } }\n{ \"movie_id\": 5, \"title\": \"Tragic Tale\", \"release_year\": 2018, \"genre_code\": \"DRM\" }\nCreate the genre index with sample documents\nPUT /genre\n{\n  \"mappings\": {\n    \"properties\": {\n      \"genre_code\": { \"type\": \"keyword\" },\n      \"description\": { \"type\": \"text\" }\n    }\n  }\n}\n\nPOST /genre/_bulk\n{ \"index\": { \"_id\": \"ACT\" } }\n{ \"genre_code\": \"ACT\", \"description\": \"Action - Movies with high energy and lots of physical activity\" }\n{ \"index\": { \"_id\": \"DRM\" } }\n{ \"genre_code\": \"DRM\", \"description\": \"Drama - Movies with serious, emotional, and often realistic stories\" }\n{ \"index\": { \"_id\": \"COM\" } }\n{ \"genre_code\": \"COM\", \"description\": \"Comedy - Movies designed to make the audience laugh\" }\nOptionally, create the movie_with_genre index\nPUT /movie_with_genre\n{\n  \"mappings\": {\n    \"properties\": {\n      \"movie_id\": { \"type\": \"integer\" },\n      \"title\": { \"type\": \"text\" },\n      \"release_year\": { \"type\": \"integer\" },\n      \"genre_code\": { \"type\": \"keyword\" },\n      \"genre_description\": { \"type\": \"text\" }\n    }\n  }\n}\n\nFrom the Kibana dashboard: Home &gt; Management &gt; Index Management\nPress Add an Enrich Policy\nConfiguration\n\nPolicy Name: movie-genre-policy\nPolicy Type: Match\nSource Indices: genre\n\nNext: Field Selection\n\nMatch field: genre_code\nEnrich field: description\n\nNext: Create\nPress Create and Execute (if everything looks correct)\nHome &gt; Management &gt; Data &gt; Ingest &gt; Ingest Pipelines\n\n\nPress: Create Pipeline &gt; New Pipeline\nCreate Pipeline\n\nName: genre_ingest_pipeline\nPress: Add Your First Processor &gt; Add a Processor\n\nAdd Processor\n\nProcessor: Enrich\nField: genre_code (from the movie index)\nPolicy name: movie-genre-policy\nTarget field: genre_description (from movie_with_genre index)\n\nPress Add Processor\n\nPress: Test Document: Add Documents\n\nEnter:\n[\n  {\n    \"_index\": \"movie\",\n    \"_source\": {\n      \"movie_id\": 1,\n      \"title\": \"The Adventure Begins\",\n      \"release_year\": 2021,\n      \"genre_code\": \"ACT\"\n    }\n  }\n]\nPress: Run the Pipeline\nIf the information entered is correct the response will be:\n{\n  \"docs\": [\n    {\n      \"doc\": {\n        \"_index\": \"movie\",\n        \"_version\": \"-3\",\n        \"_id\": \"_id\",\n        \"_source\": {\n          \"release_year\": 2021,\n          \"genre_description\": {\n            \"description\": \"Action - Movies with high energy and lots of physical activity\",\n            \"genre_code\": \"ACT\"\n          },\n          \"movie_id\": 1,\n          \"title\": \"The Adventure Begins\",\n          \"genre_code\": \"ACT\"\n        },\n        \"_ingest\": {\n          \"timestamp\": \"2024-08-04T17:18:50.159798109Z\"\n        }\n      }\n    }\n  ]\n}    \nWhich is wrong as we just want the genre description field and not both the genre_code and description. The answer is given in the JSON below.\nPress the X in the top right hand corner of the panel to close the panel (not the browser).\nPress: Create Pipeline (when thhe side panel opens press Close)\n\nWTF? Not sure why the enrich pipeline does that, but it needs to be corrected.\nFROM THE KIBANA CONSOLE\n\nOpen the Kibana Console or use a REST client\n\nCreate the movie index with sample documents\n\nPUT /movie\n{\n  \"mappings\": {\n    \"properties\": {\n      \"movie_id\": { \"type\": \"integer\" },\n      \"title\": { \"type\": \"text\" },\n      \"release_year\": { \"type\": \"integer\" },\n      \"genre_code\": { \"type\": \"keyword\" }\n    }\n  }\n}\n\nPOST /movie/_bulk\n{ \"index\": { \"_id\": 1 } }\n{ \"movie_id\": 1, \"title\": \"The Adventure Begins\", \"release_year\": 2021, \"genre_code\": \"ACT\" }\n{ \"index\": { \"_id\": 2 } }\n{ \"movie_id\": 2, \"title\": \"Drama Unfolds\", \"release_year\": 2019, \"genre_code\": \"DRM\" }\n{ \"index\": { \"_id\": 3 } }\n{ \"movie_id\": 3, \"title\": \"Comedy Night\", \"release_year\": 2020, \"genre_code\": \"COM\" }\n{ \"index\": { \"_id\": 4 } }\n{ \"movie_id\": 4, \"title\": \"Epic Adventure\", \"release_year\": 2022, \"genre_code\": \"ACT\" }\n{ \"index\": { \"_id\": 5 } }\n{ \"movie_id\": 5, \"title\": \"Tragic Tale\", \"release_year\": 2018, \"genre_code\": \"DRM\" }\n\nCreate the genre index with sample documents\n\nPUT /genre\n{\n  \"mappings\": {\n    \"properties\": {\n      \"genre_code\": { \"type\": \"keyword\" },\n      \"description\": { \"type\": \"text\" }\n    }\n  }\n}\n\nPOST /genre/_bulk\n{ \"index\": { \"_id\": \"ACT\" } }\n{ \"genre_code\": \"ACT\", \"description\": \"Action - Movies with high energy and lots of physical activity\" }\n{ \"index\": { \"_id\": \"DRM\" } }\n{ \"genre_code\": \"DRM\", \"description\": \"Drama - Movies with serious, emotional, and often realistic stories\" }\n{ \"index\": { \"_id\": \"COM\" } }\n{ \"genre_code\": \"COM\", \"description\": \"Comedy - Movies designed to make the audience laugh\" }\n\nOptionally, create the movie_with_genre index\n\nPUT /movie_with_genre\n{\n  \"mappings\": {\n    \"properties\": {\n      \"movie_id\": { \"type\": \"integer\" },\n      \"title\": { \"type\": \"text\" },\n      \"release_year\": { \"type\": \"integer\" },\n      \"genre_code\": { \"type\": \"keyword\" },\n      \"genre_description\": { \"type\": \"text\" }\n    }\n  }\n}\nCreate an enrich policy\nPUT /_enrich/policy/movie-genre-policy\n{\n  \"match\": {\n    \"indices\": \"genre\",\n    \"match_field\": \"genre_code\",\n    \"enrich_fields\": [\"description\"]\n  }\n}\nExecute the enrich policy\nPUT _enrich/policy/movie-genre-policy/_execute\nDefine the ingest pipeline that will merge the content from genre into movie_with_genre. Notice the use of a temporary field as the genre content is being copied in its entirety into the new index. To correct that, we copy it into a temp field and then delete the temp field.\nPUT _ingest/pipeline/movie_genre_pipeline\n{\n  \"processors\": [\n    {\n      \"enrich\": {\n        \"policy_name\": \"movie-genre-policy\",\n        \"field\": \"genre_code\",\n        \"target_field\": \"enriched_data\",\n        \"max_matches\": \"1\"\n      }\n    },\n    {\n      \"script\": {\n        \"source\": \"\"\"\n          if (ctx.enriched_data != null && ctx.enriched_data.description != null) {\n            ctx.genre_description = ctx.enriched_data.description;\n          }\n          ctx.remove(\"enriched_data\");\n        \"\"\"\n      }\n    }\n  ]\n}\nReindex movie into movie_with_genre\nPOST _reindex\n{\n  \"source\": {\n    \"index\": \"movie\"\n  },\n  \"dest\": {\n    \"index\": \"movie_with_genre\",\n    \"pipeline\": \"movie_genre_pipeline\"\n  }\n}\n\n\n\nTest\n\nValidate the creation of the movie index\nGET movie/_search\nValidate the creation of the genre index\nGET genre/_search\nValidate the creation of the enrich policy\nGET _enrich/policy/movie-genre-policy\nValidate the creation of the ingest pipeline\nGET _ingest/pipeline/movie_genre_pipeline\nSimulate the use of the ingest pipeline\nGET _ingest/pipeline/movie_genre_pipeline/_simulate\n{\n  \"docs\": [\n    {\n      \"_index\": \"movie\",\n      \"_source\": {\n        \"movie_id\": 1,\n        \"title\": \"The Adventure Begins\",\n        \"release_year\": 2021,\n        \"genre_code\": \"ACT\"\n      }\n    }\n  ]\n}\nValidate the genre_description in movie_with_genre\nGET movie_with_genre/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"_source\": [ \"genre_code\", \"genre_description\" ]\n}\n// edited response\n{\n  ...\n \"hits\": [\n   {\n     \"_index\": \"movie_with_genre\",\n     \"_id\": \"1\",\n     \"_score\": 1,\n     \"_source\": {\n       \"genre_description\": \"Action - Movies with high energy and lots of physical activity\",\n       \"genre_code\": \"ACT\"\n     }\n   },\n   {\n     \"_index\": \"movie_with_genre\",\n     \"_id\": \"2\",\n     \"_score\": 1,\n     \"_source\": {\n       \"genre_description\": \"Drama - Movies with serious, emotional, and often realistic stories\",\n       \"genre_code\": \"DRM\"\n     }\n   },\n   {\n     \"_index\": \"movie_with_genre\",\n     \"_id\": \"3\",\n     \"_score\": 1,\n     \"_source\": {\n       \"genre_description\": \"Comedy - Movies designed to make the audience laugh\",\n       \"genre_code\": \"COM\"\n     }\n   },\n   ...\n ]\n  }\n}\nConsiderations\n\n\nThe Painless script calculates a 10% discount on the price.\nRuntime fields are defined in the index mappings and can be used for querying and aggregations without being stored in the index.\n\n\n\nClean-up (optional)\n\nDelete the final index\nDELETE movie_with_genre\nDelete the ingest pipeline and the enrich policy\nDELETE _ingest/pipeline/movie_genre_pipeline\nDELETE _enrich/policy/movie-genre-policy\nDelete the movie and genre indices\nDELETE movie\nDELETE genre\n\n\n\nDocumentation\n\nEnrich Processor Documentation\nEnrich Policy Management\nIngest Node Overview\nScript Processor\nReindex API\nUpdate By Query API",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "4-data-processing.html#task-define-runtime-fields-to-retrieve-custom-values-using-painless-scripting",
    "href": "4-data-processing.html#task-define-runtime-fields-to-retrieve-custom-values-using-painless-scripting",
    "title": "4  Data Processing",
    "section": "4.6 Task: Define runtime fields to retrieve custom values using Painless scripting",
    "text": "4.6 Task: Define runtime fields to retrieve custom values using Painless scripting\n\nExample 1: Creating a runtime field for discounted prices in a product catalog\n\nRequirements\n\nCreate a mapping for the product_catalog index\n\nInclude runtime field discounted_price to calculate a discount on product prices.\nApply a Painless script to dynamically compute the discounted price.\nEnsure the runtime field is available for queries and aggregations.\n\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nDefine the index with appropriate mappings:\nPUT /product_catalog\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product_id\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    },\n    \"runtime\": {\n      \"discounted_price\": {\n        \"type\": \"double\",\n        \"script\": {\n          \"source\": \"\"\"\n            if (doc['price'].size() != 0) {\n              emit(doc['price'].value * 0.9);\n            } else {\n              emit(Double.NaN);\n            }\n          \"\"\"\n        }\n      }\n    }\n  }\n}\nIndex sample documents using the _bulk endpoint:\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 20.0 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 30.0 }\n\n\n\nTest\n\nSearch the indexed documents and retrieve the runtime field\nGET /product_catalog/_search\n{\n  \"_source\": [\"name\", \"price\"],\n  \"fields\": [\"discounted_price\"],\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n// edited response\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"product_catalog\",\n        \"_id\": \"1\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Product 1\",\n          \"price\": 20\n        },\n        \"fields\": {\n          \"discounted_price\": [\n            18\n          ]\n        }\n      },\n      {\n        \"_index\": \"product_catalog\",\n        \"_id\": \"2\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Product 2\",\n          \"price\": 30\n        },\n        \"fields\": {\n          \"discounted_price\": [\n            27\n          ]\n        }\n      }\n    ]\n  }\n}\nVerify the discounted price in the search results\nGET /product_catalog/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"script_fields\": {\n    \"discounted_price\": {\n      \"script\": {\n        \"source\": \"doc['price'].value * 0.9\"\n      }\n    }\n  }\n}\n\n\n\nConsiderations\n\nThe Painless script calculates a 10% discount on the price.\nRuntime fields are defined in the index mappings and can be used for querying and aggregations without being stored in the index.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE product_catalog\n\n\n\nDocumentation\n\nBulk API\nPainless Scripting Language\nRuntime Fields\nScripts Fields\n\n\n\n\nExample 2: Create a runtime field to extract the domain from a URL\n\nRequirements\n\nCreate a mapping to the myindex index\n\nDefine a field called url\n\nExtract the domain from a URL field using Painless scripting to define a runtime field named domain.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nCreate an index with a URL field:\nPUT /myindex\n{\n  \"mappings\": {\n    \"properties\": {\n      \"url\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\nDefine a runtime field to extract the domain:\nPUT myindex\n{\n  \"mappings\": {\n    \"properties\": {\n      \"url\": {\n        \"type\": \"keyword\"\n      }\n    },\n    \"runtime\": {\n      \"domain\": {\n        \"type\": \"keyword\",\n        \"script\": {\n          \"source\": \"\"\"\n          // https://xyz.domain.com/stuff/stuff\n        String domain = grok(\"%{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:domain})?(?:%{URIPATHPARAM})?\").extract(doc[\"url\"].value)?.domain;\n        if (domain != null) emit(domain);\n        else emit(\"grok failed\");\n      \"\"\"\n        }\n      }\n    }\n  }\n}\nAdd documents to the index:\nPOST /myindex/_bulk\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"url\": \"https://www.example.com/path/to/page\" }\n{ \"index\": { \"_index\": \"myindex\" } }\n{ \"url\": \"http://sub.example.com/other/page\" }\n\n\n\nTest\n\nVerify that the runtime field is working correctly:\nGET /myindex/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\"domain\"]\n}\n\n\n\nConsiderations\n\nThe runtime field uses Painless scripting to extract the domain from the URL field.\nThe script splits the URL into components and returns the domain (including the sub-domain. Removing it involves ugly logic).\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE myindex\n\n\n\nDocumentation\n\nPainless Scripting\nRuntime Fields\n\n\n\n\nExample 3: Calculating the age difference in years based on date fields\n\nRequirements\n\nCreate a mapping to the people index\nDefine a search query that utilizes a runtime field (current_age) to calculate the age difference in years between two date fields (date_of_birth and current_date) within the search results.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nCreate the index\nPUT people\n{\n  \"mappings\": {\n    \"properties\": {\n      \"date_of_birth\": {\n        \"type\": \"date\"\n      },\n      \"current_date\": {\n        \"type\": \"date\"\n      }\n    },\n    \"runtime\": {\n      \"current_age\": {\n        \"type\": \"long\",\n        \"script\": {\n          \"source\": \"\"\"\n          int birthday_year = ZonedDateTime.parse(doc[\"date_of_birth\"].value.toString()).getYear();\n          int today_year = ZonedDateTime.parse(doc[\"current_date\"].value.toString()).getYear();\n          long age = today_year - birthday_year;\n          emit(age);\n          \"\"\"\n        }\n      }\n    }\n  }\n}\nIndex sample documents\nPOST /people/_bulk\n{ \"index\": { \"_index\": \"people\", \"_id\": \"1\" } }\n{ \"name\": \"Alice\", \"date_of_birth\": \"1990-01-01\", \"current_date\": \"2024-07-08\" }\n{ \"index\": { \"_index\": \"people\", \"_id\": \"2\" } }\n{ \"name\": \"Bob\", \"date_of_birth\": \"1985-05-15\", \"current_date\": \"2024-07-08\" }\n{ \"index\": { \"_index\": \"people\", \"_id\": \"3\" } }\n{ \"name\": \"Charlie\", \"date_of_birth\": \"2000-12-25\", \"current_date\": \"2024-07-08\" }\nConstruct a search query and return the runtime field:\nGET people/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\n    \"current_age\"\n  ]\n}\n\n\n\nTest\n\nEnsure the documents in your index have date_of_birth and current_date fields in a compatible date format\nGET people/_search\nRun the search query and examine the response. The results should include an additional field named current_age representing the calculated age difference in years for each document.\nGET people/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"fields\": [\n    \"current_age\"\n  ]\n}\n// edited responses\n{\n  ...\n    \"hits\": [\n      {\n        \"_index\": \"people\",\n        \"_id\": \"1\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Alice\",\n          \"date_of_birth\": \"1990-01-01\",\n          \"current_date\": \"2024-07-08\"\n        },\n        \"fields\": {\n          \"current_age\": [\n            34\n          ]\n        }\n      },\n      {\n        \"_index\": \"people\",\n        \"_id\": \"2\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Bob\",\n          \"date_of_birth\": \"1985-05-15\",\n          \"current_date\": \"2024-07-08\"\n        },\n        \"fields\": {\n          \"current_age\": [\n            39\n          ]\n        }\n      },\n      {\n        \"_index\": \"people\",\n        \"_id\": \"3\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"Charlie\",\n          \"date_of_birth\": \"2000-12-25\",\n          \"current_date\": \"2024-07-08\"\n        },\n        \"fields\": {\n          \"current_age\": [\n            24\n          ]\n        }\n      }\n    ]\n  }\n}\n\n\n\nConsiderations\n\nThe runtime field definition utilizes Painless scripting to perform the age calculation.\nThe script calculates the difference in years between current_date and date_of_birth to determine the user’s age.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE people\n\n\n\nDocumentation\n\nPainless Scripting\nRuntime Fields",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "5-cluster-management.html",
    "href": "5-cluster-management.html",
    "title": "5  Cluster Management",
    "section": "",
    "text": "5.1 Task: Diagnose shard issues and repair a cluster's health\nWhile the odds are rather high that you will have some unassigned shards if you have done enough of the examples and not cleaned up after yourself we will artifically create some so the below will make some degree of sense.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cluster Management</span>"
    ]
  },
  {
    "objectID": "5-cluster-management.html#task-diagnose-shard-issues-and-repair-a-clusters-health",
    "href": "5-cluster-management.html#task-diagnose-shard-issues-and-repair-a-clusters-health",
    "title": "5  Cluster Management",
    "section": "",
    "text": "Example 1: Identifying and resolving unassigned shards to improve cluster health\n\nRequirements\n\nIdentify the cause of unassigned shards.\nReassign shards to nodes to improve cluster health.\nEnsure all indices are properly allocated and the cluster health status is green.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nCreate an index that needs more replicas than available nodes\nPUT /a-bad-index\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 2\n  }\n}\nCheck the cluster health and identify unassigned shards (you should see at least 2 unassigned shards)\n\nGET /_cluster/health\n\n\n\nNumber of unassigned shards\n\n\n\nList the unassigned shards (you should see the index name created above with 2 messages of UNASSIGNED)\nGET _cat/shards?v=true&h=index,shard,prirep,state,node,unassigned.reason&s=state\n\n\n\nNames of the unassigned shards\n\n\nIdentify the reason for the unassigned shards\nGET _cluster/allocation/explain\n{\n  \"index\": \"a-bad-index\", \n  \"shard\": 0, \n  \"primary\": true \n}\nIn the scenario where you are running an Elasticsearch cluster locally and only have one node then you simply have to lower the number of replicas\nPUT /a-bad-index/_settings\n{\n  \"index\": {\n    \"number_of_replicas\": 0\n  }\n}\nRunning the shard check again will show that the unassigned shards are now gone.\nGET _cat/shards?v=true&h=index,shard,prirep,state,node,unassigned.reason&s=state\n\n\n\nThe unassigned shards are gone\n\n\nVerify the cluster health again\nGET /_cluster/health\n\n\n\nTest\n\nCheck the cluster health status\nGET /_cluster/health\nEnsure there are no unassigned shards\nGET /_cat/shards?v&h=index,shard,prirep,state,unassigned.reason,node\n\n\n\nConsiderations\n\nThe cluster reroute command should be used carefully, especially when accepting data loss.\nForce merging should be done during low traffic periods as it is resource-intensive.\nRegularly monitoring cluster health can prevent shard allocation issues.\n\n\n\nDocumentation\nStart here:\n\nDiagnose Unassigned Shards\nRed or Yellow Cluster Health Status\n\nFor more detail:\n\nCat Shards API\nCluster Health API\nCluster Reroute API\nForce Merge API\nNodes Stats API\nTroubleshooting\n\n\n\n\nExample 2: Identifying and resolving a shard failure in a cluster\n\nRequirements\n\nIdentify the cause of a shard failure\nResolve the issue and restore the cluster’s health\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client\nCheck the cluster’s health\nGET /_cluster/health\nIdentify the index and shard with issues\nGET /_cat/shards\nCheck the shard’s status\nGET /_cat/shards/{index_name}-{shard_number}\nResolve the issue (e.g., restart a node, reassign the shard)\nPOST /_cluster/reroute\n{\n  \"commands\": [\n    {\n      \"move\": {\n        \"index\": \"{index_name}\",\n        \"shard\": {shard_number},\n        \"from_node\": \"{node_name}\",\n        \"to_node\": \"{new_node_name}\"\n      }\n    }\n  ]\n}\nVerify the cluster’s health\nGET /_cluster/health\n\n\n\nTest\n\nVerify that the shard is no longer in a failed state\nGET /_cat/shards/{index_name}-{shard_number}\n\n\n\nConsiderations\n\nRegularly monitoring the cluster’s health can help identify issues before they become critical.\nUnderstanding the cause of the shard failure is crucial to resolving the issue effectively.\n\n\n\nDocumentation\n\nCluster Health\nCluster-level Shard Allocation and Routing Settings",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cluster Management</span>"
    ]
  },
  {
    "objectID": "5-cluster-management.html#task-backup-and-restore-a-cluster-andor-specific-indices",
    "href": "5-cluster-management.html#task-backup-and-restore-a-cluster-andor-specific-indices",
    "title": "5  Cluster Management",
    "section": "5.2 Task: Backup and restore a cluster and/or specific indices",
    "text": "5.2 Task: Backup and restore a cluster and/or specific indices\nThis example is specific to a local Elasticsearch cluster, not the Elastic Cloud version. I would recommend learning how to set up a backup and restore from the Kibana UI at Home &gt; Management &gt; Data &gt; Snapshot and Restore. I didn’t find any documentation on the use of the Kibana dashboard to perform backups and restores.\n\nExample 1: Create a snapshot of multiple indices and and restore them\nThis example is specific to a local Elasticsearch cluster, not the Elastic Cloud version. I would recommend learning how to set up a backup and restore from the Kibana UI at Home &gt; Management &gt; Data &gt; Snapshot and Restore. I didn’t find any documentation on the use of the Kibana dashboard to perform backups and restores.\n\nRequirements\n\nBack up the entire Elasticsearch cluster (all the indices on the cluster)\nRestore specific indices from the backup\n\n\n\nSteps\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client.\nCreate two sample indexes with some data\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"1\" } }\n{ \"name\": \"Document 1.1\" }\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"2\" } }\n{ \"name\": \"Document 1.2\" }\n\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"1\" } }\n{ \"name\": \"Document 2.1\" }\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"2\" } }\n{ \"name\": \"Document 2.2\" }\nConfirm the documents were indexed\nGET example_index*/_search\nCreate a snapshot repository\nPUT /_snapshot/example_index_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nCreate a snapshot of the two example indices\nPUT /_snapshot/example_index_backup/snapshot_1\n{\n    \"indices\": \"example_index1,example_index2\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\nVerify the snapshot status\nGET /_snapshot/example_index_backup/snapshot_1\nDelete the two known indices\nDELETE /example_index1\nDELETE /example_index2\nCheck that the two indexes are gone.\nGET /example_index*/_search\nRestore both indices from the snapshot\nPOST /_snapshot/example_index_backup/snapshot_1/_restore\nConfirm both indices were restored\nGET /example_index*/_search\nRestore one index from the snapshot\nDELETE /example_index1\nDELETE /example_index2\nPOST /_snapshot/example_index_backup/snapshot_1/_restore\n{\n  \"indices\": \"example_index2\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false\n}\n\n\n\nTest\n\nVerify the index has been restored\nGET /example_index2/_search\nVerify the integrity of the snapshot\nPOST /_snapshot/example_index_backup/_verify\nCheck the cluster health to ensure the index is properly allocated\nGET /_cluster/health/example_index2\n\n\n\nConsiderations\n\nThe snapshot repository is configured with the fs (file system) type, which stores the backup data in the container’s local file system. For production use, you may want to use a more suitable repository type, such as s3 or gcs.\nThe snapshot name snapshot_1 is used to create a backup of the two indices.\n\n\n\nClean-up (optional)\n\nDelete the indices\nDELETE /example_index1\nDELETE /example_index2\nDelete the Backup\nDELETE /_snapshot/example_index_backup/snapshot_1\n\n\n\nDocumentation\n\nSnapshot and Restore\nSnapshot Repository APIs\nSnapshot Restore API\n\n\n\n\nExample 2: Create a snapshot of an entire cluster and restore a single index\n\nRequirements\n\nBack up the entire Elasticsearch cluster\nRestore specific indices from the backup\n\n\n\nSteps\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client.\nCreate two sample indexes with some data\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"1\" } }\n{ \"name\": \"Document 1.1\" }\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"2\" } }\n{ \"name\": \"Document 1.2\" }\n\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"1\" } }\n{ \"name\": \"Document 2.1\" }\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"2\" } }\n{ \"name\": \"Document 2.2\" }\nConfirm the documents were indexed\nGET example_index*/_search\nCreate a snapshot repository\nPUT /_snapshot/example_cluster_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nCreate a snapshot of the entire cluster\nPUT /_snapshot/example_cluster_backup/full_cluster_backup\nVerify the snapshot status\nGET /_snapshot/example_cluster_backup/full_cluster_backup\nDelete one of the existing indices\nDELETE example_index2\nRestore that specific index from the snapshot with a different name\nPOST /_snapshot/example_cluster_backup/full_cluster_backup/_restore\n{\n    \"indices\": \"example_index2\",\n    \"rename_pattern\": \"example_index2\",\n    \"rename_replacement\": \"restored_example_index2\"\n}\n\n\n\nTest\n\nVerify the index has been restored\nGET /restored_example_index2/_search\nThe response should include the documents from the original example_index2.\nOptionally, you can delete the original index and verify that the restored index remains\nDELETE /example_index2\nGET /restored_example_index2/_search\nVerify the integrity of the snapshot\nPOST /_snapshot/example_cluster_backup/_verify\nCheck the cluster health to ensure the index is properly allocated\nGET /_cluster/health/restored_example_index2\n\n\n\nConsiderations\n\nThe snapshot repository is configured with the fs (file system) type, which stores the backup data in the container’s local file system. For production use, you may want to use a more suitable repository type, such as s3 or gcs.\nThe snapshot name full_cluster_backup is used to create a backup of the entire cluster.\nDuring the restore process, the rename_pattern and rename_replacement options are used to rename the restored index to restored_example_index2.\n\n\n\nClean-up (optional)\n\nDelete the indices\nDELETE /example_index1\nDELETE /example_index2\nDELETE /restored_example_index2\nDelete the backup\nDELETE /_snapshot/example_cluster_backup/full_cluster_backup\n\n\n\nDocumentation\n\nSnapshot and Restore\nSnapshot Repository APIs\nSnapshot Restore API\n\n\n\n\nExample 3: Creating a snapshot of a single index and restoring it\n\nRequirements\n\nCreate a repository for storing snapshots.\nTake a snapshot of the specified index.\nRestore the snapshot to the cluster.\nVerify the integrity and availability of the restored data.\n\n\n\nSteps\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client\nCreate two sample indexes with some data\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"1\" } }\n{ \"name\": \"Document 1.1\" }\n{ \"index\": { \"_index\": \"example_index1\", \"_id\": \"2\" } }\n{ \"name\": \"Document 1.2\" }\n\nPOST /_bulk\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"1\" } }\n{ \"name\": \"Document 2.1\" }\n{ \"index\": { \"_index\": \"example_index2\", \"_id\": \"2\" } }\n{ \"name\": \"Document 2.2\" }\nConfirm the documents were indexed\nGET example_index*/_search\nCreate a snapshot repository\nPUT /_snapshot/single_index_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nTake a snapshot of the specific index\nPUT /_snapshot/single_index_backup/snapshot_1\n{\n    \"indices\": \"example_index1\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\nVerify the snapshot status\nGET /_snapshot/single_index_backup/snapshot_1\nDelete the index to simulate data loss (optional for testing restore)\nDELETE /example_index1\nRestore the snapshot\nPOST /_snapshot/single_index_backup/snapshot_1/_restore\n{\n    \"indices\": \"example_index1\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\n\n\n\nTest\n\nVerify the index has been restored\nGET /example_index1/_search\nVerify the integrity of the snapshot\nPOST /_snapshot/single_index_backup/_verify\nCheck the cluster health to ensure the index is properly allocated\nGET /_cluster/health/example_index1\n\n\n\nConsiderations\n\nThe repository type fs is used for simplicity; other types like s3 can be used depending on the environment.\nignore_unavailable ensures the snapshot process continues even if some indices are missing.\ninclude_global_state is set to false to avoid restoring cluster-wide settings unintentionally.\n\n\n\nClean-up (optional)\n\nDelete the indices\nDELETE /example_index1\nDELETE /example_index2\nDELETE /restored_example_index2\nDelete the Backup\nDELETE /_snapshot/single_index_backup/snapshot_1\n\n\n\nDocumentation\n\nSnapshot and Restore\nCreate Snapshot API\nRestore Snapshot API",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cluster Management</span>"
    ]
  },
  {
    "objectID": "5-cluster-management.html#task-configure-a-snapshot-to-be-searchable",
    "href": "5-cluster-management.html#task-configure-a-snapshot-to-be-searchable",
    "title": "5  Cluster Management",
    "section": "5.3 Task: Configure a snapshot to be searchable",
    "text": "5.3 Task: Configure a snapshot to be searchable\n\nExample 1: Creating a searchable snapshot for the product catalog index\nThis can also be done through the Home &gt; Management &gt; Data &gt; Index Lifecycle Mangement UI. Again, no documentation on how to perform this using the Kibana dashboard.\nSigh. This will only work if you have an Enterprise license.\n\nRequirements\n\nCreate a repository for storing snapshots.\nTake a snapshot of the specified index.\nMount the snapshot as a searchable index.\nVerify the index is searchable without restoring it to the cluster.\n\n\n\nSteps\n\n(Do this is you haven’t already done it due to a previous exercise) Configure the es01 container instance with a backups directory\n\nIn a terminal execute bash on the docker container\nsudo docker exec -it es01 /bin/bash\nCreate a backup directory in the current directory of the container\nmkdir backups\nIf you change directory to backups and run pwd you’ll find that the full path is /usr/share/elasticsearch/backups.\nExit the container shell\nexit\nUpdate the elasticsearch.yml path.repo variable and restart the cluster\n\nWhen we created two single-node clusters (Appendix: Setting Up An Additional Single-node Cluster for Cross-cluster Search (CCS)) we renamed the YAML files for the two cluster:\n\nelasticsearch-es01.yml\nelasticsearch-es02.yml\n\nFor the purposes of this example update elasticsearch-es01.yml.\n\npath.repo: [\"/usr/share/elasticsearch/backups\"]\n\nCopy the YAML file back into the container\n\nsudo docker cp elasticsearch-es01.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml\n\nRestart es01\n\n\nOpen the Kibana Console or use a REST client.\nCreate a sample index with some data\nPOST _bulk\n{ \"index\": { \"_index\": \"products\", \"_id\": \"1\" } }\n{ \"name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 999.99, \"stock\": 50, \"description\": \"A high-performance laptop with 16GB RAM and 512GB SSD.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"2\" } }\n{ \"name\": \"Smartphone\", \"category\": \"Electronics\", \"price\": 699.99, \"stock\": 100, \"description\": \"A latest model smartphone with a stunning display and powerful processor.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"3\" } }\n{ \"name\": \"Headphones\", \"category\": \"Accessories\", \"price\": 199.99, \"stock\": 200, \"description\": \"Noise-cancelling over-ear headphones with superior sound quality.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"4\" } }\n{ \"name\": \"Coffee Maker\", \"category\": \"Home Appliances\", \"price\": 89.99, \"stock\": 75, \"description\": \"A programmable coffee maker with a 12-cup capacity.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"5\" } }\n{ \"name\": \"Running Shoes\", \"category\": \"Footwear\", \"price\": 129.99, \"stock\": 150, \"description\": \"Lightweight running shoes with excellent cushioning and support.\" }\n{ \"index\": { \"_index\": \"products\", \"_id\": \"6\" } }\n{ \"name\": \"Backpack\", \"category\": \"Accessories\", \"price\": 49.99, \"stock\": 300, \"description\": \"Durable backpack with multiple compartments and ergonomic design.\" }\nConfirm the documents were indexed\nGET products/_search\nCreate a snapshot repository\nPUT /_snapshot/products_index_backup\n{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/usr/share/elasticsearch/backups\"\n    }\n}\nTake a snapshot of the specific index\nPUT /_snapshot/products_index_backup/snapshot_1\n{\n  \"indices\": \"products\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false\n}\nVerify the snapshot status\nGET /_snapshot/products_index_backup/snapshot_1\nDelete the index to simulate data loss (optional for testing restore)\nDELETE /products\nMount the snapshot as a searchable index\nPUT /_snapshot/products_index_backup/snapshot_1/_mount\n{\n    \"index\": \"products\",\n    \"renamed_index\": \"products_backup_searchable\"\n}\nIf you don’t have an Enterprise license the above will fail.\n\n\n\nTest\n\nVerify the mounted index is searchable\nGET /products_backup_searchable/_search\nCheck the cluster health to ensure the searchable snapshot is properly allocated\nGET /_cluster/health/products_backup_searchable\n\n\n\nConsiderations\n\nThe repository type fs is used for simplicity; other types like s3 can be used depending on the environment.\nignore_unavailable ensures the snapshot process continues even if some indices are missing.\ninclude_global_state is set to false to avoid restoring cluster-wide settings unintentionally.\nMounting the snapshot as a searchable index allows for searching the data without the need to fully restore it, saving resources and time.\n\n\n\nClean-up (optional)\n\nDelete the index\nDELETE /products\nDelete the Backup\nDELETE /_snapshot/products_index_backup/snapshot_1\n\n\n\nDocumentation\n\nCreate Snapshot API\nMount Searchable Snapshot API\nSearch API\nSearchable Snapshot\nSnapshot and Restore",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cluster Management</span>"
    ]
  },
  {
    "objectID": "5-cluster-management.html#task-configure-a-cluster-for-cross-cluster-search",
    "href": "5-cluster-management.html#task-configure-a-cluster-for-cross-cluster-search",
    "title": "5  Cluster Management",
    "section": "5.4 Task: Configure a cluster for cross-cluster search",
    "text": "5.4 Task: Configure a cluster for cross-cluster search\nFYI: This is similar to the example at Searching Data &gt; Write and execute a query that searches across multiple clusters\n\nExample 1: Setting up cross-cluster search between a local cluster and a remote cluster for an e-commerce catalog\nThe following instructions are for two single-node clusters running locally on your computer.\n\nRequirements\n\nConfigure the remote cluster to be searchable from the local cluster.\nEnsure secure communication between clusters.\nVerify the cross-cluster search functionality.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nConfigure the remote cluster on the local cluster\nPUT /_cluster/settings\n{\n\"persistent\": {\n    \"cluster\": {\n    \"remote\": {\n        \"es01\": {\n        \"seeds\": [\n            \"es01:9300\"\n        ],\n        \"skip_unavailable\": true\n        },\n        \"es02\": {\n        \"seeds\": [\n            \"es02:9300\"\n        ],\n        \"skip_unavailable\": false\n        }\n    }\n    }\n}\n}\n(optional if you are doing this locally) Set up security settings where you have keystores properly setup. On the remote cluster:\nPUT /_cluster/settings\n{\n    \"persistent\": {\n        \"xpack.security.enabled\": true,\n        \"xpack.security.transport.ssl.enabled\": true,\n        \"xpack.security.transport.ssl.verification_mode\": \"certificate\",\n        \"xpack.security.transport.ssl.keystore.path\": \"/path/to/keystore.jks\",\n        \"xpack.security.transport.ssl.truststore.path\": \"/path/to/truststore.jks\"\n    }\n}\nOn the local cluster:\nPUT /_cluster/settings\n{\n    \"persistent\": {\n        \"xpack.security.enabled\": true,\n        \"xpack.security.transport.ssl.enabled\": true,\n        \"xpack.security.transport.ssl.verification_mode\": \"certificate\",\n        \"xpack.security.transport.ssl.keystore.path\": \"/path/to/keystore.jks\",\n        \"xpack.security.transport.ssl.truststore.path\": \"/path/to/truststore.jks\"\n    }\n}\nVerify the remote cluster configuration\nGET /_remote/info\nIndex product documents into each cluster.\n\n\nFor es01 (potentially the local cluster):\nPOST /products/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product\": \"Elasticsearch Guide\", \"category\": \"Books\", \"price\": 29.99 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product\": \"Advanced Elasticsearch\", \"category\": \"Books\", \"price\": 39.99 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product\": \"Elasticsearch T-shirt\", \"category\": \"Apparel\", \"price\": 19.99 }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"product\": \"Elasticsearch Mug\", \"category\": \"Apparel\", \"price\": 12.99 }\nFor es02 (potentially the “remote” cluster) through the command line:\ncurl -u elastic:[your password here] -X POST \"http://localhost:9201/products/_bulk?pretty\" -H 'Content-Type: application/json' -d'\n{ \"index\": { \"_id\": \"5\" } }\n{ \"product\": \"Elasticsearch Stickers\", \"category\": \"Accessories\", \"price\": 4.99 }\n{ \"index\": { \"_id\": \"6\" } }\n{ \"product\": \"Elasticsearch Notebook\", \"category\": \"Stationery\", \"price\": 7.99 }\n{ \"index\": { \"_id\": \"7\" } }\n{ \"product\": \"Elasticsearch Pen\", \"category\": \"Stationery\", \"price\": 3.49 }\n{ \"index\": { \"_id\": \"8\" } }\n{ \"product\": \"Elasticsearch Hoodie\", \"category\": \"Apparel\", \"price\": 45.99 }    '\n\n\nPerform a cross-cluster search query\nGET /remote_cluster:products/_search\n\n\n\nTest\n\nVerify the remote cluster info\nGET /_remote/info\nSearch the remote cluster from the local cluster\nGET /remote_cluster:product_catalog/_search\n\n\n\nConsiderations\n\nEnsure that the nodes listed in the seeds setting are accessible from the local cluster.\nSecurity settings such as SSL/TLS should be configured to ensure secure communication between clusters.\nRegularly monitor the connection status between the clusters to ensure reliability.\n\n\n\nClean-up (optional)\n\nDelete the es01 index.\nDELETE products\nDelete the es02 index from the command line.\ncurl -u elastic:[your password here] -X DELETE \"http://localhost:9201/products?pretty\"\n\n\n\nDocumentation\n\nCross-Cluster Search\nCluster Remote Info API\nSearch API\nSecurity Settings",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cluster Management</span>"
    ]
  },
  {
    "objectID": "5-cluster-management.html#task-implement-cross-cluster-replication",
    "href": "5-cluster-management.html#task-implement-cross-cluster-replication",
    "title": "5  Cluster Management",
    "section": "5.5 Task: Implement cross-cluster replication",
    "text": "5.5 Task: Implement cross-cluster replication\nThere are a number of ways to set up cross-cluster replication and they can all be found here.\nCross-cluster replication needs an Enterprise license\n\n\nExample 1: Setting up cross-cluster replication for the product catalog index between a leader cluster and a follower cluster\nIn this example, we will run 2 single-node clusters locally using containers (as we have for all the other examples).\n\nThe es01 container instance will be considered\n\nleader\nremote\n\nThe es02 container instance will be considered\n\nfollower\nlocal\n\n\nYou may also need to get a free 30-day trial license of certain features including cross-cluster replication. Since the second cluster is not hooked up to Kibana execute this from the command line (assuming you called the docker instance es02 as we have been using in this guide):\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X POST \"http://localhost:9201/_license/start_trial?pretty&acknowledge=true\"\n\nRequirements\n\nConfigure remote cluster settings on both leader and follower clusters.\nSet up the leader index on the leader cluster.\nConfigure the follower index on the follower cluster to replicate from the leader index.\nEnsure secure communication between clusters.\nVerify replication and data consistency.\n\n\n\nSteps\n\nOpen the Kibana Console or use a REST client.\nConfigure the remote cluster settings on the leader cluster (es01)\nPUT /_cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n        \"seeds\": [\n            \"es01:9300\"\n        ],\n        \"skip_unavailable\": true\n        },\n        \"es02\": {\n        \"seeds\": [\n            \"es02:9300\"\n        ],\n        \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}\nConfigure the local cluster settings on the follower cluster (es02)\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X PUT \"http://localhost:9201/_cluster/settings?pretty\" -H \"Content-Type: application/json\" -d'\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"es01\": {\n        \"seeds\": [\n            \"es01:9300\"\n        ],\n        \"skip_unavailable\": true\n        },\n        \"es02\": {\n        \"seeds\": [\n            \"es02:9300\"\n        ],\n        \"skip_unavailable\": false\n        }\n      }\n    }\n  }\n}'\nCreate the leader index on the leader cluster (es01)\nPUT /product_catalog\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n     \"properties\": {\n     \"product_id\": {\n       \"type\": \"keyword\"\n     },\n     \"name\": {\n       \"type\": \"text\"\n     },\n     \"description\": {\n       \"type\": \"text\"\n      },\n      \"price\": {\n        \"type\": \"double\"\n      }\n    }\n  }\n}\nIndex sample documents in the leader index\nPOST /product_catalog/_bulk\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product_id\": \"p001\", \"name\": \"Product 1\", \"description\": \"Description of product 1\", \"price\": 20.0 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product_id\": \"p002\", \"name\": \"Product 2\", \"description\": \"Description of product 2\", \"price\": 30.0 }\nConfigure the follower index on the follower cluster through the command line\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X PUT \"http://localhost:9201/product_catalog_follower/_ccr/follow?pretty\" -H \"Content-Type: application/json\" -d'\n{\n  \"remote_cluster\": \"es01\",\n  \"leader_index\": \"product_catalog\"\n}'\n\n\n\nTest\n\nVerify the follower index (es02) is following the leader index\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] \"http://localhost:9201/product_catalog_follower/_stats?pretty\"\nCheck the data in the follower index (es02) to ensure it matches the leader (es01) index\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] \"http://localhost:9201/product_catalog_follower/_search?pretty\"\n\n\n\nConsiderations\n\nEnsure the nodes listed in the seeds setting are accessible from the follower cluster.\nSecurity settings such as SSL/TLS should be configured to ensure secure communication between clusters (but not for this example given the YAML changes suggested in the Appendix).\nRegularly monitor the replication status and performance to ensure data consistency and reliability.\n\n\n\nClean-up (optional)\n\nDelete the follower configuration\ncurl -v -u elastic:[YOUR ELASTIC PASSWORD HERE] -X DELETE \"http://localhost:9201/product_catalog_follower?pretty\"\nDelete the index\nDELETE product_catalog\n\n\n\nDocumentation\n\nCross-Cluster Replication\nCreate Follower Index API\nCluster Remote Info API\nSearch API\nSecurity Settings",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cluster Management</span>"
    ]
  }
]